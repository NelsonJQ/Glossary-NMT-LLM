{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWyQ4MhiQmqS"
      },
      "source": [
        "Translation with NLLB-200 using CTranslate2\n",
        "\n",
        "This notebook is part of the repository [Adaptive-MT-LLM-Fine-tuning](https://github.com/ymoslem/Adaptive-MT-LLM-Fine-tuning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Requirements\n",
        "#!pip install ctranslate2 sentencepiece -q\n",
        "#!ls /content/models/ct2-nllb-200-3.3B-int8\n",
        "\n",
        "################\n",
        "# Example of converting an NLLB model to CTranslate2 with int8 quantization (it takes a while, so you can skip this step if you already have the model or run it on Google Colab and then download the resulting model):\n",
        "################\n",
        "\n",
        "#!ct2-transformers-converter --model facebook/nllb-200-1.3B --quantization int8 --output_dir /content/models/ct2-nllb-200-1.3B-int8\n",
        "\n",
        "# Download the SentencePiece model\n",
        "#!wget https://s3.amazonaws.com/opennmt-models/nllb-200/flores200_sacrebleu_tokenizer_spm.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnh5nkfxQsyb"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "7jIwEIkCQe1b",
        "outputId": "5d2ff024-b425-4f6b-ce40-2c032945332f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_path = \"data\"\n",
        "tgt_lang = \"spa_Latn\"\n",
        "src_lang = \"eng_Latn\"\n",
        "\n",
        "# Language dictionary for NLLB-200 including ES, EN, FR, RU, ZH, AR, PT, SW\n",
        "lang_dict = {\n",
        "    \"spa_Latn\": \"Spanish\",\n",
        "    \"eng_Latn\": \"English\",\n",
        "    \"fra_Latn\": \"French\",\n",
        "    \"rus_Cyrl\": \"Russian\",\n",
        "    \"zho_Hans\": \"Chinese\",\n",
        "    \"ara_Arab\": \"Arabic\",\n",
        "    \"por_Latn\": \"Portuguese\",\n",
        "    \"swa_Latn\": \"Swahili\"\n",
        "}\n",
        "\n",
        "src_lang_name = lang_dict[src_lang]\n",
        "tgt_lang_name = lang_dict[tgt_lang]\n",
        "\n",
        "source = \"\"\"The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.\n",
        "These initiatives are restoring almost five million hectares of marine ecosystems – an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.\n",
        "\n",
        "The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.\n",
        "The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.\n",
        "\n",
        "“After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.\n",
        "But the challenge ahead of us is significant and we need everyone to play their part,” said Inger Andersen, Executive Director of UNEP.\n",
        "“These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.\n",
        "To deliver our restoration goals, our ambition must be as big as the ocean we must protect.”\n",
        "\n",
        "FAO Director-General QU Dongyu said: “The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.\n",
        "These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"\n",
        "\n",
        "The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration – led by UNEP and FAO – which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.\n",
        "The awards track notable initiatives that support global commitments to restore one billion hectares – an area larger than China – by 2030.\"\"\"\n",
        "\n",
        "\n",
        "#directory = os.path.join(data_path, \"spanish\")\n",
        "\n",
        "#os.chdir(directory)\n",
        "os.getcwd()\n",
        "\n",
        "# print language for input src_lang\n",
        "print(lang_dict[src_lang])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_HHEGmtQ1i3"
      },
      "source": [
        "## Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KT7FdCrySL6T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# [Modify] Set paths to the CTranslate2 and SentencePiece models\n",
        "#!mkdir -p /content/models\n",
        "#!cp -r /content/ct2-nllb* /content/models\n",
        "drive = \"../models\"\n",
        "\n",
        "ct_model_path = os.path.join(drive, \"ct2-nllb-200-1.3B-int8\")\n",
        "sp_model_path = os.path.join(drive, \"flores200_sacrebleu_tokenizer_spm.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xt2rEaKPQ4WC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import ctranslate2\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the source SentecePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(sp_model_path)\n",
        "\n",
        "# Load the CTranslate2 model\n",
        "translator = ctranslate2.Translator(ct_model_path, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Dg4f3qc-exg3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁English', ':']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp.encode_as_pieces(\"English:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crGs90d-UQj6"
      },
      "source": [
        "# Translate (source sentences only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q9DfomHujcG",
        "outputId": "e80d3899-5c27-41e7-e52e-f7aa0c47501c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.\n",
            "These initiatives are restoring almost five million hectares of marine ecosystems – an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.\n",
            "\n",
            "The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.\n",
            "The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.\n",
            "\n",
            "“After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.\n",
            "But the challenge ahead of us is significant and we need everyone to play their part,” said Inger Andersen, Executive Director of UNEP.\n",
            "“These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.\n",
            "To deliver our restoration goals, our ambition must be as big as the ocean we must protect.”\n",
            "\n",
            "FAO Director-General QU Dongyu said: “The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.\n",
            "These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"\n",
            "\n",
            "The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration – led by UNEP and FAO – which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.\n",
            "The awards track notable initiatives that support global commitments to restore one billion hectares – an area larger than China – by 2030.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "source_sents = [sent.strip() for sent in source.split(\"\\n\")]\n",
        "print(*source_sents, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s33AtXObE5u4",
        "outputId": "1d712a7c-9ff0-44c1-8237-73238265f5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado los primeros buques insignia de restauración mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área de aproximadamente el tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre los Océanos.\n",
            "Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del canal de Mozambique, rica en corales, más de 60 de las islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "Las iniciativas ganadoras se anunciaron en un evento durante la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para el apoyo de las Naciones Unidas.\n",
            "\"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "Pero el desafío que tenemos por delante es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, directora ejecutiva del PNUMA.\n",
            "\"Estos buques insignia de la restauración mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "Para lograr nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenible y la reducción de los recursos naturales están afectando nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de subsistencia de las comunidades dependientes.\n",
            "Estos nuevos buques insignia de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n"
          ]
        }
      ],
      "source": [
        "# src_lang = \"eng_Latn\"\n",
        "# tgt_lang = \"spa_Latn\"\n",
        "\n",
        "beam_size = 2\n",
        "\n",
        "# Replace special characters in source_sents, like “,«, –\n",
        "source_sents = [sent.replace(\"“\", '\"').replace(\"”\", '\"') for sent in source_sents]\n",
        "source_sents = [sent.replace(\"–\", \"-\") for sent in source_sents]\n",
        "\n",
        "# Remove empty string from source_sents\n",
        "source_sents = [sent.strip() for sent in source_sents if sent.strip()]\n",
        "target_prefix = [[tgt_lang]] * len(source_sents)\n",
        "\n",
        "# Subword the source sentences\n",
        "source_sents_subworded = sp.encode_as_pieces(source_sents)\n",
        "source_sents_subworded = [[src_lang] + sent + [\"</s>\"] for sent in source_sents_subworded]\n",
        "\n",
        "# Translate the source sentences\n",
        "translations = translator.translate_batch(source_sents_subworded,\n",
        "                                          batch_type=\"tokens\",\n",
        "                                          max_batch_size=2024,\n",
        "                                          beam_size=beam_size,\n",
        "                                          target_prefix=target_prefix)\n",
        "translations = [translation.hypotheses[0] for translation in translations]\n",
        "\n",
        "# Desubword the target sentences\n",
        "translations_desubword = sp.decode(translations)\n",
        "translations_desubword = [sent[len(tgt_lang):].strip() for sent in translations_desubword]\n",
        "\n",
        "print(*translations_desubword[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWX5oeOTXTNK"
      },
      "outputs": [],
      "source": [
        "# Save the translations\n",
        "# with open(\"testUNEP.en\", \"w+\") as output:\n",
        "#   for translation in translations_desubword:\n",
        "#     output.write(translation + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKmQ-y-IxGQZ"
      },
      "source": [
        "# Fuzzy search indexer WIP\n",
        "* ✅ glossary\n",
        "* ➡️ translation memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best match result: {'best_fuzzy': 'UN Environment Programme', 'score': 95.65217391304348, 'result': {'French': '', 'Spanish': 'Programa ONU Medio Ambiente', 'Arabic': ''}}\n",
            "All matches in text (no overlaps), 2 results in total: [{'found_in_text': 'ore', 'best_fuzzy': 'ore', 'score': 100.0, 'result': {'French': 'minerais', 'Spanish': 'yacimientos minerales'}}, {'found_in_text': 'UN Environment Programme is', 'best_fuzzy': 'United Nations Environment Programme', 'score': 80.0, 'result': {'French': '', 'Spanish': 'Programa de las Naciones Unidas para el Medio Ambiente'}}]\n",
            "Nearly-exact English matches, 3 in total: [{'found_in_text': 'UN Environment Programme', 'best_fuzzy': 'UN Environment Programme', 'score': 100.0, 'result': {'French': '', 'Spanish': 'Programa ONU Medio Ambiente'}}, {'found_in_text': 'UNEA', 'best_fuzzy': 'UNEA', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'International Day of Women Judge', 'best_fuzzy': 'International Day of Women Judges', 'score': 98.46153846153847, 'result': {'French': 'Journée internationale des femmes juges', 'Spanish': 'Día Internacional de las Juezas'}}]\n",
            "Complex text matches RESULT #4, 9 results in total: [{'found_in_text': 'United Nations Environment Programme', 'best_fuzzy': 'United Nations Environment Programme', 'score': 100.0, 'result': {'French': '', 'Spanish': 'Programa de las Naciones Unidas para el Medio Ambiente'}}, {'found_in_text': 'UNEP', 'best_fuzzy': 'UNEP', 'score': 100.0, 'result': {'French': '', 'Spanish': 'PNUMA'}}, {'found_in_text': 'South-South cooperation', 'best_fuzzy': 'South-South cooperation', 'score': 100.0, 'result': {'French': 'coopération sud-sud', 'Spanish': 'Cooperación Sur-Sur'}}, {'found_in_text': 'Climate Change', 'best_fuzzy': 'climate change', 'score': 100.0, 'result': {'French': 'changements climatiques', 'Spanish': 'cambio climático'}}, {'found_in_text': 'persistent organic pollutants', 'best_fuzzy': 'Persistent Organic Pollutants', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'ore', 'best_fuzzy': 'ore', 'score': 100.0, 'result': {'French': 'minerais', 'Spanish': 'yacimientos minerales'}}, {'found_in_text': 'UNEA', 'best_fuzzy': 'UNEA', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': '1. total greenhouse gas emissions per year', 'best_fuzzy': '1. Total greenhouse gas emissions per year', 'score': 100.0, 'result': {'French': '1. Total des émissions annuelles de gaz à effet de serre', 'Spanish': '1.  Emisiones totales de gases de efecto invernadero por año'}}, {'found_in_text': 'indicator', 'best_fuzzy': 'indicator', 'score': 100.0, 'result': {'French': 'indicateur', 'Spanish': 'indicador'}}]\n",
            "Nearly-exact English matches: 5 matches in total:  [{'found_in_text': 'United Nations Environment Programme', 'best_fuzzy': 'United Nations Environment Programme', 'score': 100.0, 'result': {'French': '', 'Spanish': 'Programa de las Naciones Unidas para el Medio Ambiente'}}, {'found_in_text': '(UNEP)', 'best_fuzzy': 'UNEP', 'score': 100.0, 'result': {'French': '', 'Spanish': 'PNUMA'}}, {'found_in_text': '(South-South cooperation)', 'best_fuzzy': 'South-South cooperation', 'score': 100.0, 'result': {'French': 'coopération sud-sud', 'Spanish': 'Cooperación Sur-Sur'}}, {'found_in_text': 'persistent organic pollutants', 'best_fuzzy': 'persistent organic pollutant', 'score': 98.24561403508773, 'result': {'French': 'polluant organique persistant', 'Spanish': 'contaminante orgánico persistente'}}, {'found_in_text': 'greenhouse gas emissions per year indicator', 'best_fuzzy': '1. Total greenhouse gas emissions per year', 'score': 91.66666666666667, 'result': {'French': '1. Total des émissions annuelles de gaz à effet de serre', 'Spanish': '1.  Emisiones totales de gases de efecto invernadero por año'}}]\n",
            "Bilingual pairs (best_fuzzy, Spanish translation):\n",
            "('United Nations Environment Programme', 'Programa de las Naciones Unidas para el Medio Ambiente')\n",
            "('UNEP', 'PNUMA')\n",
            "('South-South cooperation', 'Cooperación Sur-Sur')\n",
            "('persistent organic pollutant', 'contaminante orgánico persistente')\n",
            "('1. Total greenhouse gas emissions per year', '1.  Emisiones totales de gases de efecto invernadero por año')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Dict, Optional, Union, Callable, Any, Tuple\n",
        "from rapidfuzz import fuzz, process, utils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MultilingualGlossaryProcessor:\n",
        "    \"\"\"\n",
        "    A class for processing multilingual glossaries using RapidFuzz for fuzzy string matching.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, glossary_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the processor with a glossary file.\n",
        "        \n",
        "        Args:\n",
        "            glossary_path: Path to CSV/Excel file with columns:\n",
        "                          Keyword, Category, English, Arabic, French, Spanish, Chinese, Russian, Portuguese, Swahili\n",
        "        \"\"\"\n",
        "        if glossary_path.endswith('.xlsx') or glossary_path.endswith('.xls'):\n",
        "            self.glossary = pd.read_excel(glossary_path)\n",
        "        else:\n",
        "            self.glossary = pd.read_csv(glossary_path)\n",
        "        \n",
        "        # Define available languages\n",
        "        self.languages = ['English', 'Arabic', 'French', 'Spanish', 'Chinese', 'Russian', 'Portuguese', 'Swahili']\n",
        "        \n",
        "        # Validate glossary structure\n",
        "        required_columns = ['Keyword', 'Category'] + self.languages\n",
        "        missing_columns = [col for col in required_columns if col not in self.glossary.columns]\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "    \n",
        "    def find_best_fuzzy_match(\n",
        "        self,\n",
        "        query: str,\n",
        "        source_language: str,\n",
        "        target_languages: List[str],\n",
        "        scorer: Callable = fuzz.WRatio,\n",
        "        processor: Optional[Callable] = None,\n",
        "        score_cutoff: Optional[float] = 60.0,\n",
        "        process_method: str = \"extractOne\"\n",
        "    ) -> Dict[str, Union[str, Dict[str, str]]]:\n",
        "        \"\"\"\n",
        "        Find the best fuzzy match in the glossary for a given query.\n",
        "        \n",
        "        Args:\n",
        "            query: The text to search for\n",
        "            source_language: Language of the query\n",
        "            target_languages: List of target languages to return translations\n",
        "            scorer: RapidFuzz scorer function (default: fuzz.WRatio)\n",
        "            processor: Text preprocessing function (default: None)\n",
        "            score_cutoff: Minimum similarity score (default: 60.0)\n",
        "            process_method: RapidFuzz process method (\"extractOne\", \"extract\", \"cdist\", \"cpdist\")\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with best_fuzzy match and translations in target languages\n",
        "        \"\"\"\n",
        "        if source_language not in self.languages:\n",
        "            raise ValueError(f\"Source language '{source_language}' not supported. Available: {self.languages}\")\n",
        "        \n",
        "        invalid_targets = [lang for lang in target_languages if lang not in self.languages]\n",
        "        if invalid_targets:\n",
        "            raise ValueError(f\"Invalid target languages: {invalid_targets}. Available: {self.languages}\")\n",
        "        \n",
        "        # Get all terms in source language (excluding NaN values)\n",
        "        source_terms = self.glossary[source_language].dropna().tolist()\n",
        "        \n",
        "        if not source_terms:\n",
        "            return {\"best_fuzzy\": \"\", \"result\": {}}\n",
        "        \n",
        "        # Find best match using specified process method\n",
        "        if process_method == \"extractOne\":\n",
        "            result = process.extractOne(\n",
        "                query, \n",
        "                source_terms, \n",
        "                scorer=scorer, \n",
        "                processor=processor, \n",
        "                score_cutoff=score_cutoff\n",
        "            )\n",
        "        elif process_method == \"extract\":\n",
        "            results = process.extract(\n",
        "                query, \n",
        "                source_terms, \n",
        "                scorer=scorer, \n",
        "                processor=processor, \n",
        "                limit=1, \n",
        "                score_cutoff=score_cutoff\n",
        "            )\n",
        "            result = results[0] if results else None\n",
        "        elif process_method == \"cdist\":\n",
        "            # Using cdist for single query\n",
        "            distances = process.cdist(\n",
        "                [query], \n",
        "                source_terms, \n",
        "                scorer=scorer, \n",
        "                processor=processor, \n",
        "                score_cutoff=score_cutoff\n",
        "            )\n",
        "            if distances.size > 0:\n",
        "                best_idx = np.argmax(distances[0])\n",
        "                if distances[0][best_idx] >= (score_cutoff or 0):\n",
        "                    result = (source_terms[best_idx], distances[0][best_idx], best_idx)\n",
        "                else:\n",
        "                    result = None\n",
        "            else:\n",
        "                result = None\n",
        "        elif process_method == \"cpdist\":\n",
        "            # cpdist requires equal length arrays, so we'll use the query repeated\n",
        "            if len(source_terms) > 0:\n",
        "                distances = process.cpdist(\n",
        "                    [query] * len(source_terms), \n",
        "                    source_terms, \n",
        "                    scorer=scorer, \n",
        "                    processor=processor, \n",
        "                    score_cutoff=score_cutoff\n",
        "                )\n",
        "                if distances.size > 0:\n",
        "                    best_idx = np.argmax(distances)\n",
        "                    if distances[best_idx] >= (score_cutoff or 0):\n",
        "                        result = (source_terms[best_idx], distances[best_idx], best_idx)\n",
        "                    else:\n",
        "                        result = None\n",
        "                else:\n",
        "                    result = None\n",
        "            else:\n",
        "                result = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported process method: {process_method}\")\n",
        "        \n",
        "        if not result:\n",
        "            return {\"best_fuzzy\": \"\", \"result\": {}}\n",
        "        \n",
        "        best_match, score, index = result\n",
        "        \n",
        "        # Find the row containing this match\n",
        "        match_row = self.glossary[self.glossary[source_language] == best_match].iloc[0]\n",
        "        \n",
        "        # Get translations for target languages\n",
        "        translations = {}\n",
        "        for lang in target_languages:\n",
        "            translation = match_row[lang]\n",
        "            if pd.notna(translation):\n",
        "                translations[lang] = str(translation)\n",
        "            else:\n",
        "                translations[lang] = \"\"\n",
        "        \n",
        "        return {\n",
        "            \"best_fuzzy\": best_match,\n",
        "            \"score\": score,\n",
        "            \"result\": translations\n",
        "        }\n",
        "    \n",
        "    def _remove_overlapping_matches(self, matches: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Remove overlapping matches, keeping the longest/highest scoring ones.\n",
        "        \n",
        "        Args:\n",
        "            matches: List of match dictionaries with 'start', 'end', 'score', etc.\n",
        "            \n",
        "        Returns:\n",
        "            Filtered list with non-overlapping matches\n",
        "        \"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # Sort by length (descending) then by score (descending)\n",
        "        sorted_matches = sorted(matches, \n",
        "                              key=lambda x: (x['end'] - x['start'], x['score']), \n",
        "                              reverse=True)\n",
        "        \n",
        "        final_matches = []\n",
        "        used_positions = set()\n",
        "        \n",
        "        for match in sorted_matches:\n",
        "            # Check if this match overlaps with any already selected match\n",
        "            match_positions = set(range(match['start'], match['end']))\n",
        "            \n",
        "            if not match_positions.intersection(used_positions):\n",
        "                # No overlap, add this match\n",
        "                final_matches.append(match)\n",
        "                used_positions.update(match_positions)\n",
        "        \n",
        "        # Sort final matches by position in text\n",
        "        final_matches.sort(key=lambda x: x['start'])\n",
        "        return final_matches\n",
        "    \n",
        "    def find_all_fuzzy_matches_in_text(\n",
        "        self,\n",
        "        text: str,\n",
        "        source_language: str,\n",
        "        target_languages: List[str],\n",
        "        scorer: Callable = fuzz.partial_ratio,\n",
        "        processor: Optional[Callable] = None,\n",
        "        score_cutoff: Optional[float] = 80.0,\n",
        "        min_word_length: int = 1,\n",
        "        limit: int = None\n",
        "    ) -> List[Dict[str, Union[str, Dict[str, str]]]]:\n",
        "        \"\"\"\n",
        "        Find all glossary terms that fuzzy match within a given text using extract method and token_set_ratio.\n",
        "        Efficiently searches for glossary entries in the text and handles overlapping matches.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text to search within\n",
        "            source_language: Language of the input text\n",
        "            target_languages: List of target languages to return translations\n",
        "            scorer: RapidFuzz scorer function (default: fuzz.partial_ratio)\n",
        "            processor: Text preprocessing function (default: None)\n",
        "            score_cutoff: Minimum similarity score (default: 80.0)\n",
        "            min_word_length: Minimum length of words to consider (default: 2)\n",
        "            limit: Maximum number of matches to return (default: None for all matches)\n",
        "            \n",
        "        Returns:\n",
        "            List of dictionaries with found matches and their translations\n",
        "        \"\"\"\n",
        "        if source_language not in self.languages:\n",
        "            raise ValueError(f\"Source language '{source_language}' not supported. Available: {self.languages}\")\n",
        "        \n",
        "        invalid_targets = [lang for lang in target_languages if lang not in self.languages]\n",
        "        if invalid_targets:\n",
        "            raise ValueError(f\"Invalid target languages: {invalid_targets}. Available: {self.languages}\")\n",
        "        \n",
        "        # Get all terms in source language (excluding NaN values)\n",
        "        source_terms = self.glossary[source_language].dropna().tolist()\n",
        "        \n",
        "        if not source_terms:\n",
        "            return []\n",
        "        \n",
        "        # Filter terms by minimum word length\n",
        "        filtered_terms = [term for term in source_terms if len(str(term).strip()) >= min_word_length]\n",
        "        \n",
        "        if not filtered_terms:\n",
        "            return []\n",
        "        \n",
        "        # Sort glossary terms by length (longest first) for better matching\n",
        "        source_terms_sorted = sorted(filtered_terms, key=len, reverse=True)\n",
        "        \n",
        "        all_matches = []\n",
        "        \n",
        "        # Use extract method with token_set_ratio as the only scorer\n",
        "        extract_results = process.extract(\n",
        "            text,\n",
        "            source_terms_sorted,\n",
        "            scorer=scorer,\n",
        "            processor=processor,\n",
        "            score_cutoff=score_cutoff,\n",
        "            limit=limit\n",
        "        )\n",
        "        \n",
        "        if not extract_results:\n",
        "            return []\n",
        "        \n",
        "        #print(f\"Found {len(extract_results)} matches in text using extract method with token_set_ratio\")\n",
        "        \n",
        "        # Process each match result\n",
        "        for match_term, similarity, _ in extract_results:\n",
        "            # Find the row in glossary containing this match\n",
        "            match_rows = self.glossary[self.glossary[source_language] == match_term]\n",
        "            \n",
        "            if match_rows.empty:\n",
        "                continue\n",
        "                \n",
        "            match_row = match_rows.iloc[0]\n",
        "            \n",
        "            # Get translations for target languages\n",
        "            translations = {}\n",
        "            for lang in target_languages:\n",
        "                translation = match_row[lang] if lang in match_row else None\n",
        "                if pd.notna(translation):\n",
        "                    translations[lang] = str(translation)\n",
        "                else:\n",
        "                    translations[lang] = \"\"\n",
        "            \n",
        "            # Find approximate positions in text for overlap detection\n",
        "            # Using case-insensitive search to find the term in text\n",
        "            text_lower = text.lower()\n",
        "            term_lower = match_term.lower()\n",
        "            \n",
        "            # Try to find the exact match position\n",
        "            start_pos = text_lower.find(term_lower)\n",
        "            if start_pos != -1:\n",
        "                end_pos = start_pos + len(match_term)\n",
        "            else:\n",
        "                # If exact match not found, use fuzzy position estimation\n",
        "                # Split text into words and try to find approximate position\n",
        "                words = text.split()\n",
        "                best_match_idx = 0\n",
        "                best_score = 0\n",
        "                \n",
        "                term_words = match_term.split()\n",
        "                term_length = len(term_words)\n",
        "                \n",
        "                # Search for best matching position using sliding window\n",
        "                for i in range(len(words) - term_length + 1):\n",
        "                    window_text = \" \".join(words[i:i + term_length])\n",
        "                    window_score = fuzz.token_set_ratio(window_text.lower(), term_lower)\n",
        "                    if window_score > best_score:\n",
        "                        best_score = window_score\n",
        "                        best_match_idx = i\n",
        "                \n",
        "                # Calculate approximate positions based on best match\n",
        "                if best_match_idx < len(words):\n",
        "                    words_before = \" \".join(words[:best_match_idx])\n",
        "                    start_pos = len(words_before) + (1 if words_before else 0)\n",
        "                    \n",
        "                    matched_words = words[best_match_idx:best_match_idx + term_length]\n",
        "                    end_pos = start_pos + len(\" \".join(matched_words))\n",
        "                else:\n",
        "                    start_pos = 0\n",
        "                    end_pos = len(match_term)\n",
        "            \n",
        "            # Extract the actual text segment that was matched\n",
        "            if start_pos >= 0 and end_pos <= len(text):\n",
        "                found_text = text[start_pos:end_pos]\n",
        "            else:\n",
        "                found_text = match_term  # Fallback to the glossary term\n",
        "            \n",
        "            all_matches.append({\n",
        "                \"found_in_text\": found_text,\n",
        "                \"best_fuzzy\": match_term,\n",
        "                \"score\": similarity,\n",
        "                \"result\": translations,\n",
        "                \"start\": start_pos,\n",
        "                \"end\": end_pos\n",
        "            })\n",
        "        \n",
        "        # Remove overlapping matches (prefer longer and higher scoring matches)\n",
        "        final_matches = self._remove_overlapping_matches(all_matches)\n",
        "        \n",
        "        # Remove position information from final output and sort by score\n",
        "        result_matches = []\n",
        "        for match in final_matches:\n",
        "            result_match = {k: v for k, v in match.items() if k not in ['start', 'end']}\n",
        "            result_matches.append(result_match)\n",
        "        \n",
        "        # Sort by score (highest first)\n",
        "        result_matches.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        \n",
        "        return result_matches\n",
        "\n",
        "    def find_nearly_exact_english_matches(\n",
        "        self,\n",
        "        text: str,\n",
        "        target_languages: List[str],\n",
        "        score_cutoff: float = 95.0,\n",
        "        normalize_text: bool = True,\n",
        "        remove_overlaps: bool = True\n",
        "    ) -> List[Dict[str, Union[str, Dict[str, str]]]]:\n",
        "        \"\"\"\n",
        "        Find nearly-exact matches for English glossary terms in the given text.\n",
        "        Optionally normalizes the text and glossary terms before matching.\n",
        "        \n",
        "        Args:\n",
        "            text: Input English text\n",
        "            target_languages: List of target languages to return translations\n",
        "            score_cutoff: Minimum similarity score (default: 95.0)\n",
        "            normalize_text: Whether to normalize text and terms (default: True)\n",
        "            remove_overlaps: Whether to remove overlapping matches (default: True)\n",
        "        \n",
        "        Returns:\n",
        "            List of dictionaries with found matches and their translations\n",
        "        \"\"\"\n",
        "        def normalize(s):\n",
        "            s = str(s).strip()\n",
        "            # Remove numerical substring at the start and its trailing space and punctuation\n",
        "            s = re.sub(r'^\\d+\\s*', '', s) # Example: \"123 term\" -> \"term\", \"123. term\" -> \"term\"\n",
        "            s = re.sub(r'[^\\w\\s]', '', s) # Remove punctuation\n",
        "            s = re.sub(r'\\s+', ' ', s) # Normalize whitespace\n",
        "            return s\n",
        "        \n",
        "        if \"English\" not in self.languages:\n",
        "            raise ValueError(\"English language not available in glossary.\")\n",
        "        \n",
        "        invalid_targets = [lang for lang in target_languages if lang not in self.languages]\n",
        "        if invalid_targets:\n",
        "            raise ValueError(f\"Invalid target languages: {invalid_targets}. Available: {self.languages}\")\n",
        "        \n",
        "        # Get all English terms\n",
        "        english_terms = self.glossary[\"English\"].dropna().tolist()\n",
        "        if not english_terms:\n",
        "            return []\n",
        "        \n",
        "        # Sort terms by length (longest first) for better matching priority\n",
        "        english_terms_sorted = sorted(english_terms, key=len, reverse=True)\n",
        "        \n",
        "        all_matches = []\n",
        "        \n",
        "        # For each glossary term, try to find it in the text\n",
        "        for orig_term in english_terms_sorted:\n",
        "            # Normalize term if requested\n",
        "            if normalize_text:\n",
        "                search_term = normalize(orig_term)\n",
        "                search_text = normalize(text)\n",
        "            else:\n",
        "                search_term = orig_term.strip()\n",
        "                search_text = text.strip()\n",
        "            \n",
        "            # Split into words for position tracking\n",
        "            term_words = search_term.split()\n",
        "            if not term_words:\n",
        "                continue\n",
        "            \n",
        "            # Find all possible matches in the text\n",
        "            text_words = search_text.split()\n",
        "            term_length = len(term_words)\n",
        "            \n",
        "            for i in range(len(text_words) - term_length + 1):\n",
        "                # Get n-gram from text\n",
        "                ngram_words = text_words[i:i + term_length]\n",
        "                ngram_text = \" \".join(ngram_words)\n",
        "                \n",
        "                # Calculate similarity using token_set_ratio for better subset matching\n",
        "                score = fuzz.token_set_ratio(ngram_text, search_term)\n",
        "                \n",
        "                if score >= score_cutoff:\n",
        "                    # Find positions in original text\n",
        "                    # This is approximate since we're working with normalized text\n",
        "                    original_words = text.split()\n",
        "                    if i < len(original_words) and i + term_length <= len(original_words):\n",
        "                        # Get the original text segment\n",
        "                        original_segment = \" \".join(original_words[i:i + term_length])\n",
        "                        \n",
        "                        # Estimate positions (approximate)\n",
        "                        start_pos = text.lower().find(original_segment.lower())\n",
        "                        if start_pos == -1:\n",
        "                            # Fallback: use word-based estimation\n",
        "                            words_before = \" \".join(original_words[:i])\n",
        "                            start_pos = len(words_before) + (1 if words_before else 0)\n",
        "                        end_pos = start_pos + len(original_segment)\n",
        "                        \n",
        "                        # Get translations\n",
        "                        match_row = self.glossary[self.glossary[\"English\"] == orig_term].iloc[0]\n",
        "                        translations = {}\n",
        "                        for lang in target_languages:\n",
        "                            translation = match_row[lang]\n",
        "                            if pd.notna(translation):\n",
        "                                translations[lang] = str(translation)\n",
        "                            else:\n",
        "                                translations[lang] = \"\"\n",
        "                        \n",
        "                        all_matches.append({\n",
        "                            \"found_in_text\": original_segment,\n",
        "                            \"best_fuzzy\": orig_term,\n",
        "                            \"score\": score,\n",
        "                            \"result\": translations,\n",
        "                            \"start\": start_pos,\n",
        "                            \"end\": end_pos\n",
        "                        })\n",
        "        \n",
        "        # Remove overlapping matches if requested\n",
        "        if remove_overlaps:\n",
        "            final_matches = self._remove_overlapping_matches(all_matches)\n",
        "        else:\n",
        "            final_matches = all_matches\n",
        "        \n",
        "        # Remove position information from final output and sort by score\n",
        "        result_matches = []\n",
        "        for match in final_matches:\n",
        "            result_match = {k: v for k, v in match.items() if k not in ['start', 'end']}\n",
        "            result_matches.append(result_match)\n",
        "        \n",
        "        # Sort by score (highest first)\n",
        "        result_matches.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        \n",
        "        return result_matches\n",
        "\n",
        "\n",
        "def create_processor_function(processor_type: str) -> Optional[Callable]:\n",
        "    \"\"\"\n",
        "    Create a processor function based on the specified type.\n",
        "    \n",
        "    Args:\n",
        "        processor_type: Type of processor (\"none\", \"default\", \"custom\")\n",
        "        \n",
        "    Returns:\n",
        "        Processor function or None\n",
        "    \"\"\"\n",
        "    if processor_type == \"none\":\n",
        "        return None\n",
        "    elif processor_type == \"default\":\n",
        "        return utils.default_process\n",
        "    elif processor_type == \"custom\":\n",
        "        # Custom processor that handles special cases\n",
        "        def custom_processor(text):\n",
        "            if not text:\n",
        "                return \"\"\n",
        "            # Remove extra whitespace, keep alphanumeric and spaces\n",
        "            processed = re.sub(r'^\\d+\\s*', '', str(text).strip()) # Example: \"123 term\" -> \"term\", \"123. term\" -> \"term\"\n",
        "            #processed = re.sub(r'[^\\w\\s]', '', processed) # Remove punctuation\n",
        "            processed = re.sub(r'\\s+', ' ', processed)\n",
        "            return processed\n",
        "        return custom_processor\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown processor type: {processor_type}\")\n",
        "\n",
        "\n",
        "# Example usage and testing functions\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example usage of the MultilingualGlossaryProcessor.\n",
        "    \"\"\"\n",
        "    # Initialize processor (assuming you have a glossary file)\n",
        "    processor = MultilingualGlossaryProcessor(\"data/glossaryUNEP_corrected.xlsx\")\n",
        "    \n",
        "    # Example 1: Find best fuzzy match\n",
        "    result1 = processor.find_best_fuzzy_match(\n",
        "        query=\"UN Environment Program\",\n",
        "        source_language=\"English\",\n",
        "        target_languages=[\"French\", \"Spanish\", \"Arabic\"],\n",
        "        scorer=fuzz.WRatio,\n",
        "        processor=utils.default_process,\n",
        "        score_cutoff=70.0,\n",
        "        process_method=\"extractOne\"\n",
        "    )\n",
        "    print(\"Best match result:\", result1)\n",
        "    \n",
        "    # Example 2: Find all matches in text with overlap handling\n",
        "    text = \"UN Environment Programme is sponsored by UNESCO.\"\n",
        "    result2 = processor.find_all_fuzzy_matches_in_text(\n",
        "        text=text,\n",
        "        source_language=\"English\",\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        processor=utils.default_process,\n",
        "        score_cutoff=80.0\n",
        "    )\n",
        "    print(f\"All matches in text (no overlaps), {len(result2)} results in total:\", result2)\n",
        "    \n",
        "    # Example 3: Nearly-exact English matches with normalization\n",
        "    english_text = \"The UN Environment Programme and UNEA are working on the International Day of Women Judge with organizations and developing new policies.\"\n",
        "    result3 = processor.find_nearly_exact_english_matches(\n",
        "        text=english_text,\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        score_cutoff=95.0,\n",
        "        normalize_text=True,\n",
        "        remove_overlaps=True\n",
        "    )\n",
        "    print(f\"Nearly-exact English matches, {len(result3)} in total:\", result3)\n",
        "    \n",
        "    # Example 4: More complex text\n",
        "    complex_text = \"This year, the United Nations Environment Programme (UNEP) and the SSC (South-South cooperation) are presiding the COP on Climate Change to address persistent organic pollutants before the UNEA7 with FAO and UNESCO, where the 1. total greenhouse gas emissions per year indicator is expected to be reduced by 50%.\"\n",
        "    result4 = processor.find_all_fuzzy_matches_in_text(\n",
        "        text=complex_text,\n",
        "        source_language=\"English\",\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        processor=utils.default_process,\n",
        "        score_cutoff=95.0\n",
        "    )\n",
        "    print(f\"Complex text matches RESULT #4, {len(result4)} results in total:\", result4)\n",
        "\n",
        "    # Example 5: Nearly-exact English matches with normalization\n",
        "    english_text = complex_text\n",
        "    result5 = processor.find_nearly_exact_english_matches(\n",
        "        text=english_text,\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        score_cutoff=90.0,\n",
        "        normalize_text=True,\n",
        "        remove_overlaps=True\n",
        "    )\n",
        "    print(f\"Nearly-exact English matches: {len(result5)} matches in total: \", result5)\n",
        "    # print set of best_fuzzy and result['Spanish'] of result5\n",
        "    bilingual_pairs = [(match['best_fuzzy'], match['result'].get('Spanish', '')) for match in result5]\n",
        "    print(\"Bilingual pairs (best_fuzzy, Spanish translation):\")\n",
        "    for pair in bilingual_pairs:\n",
        "        print(pair)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_usage()\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxqtfbGuUY2b"
      },
      "source": [
        "# TM fuzzy matches TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXaOfYnwxO1S",
        "outputId": "70e5b823-d217-4db7-fc53-6e4649d6b15a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of fuzzy source and fuzzy target prefixes:\n",
            "10\n",
            "10\n",
            "The United Nations Environment Programme (UNEP) and the Food and Agriculture Organization (FAO) have named the first World Restoration Flagships for this year, which address ecosystem degradation across the globe.\n",
            "El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la degradación de los ecosistemas en todo el planeta.\n"
          ]
        }
      ],
      "source": [
        "similar_text = \"\"\"El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la degradación de los ecosistemas en todo el planeta.\n",
        "Estas iniciativas han estado restaurando alrededor tres millones de hectáreas de ecosistemas marinos, un área del tamaño de El Salvador.\n",
        "Las siete nuevas Iniciativas Emblemáticas comprenden iniciativas de restauración en Ecuador, Colombia, Kenya e Indonesia.\n",
        ".\n",
        "\"Por mucho tiempo se ha dado por sentado el poder de los bosques, tan esenciales para la restauración.\n",
        "Cada persona debe cumplir su parte\", afirmó Inger Andersen, Directora Ejecutiva del PNUMA.\n",
        "\"Las Iniciativas Emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
        "Para lograr nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
        "El Director General de la FAO, QU Dongyu, manifestó: \"La crisis climática, las prácticas de explotación insostenible y la reducción de los recursos naturales están afectando nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades.\n",
        "Estas nuevas 7 Iniciativas Emblemáticas muestran que detener y revertir la degradación es posible y beneficioso para el planeta y las personas\".\"\"\"\n",
        "fuzzy_sents = [sent.strip() for sent in similar_text.split(\"\\n\")]\n",
        "fuzzy_target_prefixes = [sent.strip() for sent in fuzzy_sents if sent.strip()]\n",
        "\n",
        "similar_text_en = \"\"\"The United Nations Environment Programme (UNEP) and the Food and Agriculture Organization (FAO) have named the first World Restoration Flagships for this year, which address ecosystem degradation across the globe.\n",
        "These initiatives have been restoring around three million hectares of marine ecosystems, an area the size of El Salvador.\n",
        "The seven new flagships include restoration initiatives in Ecuador, Colombia, Kenya and Indonesia.\n",
        ".\n",
        "\"The power of forests, so essential to restoration, has long been taken for granted.\n",
        "Everyone must do their part,\" said Inger Andersen, Executive Director of UNEP.\n",
        "\"The World Restoration Flagships show how biodiversity protection, climate action and economic development are deeply interconnected.\n",
        "To achieve our restoration goals, our ambition must be as big as the ocean we must protect.\"\n",
        "FAO Director-General QU Dongyu said, \"The climate crisis, unsustainable exploitation practices and depletion of natural resources are affecting our blue ecosystems, damaging marine life and threatening the livelihoods of communities.\n",
        "These new 7 flagships show that halting and reversing degradation is possible and beneficial for the planet and people.\"\n",
        "\"\"\"\n",
        "\n",
        "#glossary_entry = [\"World Restoration Flagships\", \"Iniciativas Emblemáticas de la Restauración Mundial\"] #plural\n",
        "#glossary_entry = [\"World Restoration Flagship\", \"Iniciativa Emblemática de la Restauración Mundial\"] #singular\n",
        "#glossary_entry = [\"world restoration flagship\", \"iniciativa emblemática de la restauración mundial\"] #singular_lowercase\n",
        "\n",
        "fuzzy_src_sents = [sent.strip() for sent in similar_text_en.split(\"\\n\")]\n",
        "fuzzy_source_sentences = [sent.strip() for sent in fuzzy_src_sents if sent.strip()]\n",
        "\n",
        "# Replace first and second elements of source and target texts with the glossary entry\n",
        "#fuzzy_source_sentences[0] = glossary_entry[0]\n",
        "#fuzzy_target_prefixes[0] = glossary_entry[1]\n",
        "\n",
        "print(\"Length of fuzzy source and fuzzy target prefixes:\")\n",
        "print(len(fuzzy_source_sentences))\n",
        "print(len(fuzzy_target_prefixes))\n",
        "\n",
        "print(fuzzy_source_sentences[0])\n",
        "print(fuzzy_target_prefixes[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Glossary fuzzy matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Spanish'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tgt_lang_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matches found for source sentence 'The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.': 4 - [('pollution', 'contaminación'), ('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial'), ('UN Environment Programme', 'Programa ONU Medio Ambiente'), ('invasive alien species', 'Especie exótica invasiva')]\n",
            "Matches found for source sentence 'These initiatives are restoring almost five million hectares of marine ecosystems - an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.': 4 - [('marine ecosystems', 'ecosistemas marinos'), ('Costa Rica', 'Costa Rica'), ('France', 'Francia'), ('conference', 'conferencias')]\n",
            "Matches found for source sentence 'The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.': 7 - [('restoration', 'Restauración'), ('Mozambique', 'Mozambique'), ('Spain', 'España'), ('Mexico', 'México'), ('island', 'islas'), ('Europe', 'Europa'), ('forest ecosystem', 'ecosistemas forestales')]\n",
            "Matches found for source sentence 'The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.': 2 - [('France', 'Francia'), ('conference', 'conferencias')]\n",
            "Matches found for source sentence '\"After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.': 2 - [('ocean', 'océanos'), ('restoration', 'Restauración')]\n",
            "Matches found for source sentence 'But the challenge ahead of us is significant and we need everyone to play their part,\" said Inger Andersen, Executive Director of UNEP.': 2 - [('Executive Director', 'Director Ejecutivo'), ('UNEP', 'PNUMA')]\n",
            "Matches found for source sentence '\"These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.': 4 - [('biodiversity protection', 'protección de la diversidad biológica'), ('climate', 'clima'), ('economic development', 'desarrollo económico'), ('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial')]\n",
            "Matches found for source sentence 'To deliver our restoration goals, our ambition must be as big as the ocean we must protect.\"': 2 - [('restoration', 'Restauración'), ('ocean', 'océanos')]\n",
            "Matches found for source sentence 'FAO Director-General QU Dongyu said: \"The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.': 4 - [('climate', 'clima'), ('ecosystems', 'ecosistemas'), ('life', 'vida'), ('livelihood', 'Medio de vida')]\n",
            "Matches found for source sentence 'These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"': 3 - [('degradation', 'degradación'), ('planet', 'planetas'), ('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial')]\n",
            "Matches found for source sentence 'The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration - led by UNEP and FAO - which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.': 7 - [('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial'), ('UNEP', 'PNUMA'), ('degradation', 'degradación'), ('ecosystems', 'ecosistemas'), ('ocean', 'océanos'), ('award', 'premios'), ('ecosystem restoration', 'restauración de los ecosistemas')]\n",
            "Matches found for source sentence 'The awards track notable initiatives that support global commitments to restore one billion hectares - an area larger than China - by 2030.': 2 - [('China', 'China'), ('award', 'premios')]\n"
          ]
        }
      ],
      "source": [
        "processor = MultilingualGlossaryProcessor(\"data/glossaryUNEP_corrected.xlsx\")\n",
        "\n",
        "glossary_matches = []\n",
        "\n",
        "for src_sent in source_sents:\n",
        "    sent_matches = processor.find_nearly_exact_english_matches(\n",
        "        text=src_sent,\n",
        "        target_languages=[tgt_lang_name],\n",
        "        score_cutoff=90.0,\n",
        "        normalize_text=True,\n",
        "        remove_overlaps=True\n",
        "    )\n",
        "\n",
        "    # filter sent_matches as a list of str of best_fuzzy and results['Spanish'] if both are not empty\n",
        "    sent_matches = [(match['best_fuzzy'], match['result'].get(tgt_lang_name, '')) for match in sent_matches if 'best_fuzzy' in match and tgt_lang_name in match['result']]\n",
        "    # remove set in sent_matches if best_fuzzy is empty or Spanish translation is empty\n",
        "    sent_matches = [match for match in sent_matches if match[0] and match[1]]\n",
        "    if sent_matches:\n",
        "        print(f\"Matches found for source sentence '{src_sent}': {len(sent_matches)}\"\n",
        "              f\" - {sent_matches}\")\n",
        "        glossary_matches.append(sent_matches)\n",
        "    else:\n",
        "        # add empty tuple if no matches found\n",
        "        print(f\"No matches found for source sentence '{src_sent}'\")\n",
        "        glossary_matches.append((\"\", \"\"))\n",
        "\n",
        "# Transform glossary_matches into a list of tuples (A, B) where A is joined string from first elements of each tuple in glossary_matches and B is joined string from second elements of each tuple in glossary_matches\n",
        "glossary_matches = [(\", \".join([match[0] for match in matches]),\n",
        "                                 \", \".join([match[1] for match in matches])) for matches in glossary_matches]\n",
        "\n",
        "# separate the glossary_matches into two lists: first elements and second elements\n",
        "glossary_matches_src = [match[0] for match in glossary_matches]\n",
        "glossary_matches_tgt = [match[1] for match in glossary_matches]\n",
        "\n",
        "fuzzy_source_sentences = glossary_matches_src\n",
        "fuzzy_target_prefixes = glossary_matches_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translation inserting matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpxNHl8GF4F8",
        "outputId": "e6ee9b4b-5b0e-4d36-e0a1-819fa7cc1214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['eng_Latn', '▁pollu', 'tion', ',', '▁World', '▁Rest', 'oration', '▁Flag', 'ship', ',', '▁UN', '▁Environment', '▁Programme', ',', '▁invasi', 've', '▁alien', '▁species', 'eng_Latn', '▁•', '▁The', '▁UN', '▁Environment', '▁Programme', '▁(', 'UN', 'EP', ')', '▁and', '▁the', '▁Food', '▁and', '▁Agric', 'ulture', '▁Organization', '▁of', '▁the', '▁UN', '▁(', 'FA', 'O', ')', '▁have', '▁named', '▁the', '▁first', '▁World', '▁Rest', 'oration', '▁Flag', 'shi', 'ps', '▁for', '▁this', '▁year', ',', '▁tack', 'ling', '▁pollu', 'tion', ',', '▁uns', 'usta', 'inable', '▁explo', 'itation', ',', '▁and', '▁invasi', 've', '▁species', '▁in', '▁three', '▁contin', 'ents', '.', '</s>']\n",
            "['spa_Latn', '▁contamina', 'ción', ',', '▁Inici', 'ativa', '▁Emb', 'lem', 'ática', '▁de', '▁la', '▁Resta', 'uración', '▁Mundial', ',', '▁Programa', '▁ONU', '▁Medio', '▁Ambiente', ',', '▁Es', 'pe', 'cie', '▁ex', 'ó', 'tica', '▁invasi', 'va', 'spa_Latn', '▁•']\n",
            "\n",
            "Translations:\n",
            "contaminación, Iniciativa Emblemática de la Restauración Mundial, Programa ONU Medio Ambiente, Especie exótica invasivaspa_Latn • El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "ecosistemas marinos, Costa Rica, Francia, conferenciasspa_Latn • Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área del tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre el Océano.\n",
            "Restauración, Mozambique, España, México, islas, Europa, ecosistemas forestalesspa_Latn • Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del Canal de Mozambique del Norte, rica en corales, más de 60 islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "Francia, conferenciasspa_Latn • Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para recibir el apoyo de las Naciones Unidas.\n",
            "océanos, Restauraciónspa_Latn • \"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "Director Ejecutivo, PNUMAspa_Latn • Pero el desafío que nos espera es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "protección de la diversidad biológica, clima, desarrollo económico, Iniciativa Emblemática de la Restauración Mundialspa_Latn • \"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "Restauración, océanosspa_Latn • Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "clima, ecosistemas, vida, Medio de vidaspa_Latn • El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "degradación, planetas, Iniciativa Emblemática de la Restauración Mundialspa_Latn • Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n",
            "\n",
            "Translations only:\n",
            "• El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "• Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área del tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre el Océano.\n",
            "• Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del Canal de Mozambique del Norte, rica en corales, más de 60 islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "• Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para recibir el apoyo de las Naciones Unidas.\n",
            "• \"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "• Pero el desafío que nos espera es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "• \"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "• Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "• El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "• Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n"
          ]
        }
      ],
      "source": [
        "import ctranslate2\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "\n",
        "# src_lang = \"eng_Latn\"\n",
        "# tgt_lang = \"spa_Latn\"\n",
        "\n",
        "beam_size = 2\n",
        "\n",
        "# Load the source SentecePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(sp_model_path)\n",
        "\n",
        "\n",
        "# Subword the source sentences\n",
        "fuzzy_source_sentences_subworded = sp.encode_as_pieces(fuzzy_source_sentences)\n",
        "real_source_sentences_subworded = sp.encode_as_pieces(source_sents)\n",
        "fuzzy_real_subworded = zip(fuzzy_source_sentences_subworded, real_source_sentences_subworded)\n",
        "\n",
        "separator = sp.encode_as_pieces(\"•\")  # tokenize \"•\" -- output is \"▁•\"\n",
        "\n",
        "source_sents_subworded = [[src_lang] + fuzzy_src + [src_lang] + separator + real_src + [\"</s>\"]\n",
        "                          for fuzzy_src, real_src in fuzzy_real_subworded]\n",
        "#source_sents_subworded = [[src_lang] + fuzzy_src + [src_lang] + separator + [\"</s>\"]\n",
        "                          #for fuzzy_src in fuzzy_source_sentences_subworded]\n",
        "print(source_sents_subworded[0])\n",
        "\n",
        "prefixes_subworded = sp.encode_as_pieces(fuzzy_target_prefixes)\n",
        "target_prefixes = [[tgt_lang] + sent + [tgt_lang] + separator for sent in prefixes_subworded]\n",
        "print(target_prefixes[0])\n",
        "\n",
        "# Translate the source sentences\n",
        "translator = ctranslate2.Translator(ct_model_path, device=device)\n",
        "translations = translator.translate_batch(source_sents_subworded,\n",
        "                                          batch_type=\"tokens\",\n",
        "                                          max_batch_size=2024,\n",
        "                                          beam_size=beam_size,\n",
        "                                          min_decoding_length=2,\n",
        "                                          max_decoding_length=512,\n",
        "                                          target_prefix=target_prefixes)\n",
        "translations = [translation.hypotheses[0] for translation in translations]\n",
        "\n",
        "# Desubword the target sentences\n",
        "translations_desubword = sp.decode(translations)\n",
        "translations_desubword = [sent[len(tgt_lang):].strip() for sent in translations_desubword]\n",
        "\n",
        "translations_only = [sent.split(tgt_lang)[1].strip() for sent in translations_desubword]\n",
        "\n",
        "print(\"\\nTranslations:\", *translations_desubword[:10], sep=\"\\n\")\n",
        "print(\"\\nTranslations only:\", *translations_only[:10], sep=\"\\n\")\n",
        "\n",
        "# Remove bullet points and leading/trailing whitespace from translations_only\n",
        "translations_only = [sent[1:].strip() if sent.startswith(\"•\") else sent.strip() for sent in translations_only]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQEk9087q0Ml"
      },
      "outputs": [],
      "source": [
        "# # Save the translations\n",
        "\n",
        "# translations_file_name = \"testUNEP.es\"\n",
        "\n",
        "# with open(translations_file_name, \"w+\") as output:\n",
        "#   for translation in translations_only:\n",
        "#     output.write(translation + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJyoXuaY8qqe"
      },
      "source": [
        "# Show in parallel print each line of testUNEP.en and testUNEP.es\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cunliud8-7S",
        "outputId": "44b49a1e-1b6f-4284-c125-d149ab7e9f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "12\n",
            "contaminación, Iniciativa Emblemática de la Restauración Mundial, Programa ONU Medio Ambiente, Especie exótica invasivaspa_Latn • El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.\n",
            "\n",
            "ecosistemas marinos, Costa Rica, Francia, conferenciasspa_Latn • Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área del tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre el Océano.\n",
            "Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área del tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre el Océano.\n",
            "These initiatives are restoring almost five million hectares of marine ecosystems - an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.\n",
            "\n",
            "Restauración, Mozambique, España, México, islas, Europa, ecosistemas forestalesspa_Latn • Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del Canal de Mozambique del Norte, rica en corales, más de 60 islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del Canal de Mozambique del Norte, rica en corales, más de 60 islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.\n",
            "\n",
            "Francia, conferenciasspa_Latn • Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para recibir el apoyo de las Naciones Unidas.\n",
            "Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para recibir el apoyo de las Naciones Unidas.\n",
            "The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.\n",
            "\n",
            "océanos, Restauraciónspa_Latn • \"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "\"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "\"After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.\n",
            "\n",
            "Director Ejecutivo, PNUMAspa_Latn • Pero el desafío que nos espera es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "Pero el desafío que nos espera es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "But the challenge ahead of us is significant and we need everyone to play their part,\" said Inger Andersen, Executive Director of UNEP.\n",
            "\n",
            "protección de la diversidad biológica, clima, desarrollo económico, Iniciativa Emblemática de la Restauración Mundialspa_Latn • \"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "\"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "\"These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.\n",
            "\n",
            "Restauración, océanosspa_Latn • Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "To deliver our restoration goals, our ambition must be as big as the ocean we must protect.\"\n",
            "\n",
            "clima, ecosistemas, vida, Medio de vidaspa_Latn • El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "FAO Director-General QU Dongyu said: \"The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.\n",
            "\n",
            "degradación, planetas, Iniciativa Emblemática de la Restauración Mundialspa_Latn • Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n",
            "Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n",
            "These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"\n",
            "\n",
            "Iniciativa Emblemática de la Restauración Mundial, PNUMA, degradación, ecosistemas, océanos, premios, restauración de los ecosistemasspa_Latn • Los Premios emblemáticos de la Restauración Mundial forman parte de la Década de las Naciones Unidas para la Restauración de los Ecosistemas - liderada por el PNUMA y la FAO - que tiene como objetivo prevenir, detener y revertir la degradación de los ecosistemas en todos los continentes y en todos los océanos.\n",
            "Los Premios emblemáticos de la Restauración Mundial forman parte de la Década de las Naciones Unidas para la Restauración de los Ecosistemas - liderada por el PNUMA y la FAO - que tiene como objetivo prevenir, detener y revertir la degradación de los ecosistemas en todos los continentes y en todos los océanos.\n",
            "The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration - led by UNEP and FAO - which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.\n",
            "\n",
            "China, premiosspa_Latn • Los premios siguen iniciativas notables que respaldan los compromisos globales para restaurar mil millones de hectáreas - un área más grande que China - para 2030.\n",
            "Los premios siguen iniciativas notables que respaldan los compromisos globales para restaurar mil millones de hectáreas - un área más grande que China - para 2030.\n",
            "The awards track notable initiatives that support global commitments to restore one billion hectares - an area larger than China - by 2030.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# translations_desubword and translations_only\n",
        "print(len(translations_desubword))\n",
        "print(len(translations_only))\n",
        "\n",
        "for i in range(len(translations_desubword)):\n",
        "  print(translations_desubword[i])\n",
        "  if i > len(translations_only) - 1:\n",
        "    print()\n",
        "  else:\n",
        "    print(translations_only[i])\n",
        "    print(source_sents[i])\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
