{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWyQ4MhiQmqS"
      },
      "source": [
        "# Translation with NLLB-200 using CTranslate2\n",
        "\n",
        "This notebook is part of the repository [Adaptive-MT-LLM-Fine-tuning](https://github.com/ymoslem/Adaptive-MT-LLM-Fine-tuning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnh5nkfxQsyb"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "7jIwEIkCQe1b",
        "outputId": "5d2ff024-b425-4f6b-ce40-2c032945332f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Nelso\\\\Documents\\\\Glossary-NMT-LLM'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_path = \"data\"\n",
        "src_lang = \"spa_Latn\"\n",
        "tgt_lang = \"eng_Latn\"\n",
        "\n",
        "lang_dict = { \"spa_Latn\": \"Spanish\", \"eng_Latn\": \"English\" } # TO EXTEND\n",
        "\n",
        "#directory = os.path.join(data_path, \"spanish\")\n",
        "\n",
        "#os.chdir(directory)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial number of rows in the glossary: 7279\n",
            "Removed shape: (1, 13)\n",
            "┌────────┬─────────┬────────┬────────┬───┬───────┬───────┬───────┬───────┐\n",
            "│ itemID ┆ English ┆ Arabic ┆ French ┆ … ┆ FRalt ┆ SPalt ┆ CHalt ┆ RUalt │\n",
            "│ ---    ┆ ---     ┆ ---    ┆ ---    ┆   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
            "│ u32    ┆ u32     ┆ u32    ┆ u32    ┆   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
            "╞════════╪═════════╪════════╪════════╪═══╪═══════╪═══════╪═══════╪═══════╡\n",
            "│ 0      ┆ 0       ┆ 0      ┆ 0      ┆ … ┆ 0     ┆ 0     ┆ 0     ┆ 0     │\n",
            "└────────┴─────────┴────────┴────────┴───┴───────┴───────┴───────┴───────┘ cells from the glossary. New count of NaN cells: shape: (1, 13)\n",
            "┌────────┬─────────┬────────┬────────┬───┬───────┬───────┬───────┬───────┐\n",
            "│ itemID ┆ English ┆ Arabic ┆ French ┆ … ┆ FRalt ┆ SPalt ┆ CHalt ┆ RUalt │\n",
            "│ ---    ┆ ---     ┆ ---    ┆ ---    ┆   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
            "│ u32    ┆ u32     ┆ u32    ┆ u32    ┆   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
            "╞════════╪═════════╪════════╪════════╪═══╪═══════╪═══════╪═══════╪═══════╡\n",
            "│ 0      ┆ 1       ┆ 4      ┆ 1      ┆ … ┆ 5475  ┆ 5572  ┆ 5901  ┆ 5007  │\n",
            "└────────┴─────────┴────────┴────────┴───┴───────┴───────┴───────┴───────┘\n",
            "Number of rows in the glossary after removing NaN cells: 7279\n"
          ]
        }
      ],
      "source": [
        "# Load Glossary\n",
        "import os\n",
        "glossary_file = \"glossary_UNEP_202505.xlsx\"\n",
        "glossary_file = \"UNBIS.xlsx\"\n",
        "glossary_path = os.path.join(\"data\", glossary_file)\n",
        "\n",
        "import polars as pl\n",
        "#glossary_df = pl.read_excel(glossary_path, sheet_name=\"CombinedGlossaries\")\n",
        "glossary_df = pl.read_excel(glossary_path)\n",
        "\n",
        "glossary_remove_list = [\"n/a\", \"N/A\", \"NaN\", \"nan\", \"None\", \"none\", \"null\", \"Null\", \"(Not provided)\", \"Not provided\", \"not provided\", \"N/A (not applicable)\", \"-\"]\n",
        "count_rows = glossary_df.shape[0]\n",
        "print(f\"Initial number of rows in the glossary: {count_rows}\")\n",
        "# Set NaN to cells (any column) in the glossary df that are in the remove list\n",
        "count_remove = glossary_df.select(pl.all().is_in(glossary_remove_list)).sum().sum()\n",
        "glossary_df = glossary_df.with_columns([\n",
        "    pl.when(pl.col(col).is_in(glossary_remove_list))\n",
        "    .then(None)\n",
        "    .otherwise(pl.col(col))\n",
        "    .alias(col)\n",
        "    for col in glossary_df.columns\n",
        "])\n",
        "new_count_remove = glossary_df.select(pl.all().is_null()).sum().sum()\n",
        "print(f\"Removed {count_remove} cells from the glossary. New count of NaN cells: {new_count_remove}\")\n",
        "\n",
        "count_rows_after = glossary_df.shape[0]\n",
        "print(f\"Number of rows in the glossary after removing NaN cells: {count_rows_after}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform uppercase into lowercase and correct if wrong spelling (missing diactricis) for Spanish, Russian, French\n",
        "\n",
        "#!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing spell checker:\n",
            "{'original': 'hello', 'is_correct': True, 'correction': None, 'candidates': []}\n",
            "{'original': 'hapenning', 'is_correct': False, 'correction': 'happening', 'candidates': ['japanning', 'happening', 'apennine', 'penning']}\n",
            "{'original': 'africa', 'is_correct': False, 'correction': 'áfrica', 'candidates': ['áfrica']}\n",
            "{'original': 'manana', 'is_correct': False, 'correction': 'mañana', 'candidates': ['canana', 'mañana', 'manada', 'mangana', 'macana']}\n"
          ]
        }
      ],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "def word_spell_check(word, lang):\n",
        "    \"\"\"\n",
        "    Check spelling of a word in specified language and return correction if needed.\n",
        "    \n",
        "    Args:\n",
        "        word (str): Word to check\n",
        "        lang (str): Language column name from glossary_df \n",
        "                   (English, Arabic, French, Spanish, Chinese, Russian, Portuguese, Swahili)\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            'original': original word,\n",
        "            'is_correct': boolean,\n",
        "            'correction': suggested correction or None,\n",
        "            'candidates': list of alternative suggestions\n",
        "        }\n",
        "    \"\"\"\n",
        "    \n",
        "    # Language mapping from glossary column names to pyspellchecker language codes\n",
        "    lang_mapping = {\n",
        "        'English': 'en',\n",
        "        'Arabic': 'ar', \n",
        "        'French': 'fr',\n",
        "        'Spanish': 'es',\n",
        "        'Chinese': None,  # Not supported by pyspellchecker\n",
        "        'Russian': 'ru',\n",
        "        'Portuguese': 'pt',\n",
        "        'Swahili': None   # Not supported by pyspellchecker\n",
        "    }\n",
        "    \n",
        "    # Handle empty or None input - corrected condition\n",
        "    if not word or word is None or (isinstance(word, str) and word.strip() == \"\"):\n",
        "        return {\n",
        "            'original': word,\n",
        "            'is_correct': None,\n",
        "            'correction': None,\n",
        "            'candidates': []\n",
        "        }\n",
        "    \n",
        "    # Get language code\n",
        "    lang_code = lang_mapping.get(lang)\n",
        "    \n",
        "    # Handle unsupported languages\n",
        "    if lang_code is None:\n",
        "        return {\n",
        "            'original': word,\n",
        "            'is_correct': None,\n",
        "            'correction': f\"Language '{lang}' not supported by spell checker\",\n",
        "            'candidates': []\n",
        "        }\n",
        "    \n",
        "    try:\n",
        "        # Initialize spell checker for the specific language\n",
        "        spell = SpellChecker(language=lang_code)\n",
        "        \n",
        "        # Clean the word (remove extra spaces, convert to lowercase for checking)\n",
        "        clean_word = word.strip().lower()\n",
        "        \n",
        "        # Check if word is correct\n",
        "        is_correct = clean_word in spell\n",
        "        \n",
        "        # If incorrect, get suggestions\n",
        "        if not is_correct:\n",
        "            correction = spell.correction(clean_word)\n",
        "            candidates = list(spell.candidates(clean_word))\n",
        "        else:\n",
        "            correction = None\n",
        "            candidates = []\n",
        "        \n",
        "        return {\n",
        "            'original': word,\n",
        "            'is_correct': is_correct,\n",
        "            'correction': correction,\n",
        "            'candidates': candidates[:5]  # Limit to top 5 suggestions\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'original': word,\n",
        "            'is_correct': None,\n",
        "            'correction': f\"Error: {str(e)}\",\n",
        "            'candidates': []\n",
        "        }\n",
        "\n",
        "def check_glossary_spelling(glossary_df, lang_column):\n",
        "    \"\"\"\n",
        "    Apply spell checking to all words in a specific language column of the glossary.\n",
        "    \n",
        "    Args:\n",
        "        glossary_df: Your glossary DataFrame\n",
        "        lang_column: Column name to check (e.g., 'English', 'Spanish', etc.)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with spell check results\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get non-null values from the specified column - corrected approach\n",
        "    words_to_check = glossary_df.select(lang_column).filter(\n",
        "        pl.col(lang_column).is_not_null() &\n",
        "        pl.col(lang_column).str.strip_chars().str.len_chars() > 0 &\n",
        "        pl.col(lang_column).str.contains(r'^[a-zA-ZÀ-ÿ\\s]+$')  # Allow letters and accented chars\n",
        "    )\n",
        "    \n",
        "    results = []\n",
        "    # Extract the actual string values from the DataFrame\n",
        "    for row in words_to_check.iter_rows():\n",
        "        word = row[0]  # Get the first (and only) column value\n",
        "        if word:  # Double-check it's not None\n",
        "            result = word_spell_check(word.lower(), lang_column)\n",
        "            result['column'] = lang_column\n",
        "            results.append(result)\n",
        "    \n",
        "    return pl.DataFrame(results)\n",
        "\n",
        "# Test individual words:\n",
        "print(\"Testing spell checker:\")\n",
        "print(word_spell_check(\"hello\", \"English\"))\n",
        "print(word_spell_check(\"hapenning\", \"English\"))  \n",
        "print(word_spell_check(\"africa\", \"Spanish\"))\n",
        "print(word_spell_check(\"manana\", \"Spanish\"))  # misspelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "# Initialize the text correction pipeline\n",
        "generator = pipeline(\"text2text-generation\", model=\"sdadas/byt5-text-correction\")\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "def sent_check(sentence, lang):\n",
        "    \"\"\"\n",
        "    Check and correct a sentence using ByT5 text correction model.\n",
        "    \n",
        "    Args:\n",
        "        sentence (str): Sentence to check and correct\n",
        "        lang (str): Language column name from glossary_df \n",
        "                   (English, Arabic, French, Spanish, Chinese, Russian, Portuguese, Swahili)\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            'original': original sentence,\n",
        "            'corrected': corrected sentence,\n",
        "            'changed': boolean indicating if correction was applied,\n",
        "            'language_code': language code used for correction\n",
        "        }\n",
        "    \"\"\"\n",
        "    \n",
        "    # Language mapping from glossary column names to ByT5 language codes\n",
        "    lang_mapping = {\n",
        "        'English': 'en',\n",
        "        'Arabic': 'ar', \n",
        "        'French': 'fr',\n",
        "        'Spanish': 'es',\n",
        "        'Chinese': 'zh',\n",
        "        'Russian': 'ru',\n",
        "        'Portuguese': 'pt',\n",
        "        'Swahili': 'sw'  # ByT5 supports more languages than pyspellchecker\n",
        "    }\n",
        "\n",
        "    lang_intro = {\n",
        "    'English': 'The term is: ',\n",
        "    'Arabic': 'المصطلح هو: ',         # \"Al-mustalaḥ huwa\" — correct for a conceptual term\n",
        "    'French': 'Le terme est : ',       # \"Terme\" is used for conceptual terms\n",
        "    'Spanish': 'El término es: ',      # \"Término\" is correct for a glossary/terminology entry\n",
        "    'Chinese': '术语是：',              # \"术语\" (shùyǔ) means technical or conceptual term\n",
        "    'Russian': 'Термин: ',             # \"Термин\" is used for conceptual/technical terms\n",
        "    'Portuguese': 'O termo é: ',       # \"Termo\" is correct for a conceptual term\n",
        "    'Swahili': 'Neno la istilahi ni: ' # More precise than just \"Neno ni\", which means \"The word is\"\n",
        "}\n",
        "\n",
        "    \n",
        "    # Handle empty or None input\n",
        "    if not sentence or sentence is None or (isinstance(sentence, str) and sentence.strip() == \"\"):\n",
        "        return {\n",
        "            'original': sentence,\n",
        "            'corrected': sentence,\n",
        "            'changed': False,\n",
        "            'language_code': None\n",
        "        }\n",
        "    \n",
        "    # Get language code\n",
        "    lang_code = lang_mapping.get(lang)\n",
        "    lang_intro_text = lang_intro.get(lang)\n",
        "    \n",
        "    # Handle unsupported languages (though ByT5 supports many)\n",
        "    if lang_code is None:\n",
        "        return {\n",
        "            'original': sentence,\n",
        "            'corrected': sentence,\n",
        "            'changed': False,\n",
        "            'language_code': f\"Language '{lang}' not supported\"\n",
        "        }\n",
        "    \n",
        "    try:\n",
        "        # Clean the sentence (remove extra spaces)\n",
        "        clean_sentence = sentence.strip()\n",
        "        \n",
        "        # Format sentence with language code for ByT5\n",
        "        formatted_input = f\"<{lang_code}> {lang_intro_text}{clean_sentence.lower()}\"\n",
        "        \n",
        "        # Apply text correction\n",
        "        corrected_result = generator(formatted_input, max_new_tokens=512)\n",
        "        corrected_sentence = corrected_result[0]['generated_text']\n",
        "        if corrected_sentence[-1] == \".\":\n",
        "            corrected_sentence = corrected_sentence[:-1]\n",
        "        # Remove intro text from corrected sent\n",
        "        corrected_sentence = corrected_sentence.replace(lang_intro_text, \"\")\n",
        "        corrected_sentence = corrected_sentence.strip()\n",
        "        \n",
        "        def correct_with_word_boundaries(text, geoname, correction):\n",
        "            \"\"\"Replace geoname with correction only at word boundaries\"\"\"\n",
        "            # Create pattern that matches the word at proper boundaries\n",
        "            pattern = r'\\b{}\\b'.format(re.escape(geoname))\n",
        "            # For hyphenated words, also match at hyphens\n",
        "            hyphen_pattern = r'(\\-){}'.format(re.escape(geoname))\n",
        "            # Replace at word boundaries\n",
        "            result = re.sub(pattern, correction, text, flags=re.IGNORECASE)\n",
        "            # Replace when preceded by hyphen\n",
        "            result = re.sub(hyphen_pattern, r'\\1{}'.format(correction), result, flags=re.IGNORECASE)\n",
        "            return result\n",
        "        \n",
        "        # Apply word boundary correction for geonames\n",
        "        # Apply correction with proper word boundaries\n",
        "        for geoname, proper_form in geoname_case_mapping.items():\n",
        "            corrected_sentence = correct_with_word_boundaries(corrected_sentence, geoname, proper_form)\n",
        "\n",
        "\n",
        "        # Check if correction was applied\n",
        "        changed = corrected_sentence.lower() != clean_sentence.lower()\n",
        "        \n",
        "        return {\n",
        "            'original': sentence,\n",
        "            'corrected': corrected_sentence,\n",
        "            'changed': changed,\n",
        "            'language_code': lang_code\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'original': sentence,\n",
        "            'corrected': sentence,\n",
        "            'changed': False,\n",
        "            'language_code': f\"Error: {str(e)}\"\n",
        "        }\n",
        "\n",
        "def check_glossary_sents(glossary_df, lang_column):\n",
        "    \"\"\"\n",
        "    Apply sentence-level text correction to all entries in a specific language column of the glossary.\n",
        "    \n",
        "    Args:\n",
        "        glossary_df: Your glossary DataFrame\n",
        "        lang_column: Column name to check (e.g., 'English', 'Spanish', etc.)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with sentence correction results\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get non-null values from the specified column\n",
        "    sentences_to_check = glossary_df.select(lang_column).filter(\n",
        "    (pl.col(lang_column).is_not_null()) &\n",
        "    (pl.col(lang_column).str.strip_chars().str.len_chars() > 2) &  # Minimum 3 characters\n",
        "    (pl.col(lang_column).str.to_uppercase() == pl.col(lang_column))\n",
        ")\n",
        "    \n",
        "    results = []\n",
        "    # Extract the actual string values from the DataFrame\n",
        "    for row in sentences_to_check.iter_rows():\n",
        "        sentence = row[0]  # Get the first (and only) column value\n",
        "        if sentence:  # Double-check it's not None\n",
        "            result = sent_check(sentence, lang_column)\n",
        "            result['column'] = lang_column\n",
        "            results.append(result)\n",
        "            print(f\"Processed: {sentence}\")\n",
        "    \n",
        "    return pl.DataFrame(results)\n",
        "\n",
        "# Alternative batch processing function for better performance\n",
        "def check_glossary_sents_batch(glossary_df, lang_column, batch_size=10, intro_prompt=False):\n",
        "    \"\"\"\n",
        "    Apply sentence-level text correction in batches for better performance.\n",
        "    \n",
        "    Args:\n",
        "        glossary_df: Your glossary DataFrame\n",
        "        lang_column: Column name to check\n",
        "        batch_size: Number of sentences to process at once\n",
        "        intro_prompt: Whether to use intro prompts\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with sentence correction results\n",
        "    \"\"\"\n",
        "    \n",
        "    # Language mapping\n",
        "    lang_mapping = {\n",
        "        'English': 'en', 'Arabic': 'ar', 'French': 'fr', 'Spanish': 'es',\n",
        "        'Chinese': 'zh', 'Russian': 'ru', 'Portuguese': 'pt', 'Swahili': 'sw'\n",
        "    }\n",
        "    \n",
        "    lang_intro = {\n",
        "        'English': 'The term is ',\n",
        "        'Arabic': 'المصطلح هو ',         \n",
        "        'French': 'Le terme est  ',       \n",
        "        'Spanish': 'El término es ',      \n",
        "        'Chinese': '术语是：',              \n",
        "        'Russian': 'Термин ',             \n",
        "        'Portuguese': 'O termo é ',       \n",
        "        'Swahili': 'Neno la istilahi ni ' \n",
        "    }\n",
        "\n",
        "    # Get language code\n",
        "    lang_code = lang_mapping.get(lang_column)\n",
        "    lang_intro_text = lang_intro.get(lang_column)\n",
        "    if not lang_code:\n",
        "        print(f\"Language '{lang_column}' not supported\")\n",
        "        return pl.DataFrame()\n",
        "    \n",
        "    # Get sentences to process\n",
        "    sentences_to_check = glossary_df.select(lang_column).filter(\n",
        "        (pl.col(lang_column).is_not_null()) &\n",
        "        (pl.col(lang_column).str.strip_chars().str.len_chars() > 2) &  # Minimum 3 characters\n",
        "        (pl.col(lang_column).str.to_uppercase() == pl.col(lang_column))\n",
        "    )\n",
        "    \n",
        "    sentences = [row[0] for row in sentences_to_check.iter_rows() if row[0]]\n",
        "    \n",
        "    if not sentences:\n",
        "        return pl.DataFrame()\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Calculate total number of batches for progress bar\n",
        "    total_batches = (len(sentences) + batch_size - 1) // batch_size\n",
        "    \n",
        "    # Process in batches with notebook-optimized progress bar\n",
        "    with tqdm(total=total_batches, \n",
        "              desc=f\"Processing {lang_column}\", \n",
        "              unit=\"batch\",\n",
        "              leave=True,\n",
        "              ncols=400) as pbar:\n",
        "        \n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i + batch_size]\n",
        "            \n",
        "            # Format batch with language codes\n",
        "            if intro_prompt:\n",
        "                formatted_batch = [f\"<{lang_code}> {lang_intro_text}{sent.strip().lower()}\" for sent in batch]\n",
        "            else:\n",
        "                formatted_batch = [f\"<{lang_code}> {sent.strip().lower()}.\" for sent in batch]\n",
        "            \n",
        "            try:\n",
        "                # Apply correction to batch\n",
        "                corrected_batch = generator(formatted_batch, max_new_tokens=512)\n",
        "                \n",
        "                # Process results\n",
        "                for j, (original, corrected_result) in enumerate(zip(batch, corrected_batch)):\n",
        "                    corrected = corrected_result['generated_text']\n",
        "                    changed = corrected.lower() != original.strip().lower()\n",
        "\n",
        "                    # Clean correction\n",
        "                    if corrected[-1] == \".\":\n",
        "                        corrected = corrected[:-1]\n",
        "                    # Remove intro text from corrected sent\n",
        "                    corrected = corrected.replace(lang_intro_text, \"\")\n",
        "                    corrected = corrected.strip()\n",
        "                    \n",
        "                    results.append({\n",
        "                        'original': original,\n",
        "                        'corrected': corrected,\n",
        "                        'changed': changed,\n",
        "                        'language_code': lang_code,\n",
        "                        'column': lang_column\n",
        "                    })\n",
        "                    \n",
        "            except Exception as e:\n",
        "                # If batch fails, process individually\n",
        "                for sent in batch:\n",
        "                    result = sent_check(sent, lang_column)\n",
        "                    result['column'] = lang_column\n",
        "                    results.append(result)\n",
        "            \n",
        "            # Update progress bar with additional info\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({\n",
        "                'sentences': len(results),\n",
        "                'batch_size': len(batch),\n",
        "                'changes': sum(1 for r in results if r.get('changed', False))\n",
        "            })\n",
        "    \n",
        "    return pl.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset version\n",
        "def check_glossary_sents_batch(glossary_df, lang_column, batch_size=10, intro_prompt=False):\n",
        "    \"\"\"\n",
        "    Apply sentence-level text correction in batches for better performance.\n",
        "    \"\"\"\n",
        "    # Language mapping\n",
        "    lang_mapping = {\n",
        "        'English': 'en', 'Arabic': 'ar', 'French': 'fr', 'Spanish': 'es',\n",
        "        'Chinese': 'zh', 'Russian': 'ru', 'Portuguese': 'pt', 'Swahili': 'sw'\n",
        "    }\n",
        "    \n",
        "    lang_intro = {\n",
        "        'English': 'I am quite interested in ',\n",
        "        'Arabic': 'المصطلح هو ',         \n",
        "        'French': 'Le terme est  ',       \n",
        "        'Spanish': 'El término es ',      \n",
        "        'Chinese': '术语是：',              \n",
        "        'Russian': 'Термин ',             \n",
        "        'Portuguese': 'O termo é ',       \n",
        "        'Swahili': 'Neno la istilahi ni ' \n",
        "    }\n",
        "\n",
        "    # Get language code\n",
        "    lang_code = lang_mapping.get(lang_column)\n",
        "    lang_intro_text = lang_intro.get(lang_column)\n",
        "    if not lang_code:\n",
        "        print(f\"Language '{lang_column}' not supported\")\n",
        "        return pl.DataFrame()\n",
        "    \n",
        "    # Get sentences to process\n",
        "    sentences_to_check = glossary_df.select(lang_column).filter(\n",
        "        (pl.col(lang_column).is_not_null()) &\n",
        "        (pl.col(lang_column).str.strip_chars().str.len_chars() > 2) &\n",
        "        (pl.col(lang_column).str.to_uppercase() == pl.col(lang_column))\n",
        "    )\n",
        "    \n",
        "    sentences = [row[0] for row in sentences_to_check.iter_rows() if row[0]]\n",
        "    \n",
        "    if not sentences:\n",
        "        return pl.DataFrame()\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Calculate total number of batches for progress bar\n",
        "    total_batches = (len(sentences) + batch_size - 1) // batch_size\n",
        "    \n",
        "    # Process in batches with notebook-optimized progress bar\n",
        "    with tqdm(total=total_batches, \n",
        "              desc=f\"Processing {lang_column}\", \n",
        "              unit=\"batch\",\n",
        "              leave=True,\n",
        "              ncols=400) as pbar:\n",
        "        \n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i + batch_size]\n",
        "            \n",
        "            # Format batch with language codes\n",
        "            if intro_prompt:\n",
        "                formatted_batch = [f\"<{lang_code}> {lang_intro_text}{sent.strip().lower()}\" for sent in batch]\n",
        "            else:\n",
        "                formatted_batch = [f\"<{lang_code}> {sent.strip().lower()}.\" for sent in batch]\n",
        "            \n",
        "            try:\n",
        "                # Apply correction to entire batch at once - this is the key change\n",
        "                corrected_batch = generator(formatted_batch, \n",
        "                                          max_new_tokens=512,\n",
        "                                          batch_size=len(formatted_batch),  # Process entire batch together\n",
        "                                          clean_up_tokenization_spaces=True)\n",
        "                \n",
        "                # Process results\n",
        "                for j, (original, corrected_result) in enumerate(zip(batch, corrected_batch)):\n",
        "                    corrected = corrected_result['generated_text']\n",
        "                    changed = corrected.lower() != original.strip().lower()\n",
        "\n",
        "                    # Clean correction\n",
        "                    if corrected.endswith(\".\"):\n",
        "                        corrected = corrected[:-1]\n",
        "                    # Remove intro text from corrected sent\n",
        "                    corrected = corrected.replace(lang_intro_text, \"\")\n",
        "                    corrected = corrected.strip()\n",
        "                    \n",
        "                    results.append({\n",
        "                        'original': original,\n",
        "                        'corrected': corrected,\n",
        "                        'changed': changed,\n",
        "                        'language_code': lang_code,\n",
        "                        'column': lang_column\n",
        "                    })\n",
        "                    \n",
        "            except Exception as e:\n",
        "                # If batch fails, process individually as fallback\n",
        "                print(f\"Batch processing failed, falling back to individual processing: {e}\")\n",
        "                for sent in batch:\n",
        "                    result = sent_check(sent, lang_column)\n",
        "                    result['column'] = lang_column\n",
        "                    results.append(result)\n",
        "            \n",
        "            # Update progress bar with additional info\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({\n",
        "                'sentences': len(results),\n",
        "                'batch_size': len(batch),\n",
        "                'changes': sum(1 for r in results if r.get('changed', False))\n",
        "            })\n",
        "    \n",
        "    return pl.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing sentence checker:\n",
            "Original (English): hello world this is a test\n",
            "Corrected: hello world this is a test\n",
            "Changed: False\n",
            "\n",
            "Original (Spanish): hola mundo esto es una prueba\n",
            "Corrected: hola mundo esto es una prueba\n",
            "Changed: False\n",
            "\n",
            "Original (French): BONJOUR MONDE CECI EST UN TEST\n",
            "Corrected: bonjour monde ceci est un test\n",
            "Changed: False\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "887f8b510fe54da98b9d0edfd05c4b02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing English:   0%|                                                                                     …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "\n",
        "glossary_df = pl.read_excel(\"data/unbist-20250708.xlsx\")\n",
        "\n",
        "# Test the functions\n",
        "print(\"Testing sentence checker:\")\n",
        "test_sentences = [\n",
        "    (\"hello world this is a test\", \"English\"),\n",
        "    (\"hola mundo esto es una prueba\", \"Spanish\"),\n",
        "    (\"BONJOUR MONDE CECI EST UN TEST\", \"French\"),\n",
        "    #(\"привет мир это тест\", \"Russian\")\n",
        "]\n",
        "\n",
        "for sentence, lang in test_sentences:\n",
        "    result = sent_check(sentence, lang)\n",
        "    print(f\"Original ({lang}): {result['original']}\")\n",
        "    print(f\"Corrected: {result['corrected']}\")\n",
        "    print(f\"Changed: {result['changed']}\")\n",
        "    print()\n",
        "\n",
        "# Batch process Spanish column\n",
        "#es_corrections = check_glossary_sents_batch(glossary_df, \"Spanish\", batch_size=20)\n",
        "# Export to Excel\n",
        "#es_corrections.write_excel(\"es_corrections.xlsx\")\n",
        "\n",
        "# Batch process french column\n",
        "#fr_corrections = check_glossary_sents_batch(glossary_df, \"French\", batch_size=20)\n",
        "# Export to Excel\n",
        "#fr_corrections.write_excel(\"fr_corrections.xlsx\")\n",
        "\n",
        "en_corrections2 = check_glossary_sents_batch(glossary_df, \"English\", batch_size=20, intro_prompt=True)\n",
        "#es_corrections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<xlsxwriter.workbook.Workbook at 0x213f9ef4d10>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_corrections2.write_excel(\"data/en_corrections2.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace countries and international regions by capital letter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total geographical names for exclusion: 586\n",
            "Sample geonames: ['Saint Kitts and Nevis', 'Eastern Europe', 'Hayastani', 'Cuban', 'Portugal', 'Saint Pierre and Miquelon', 'Austria', 'Ukraine', 'Åland Islands', 'Macau']\n",
            "Processing English corrections...\n",
            "en_corrections2 has 7279 rows\n",
            "Starting geoname case correction on 7279 terms...\n",
            "Attempting vectorized processing...\n",
            "Vectorized processing completed in 0.09 seconds\n",
            "English corrections saved to data/en_corrections_processed2.xlsx with 7279 rows\n",
            "Total corrections with changes: 3\n",
            "\n",
            "First few rows of processed corrections:\n",
            "shape: (5, 5)\n",
            "┌───────────────────────────────┬──────────────────────────────┬─────────┬───────────────┬─────────┐\n",
            "│ original                      ┆ corrected                    ┆ changed ┆ language_code ┆ column  │\n",
            "│ ---                           ┆ ---                          ┆ ---     ┆ ---           ┆ ---     │\n",
            "│ str                           ┆ str                          ┆ bool    ┆ str           ┆ str     │\n",
            "╞═══════════════════════════════╪══════════════════════════════╪═════════╪═══════════════╪═════════╡\n",
            "│ NATURAL RESOURCES AND THE     ┆ natural resources and the    ┆ true    ┆ en            ┆ English │\n",
            "│ ENVI…                         ┆ envi…                        ┆         ┆               ┆         │\n",
            "│ SOCIAL CONDITIONS AND EQUITY  ┆ social conditions and equity ┆ true    ┆ en            ┆ English │\n",
            "│ ECONOMIC DEVELOPMENT AND      ┆ economic development and     ┆ true    ┆ en            ┆ English │\n",
            "│ DEVEL…                        ┆ devel…                       ┆         ┆               ┆         │\n",
            "│ CULTURE                       ┆ culture                      ┆ true    ┆ en            ┆ English │\n",
            "│ INDUSTRY                      ┆ industry                     ┆ true    ┆ en            ┆ English │\n",
            "└───────────────────────────────┴──────────────────────────────┴─────────┴───────────────┴─────────┘\n",
            "\n",
            "Testing specific cases:\n",
            "'French' -> 'French'\n",
            "'Argentinians' -> 'Argentinians'\n",
            "'Baltic Sea' -> 'Baltic Sea'\n",
            "'Indian Sea' -> 'Indian Sea'\n",
            "'french' -> 'French'\n",
            "'argentinians' -> 'Argentinians'\n",
            "'baltic sea' -> 'baltic sea'\n",
            "'indian sea' -> 'Indian sea'\n",
            "'ASSYRIANS' -> 'ASSYRIANS'\n",
            "'Assyrians' -> 'Assyrians'\n",
            "'asSyrians' -> 'asSyrians'\n",
            "'syrian' -> 'Syrian'\n",
            "'syrians' -> 'Syrians'\n",
            "'french-syrian' -> 'french-syrian'\n",
            "'transfrench' -> 'transfrench'\n",
            "'puerto ricans' -> 'puerto ricans'\n",
            "'United States' -> 'United States'\n",
            "'slavic languages' -> 'Slavic languages'\n",
            "'budukh language' -> 'Budukh language'\n",
            "'english language' -> 'English language'\n",
            "'ural-Altaic languages' -> 'Ural-Altaic languages'\n",
            "'asian countries' -> 'Asian countries'\n",
            "'european union' -> 'European union'\n",
            "'balanites aegyptiaca' -> 'balanites aegyptiaca'\n",
            "'Latin America' -> 'Latin America'\n",
            "'Latin American' -> 'Latin American'\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Enhanced Wikidata Search:\n",
            "==================================================\n",
            "\n",
            "Searching for: 'Baltic Sea'\n",
            "Found 3 entities:\n",
            "  1. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: sea in Northern Europe\n",
            "     Translations: {'en': 'Baltic Sea', 'es': 'Mar Báltico', 'fr': 'mer Baltique'}\n",
            "     URL: https://www.wikidata.org/entity/Q545\n",
            "  2. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: ship built in 2005\n",
            "     Translations: {'en': 'Baltic Sea', 'es': '', 'fr': ''}\n",
            "     URL: https://www.wikidata.org/entity/Q83600854\n",
            "  3. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: ship built in 1964\n",
            "     Translations: {'en': 'Baltic Sea', 'es': '', 'fr': ''}\n",
            "     URL: https://www.wikidata.org/entity/Q83647454\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'baltic sea'\n",
            "Found 3 entities:\n",
            "  1. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: sea in Northern Europe\n",
            "     Translations: {'en': 'Baltic Sea', 'es': 'Mar Báltico', 'fr': 'mer Baltique'}\n",
            "     URL: https://www.wikidata.org/entity/Q545\n",
            "  2. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: ship built in 2005\n",
            "     Translations: {'en': 'Baltic Sea', 'es': '', 'fr': ''}\n",
            "     URL: https://www.wikidata.org/entity/Q83600854\n",
            "  3. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: ship built in 1964\n",
            "     Translations: {'en': 'Baltic Sea', 'es': '', 'fr': ''}\n",
            "     URL: https://www.wikidata.org/entity/Q83647454\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'BALTIC SEA'\n",
            "Found 3 entities:\n",
            "  1. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: sea in Northern Europe\n",
            "     Translations: {'en': 'Baltic Sea', 'es': 'Mar Báltico', 'fr': 'mer Baltique'}\n",
            "     URL: https://www.wikidata.org/entity/Q545\n",
            "  2. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: ship built in 2005\n",
            "     Translations: {'en': 'Baltic Sea', 'es': '', 'fr': ''}\n",
            "     URL: https://www.wikidata.org/entity/Q83600854\n",
            "  3. Baltic Sea (Score: 100.0, Exact: True)\n",
            "     Description: ship built in 1964\n",
            "     Translations: {'en': 'Baltic Sea', 'es': '', 'fr': ''}\n",
            "     URL: https://www.wikidata.org/entity/Q83647454\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'China'\n",
            "Found 3 entities:\n",
            "  1. China (Score: 100.0, Exact: True)\n",
            "     Description: cultural region, ancient civilization, and nation in East Asia, mostly refer to the People's Republic of China in political situation and rarely refer to the Republic of China\n",
            "     Translations: {'en': 'China', 'es': 'China', 'fr': 'Chine'}\n",
            "     URL: https://www.wikidata.org/entity/Q29520\n",
            "  2. China (Score: 100.0, Exact: True)\n",
            "     Description: female given name\n",
            "     Translations: {'en': 'China', 'es': 'China', 'fr': 'China'}\n",
            "     URL: https://www.wikidata.org/entity/Q20898938\n",
            "  3. China (Score: 100.0, Exact: True)\n",
            "     Description: family name\n",
            "     Translations: {'en': 'China', 'es': 'China', 'fr': 'China'}\n",
            "     URL: https://www.wikidata.org/entity/Q37229573\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'United Nations'\n",
            "Found 2 entities:\n",
            "  1. United Nations (Score: 100.0, Exact: True)\n",
            "     Description: global international and intergovernmental organization\n",
            "     Translations: {'en': 'United Nations', 'es': 'Organización de las Naciones Unidas', 'fr': 'Organisation des Nations unies'}\n",
            "     URL: https://www.wikidata.org/entity/Q1065\n",
            "  2. 6000 United Nations (Score: 84.84848484848484, Exact: False)\n",
            "     Description: asteroid\n",
            "     Translations: {'en': '', 'es': '(6000) United Nations', 'fr': '(6000) United Nations'}\n",
            "     URL: https://www.wikidata.org/entity/Q155782\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'climate change'\n",
            "Found 3 entities:\n",
            "  1. climate change (Score: 100.0, Exact: True)\n",
            "     Description: change in the statistical distribution of weather patterns for an extended period\n",
            "     Translations: {'en': 'climate change', 'es': 'cambio climático', 'fr': 'changement climatique'}\n",
            "     URL: https://www.wikidata.org/entity/Q125928\n",
            "  2. Climate Change (Score: 100.0, Exact: True)\n",
            "     Description: album by Pitbull\n",
            "     Translations: {'en': 'Climate Change', 'es': 'Climate Change', 'fr': 'Climate Change'}\n",
            "     URL: https://www.wikidata.org/entity/Q27701830\n",
            "  3. Climate Change (Score: 100.0, Exact: True)\n",
            "     Description: album by Destiny\n",
            "     Translations: {'en': 'Climate Change', 'es': 'Climate Change', 'fr': 'Climate Change'}\n",
            "     URL: https://www.wikidata.org/entity/Q49096773\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'european Union'\n",
            "Found 3 entities:\n",
            "  1. European Union (Score: 100.0, Exact: True)\n",
            "     Description: political and economic union of 27 European states\n",
            "     Translations: {'en': 'European Union', 'es': 'Unión Europea', 'fr': 'Union européenne'}\n",
            "     URL: https://www.wikidata.org/entity/Q458\n",
            "  2. European Union (Score: 100.0, Exact: True)\n",
            "     Description: antifascist resistance group\n",
            "     Translations: {'en': 'European Union', 'es': 'Europäische Union', 'fr': 'Union européenne'}\n",
            "     URL: https://www.wikidata.org/entity/Q319328\n",
            "  3. European Union law (Score: 87.5, Exact: False)\n",
            "     Description: rules operating within EU member states\n",
            "     Translations: {'en': 'European Union law', 'es': 'derecho de la Unión Europea', 'fr': \"droit de l'Union européenne\"}\n",
            "     URL: https://www.wikidata.org/entity/Q208202\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'european'\n",
            "Found 1 entities:\n",
            "  1. Europe (Score: 85.71428571428572, Exact: False)\n",
            "     Description: continent in the Northern Hemisphere\n",
            "     Translations: {'en': 'Europe', 'es': 'Europa', 'fr': 'Europe'}\n",
            "     URL: https://www.wikidata.org/entity/Q46\n",
            "------------------------------\n",
            "\n",
            "Searching for: 'Albania'\n",
            "Found 3 entities:\n",
            "  1. Albania (Score: 100.0, Exact: True)\n",
            "     Description: country in southeastern Europe\n",
            "     Translations: {'en': 'Albania', 'es': 'Albania', 'fr': 'Albanie'}\n",
            "     URL: https://www.wikidata.org/entity/Q222\n",
            "  2. Albania (Score: 100.0, Exact: True)\n",
            "     Description: genus of molluscs\n",
            "     Translations: {'en': 'Albania', 'es': 'Albania (género)', 'fr': 'Albania'}\n",
            "     URL: https://www.wikidata.org/entity/Q3607867\n",
            "  3. Albania (Score: 100.0, Exact: True)\n",
            "     Description: family name\n",
            "     Translations: {'en': 'Albania', 'es': 'Albania', 'fr': 'Albania'}\n",
            "     URL: https://www.wikidata.org/entity/Q107182203\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from urllib.parse import quote\n",
        "from rapidfuzz import fuzz\n",
        "import time\n",
        "\n",
        "def search_wikidata_fuzzy(query, language='en', min_score=80, max_results=5):\n",
        "    \"\"\"\n",
        "    Search Wikidata with fuzzy matching for entities\n",
        "    \n",
        "    Args:\n",
        "        query: Search term (case insensitive)\n",
        "        language: Language code (default: 'en')\n",
        "        min_score: Minimum fuzzy match score (default: 80)\n",
        "        max_results: Maximum number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of matching entities with fuzzy scores\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clean the query\n",
        "        clean_query = query.strip()\n",
        "        \n",
        "        # Wikidata search API\n",
        "        url = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'format': 'json',\n",
        "            'uselang': language,\n",
        "            'search': clean_query,\n",
        "            'language': language,\n",
        "            'limit': 10,  # Get more results for fuzzy filtering\n",
        "            'formatversion': 2\n",
        "        }\n",
        "        \n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            \n",
        "            if 'search' not in data:\n",
        "                return []\n",
        "            \n",
        "            results = []\n",
        "            query_lower = clean_query.lower()\n",
        "            \n",
        "            for item in data['search']:\n",
        "                label = item.get('label', '')\n",
        "                description = item.get('description', '')\n",
        "                \n",
        "                # Calculate fuzzy match score (case insensitive)\n",
        "                score = fuzz.ratio(query_lower, label.lower())\n",
        "                \n",
        "                # Also check if it's an exact case-insensitive match\n",
        "                exact_match = query_lower == label.lower()\n",
        "                \n",
        "                if score >= min_score or exact_match:\n",
        "                    results.append({\n",
        "                        'id': item.get('id', ''),\n",
        "                        'label': label,\n",
        "                        'description': description,\n",
        "                        'score': score,\n",
        "                        'exact_match': exact_match,\n",
        "                        'url': f\"https://www.wikidata.org/entity/{item.get('id', '')}\"\n",
        "                    })\n",
        "            \n",
        "            # Sort by score (descending) and exact matches first\n",
        "            results.sort(key=lambda x: (x['exact_match'], x['score']), reverse=True)\n",
        "            \n",
        "            return results[:max_results]\n",
        "            \n",
        "        else:\n",
        "            print(f\"Wikidata API returned status code: {response.status_code}\")\n",
        "            return []\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Wikidata search error for '{query}': {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def get_wikidata_multilingual_labels(entity_id, languages=['en', 'es', 'fr', 'de', 'it']):\n",
        "    \"\"\"\n",
        "    Get labels in multiple languages for a Wikidata entity\n",
        "    \n",
        "    Args:\n",
        "        entity_id: Wikidata entity ID (e.g., 'Q545')\n",
        "        languages: List of language codes\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with language codes as keys and labels as values\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'format': 'json',\n",
        "            'ids': entity_id,\n",
        "            'props': 'labels',\n",
        "            'languages': '|'.join(languages),\n",
        "            'formatversion': 2\n",
        "        }\n",
        "        \n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            \n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                labels = entity.get('labels', {})\n",
        "                \n",
        "                result = {}\n",
        "                for lang in languages:\n",
        "                    if lang in labels:\n",
        "                        result[lang] = labels[lang]['value']\n",
        "                    else:\n",
        "                        result[lang] = ''\n",
        "                \n",
        "                return result\n",
        "            \n",
        "        return {}\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error getting multilingual labels for {entity_id}: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "def enhanced_wikidata_search(query, target_languages=['es', 'fr'], min_score=80):\n",
        "    \"\"\"\n",
        "    Enhanced Wikidata search with multilingual results\n",
        "    \n",
        "    Args:\n",
        "        query: Search term\n",
        "        target_languages: Languages to get translations for\n",
        "        min_score: Minimum fuzzy match score\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with search results and translations\n",
        "    \"\"\"\n",
        "    # Search for entities\n",
        "    entities = search_wikidata_fuzzy(query, min_score=min_score)\n",
        "    \n",
        "    if not entities:\n",
        "        return {\n",
        "            'query': query,\n",
        "            'found': False,\n",
        "            'entities': []\n",
        "        }\n",
        "    \n",
        "    # Get multilingual labels for the best matches\n",
        "    enhanced_entities = []\n",
        "    \n",
        "    for entity in entities[:3]:  # Process top 3 matches\n",
        "        entity_id = entity['id']\n",
        "        \n",
        "        # Get labels in target languages\n",
        "        all_languages = ['en'] + target_languages\n",
        "        multilingual_labels = get_wikidata_multilingual_labels(entity_id, all_languages)\n",
        "        \n",
        "        enhanced_entity = {\n",
        "            **entity,\n",
        "            'multilingual_labels': multilingual_labels\n",
        "        }\n",
        "        enhanced_entities.append(enhanced_entity)\n",
        "        \n",
        "        # Small delay to be respectful to the API\n",
        "        time.sleep(0.1)\n",
        "    \n",
        "    return {\n",
        "        'query': query,\n",
        "        'found': True,\n",
        "        'entities': enhanced_entities\n",
        "    }\n",
        "\n",
        "# Test the enhanced search\n",
        "test_terms = [\n",
        "    \"Baltic Sea\",\n",
        "    \"baltic sea\", \n",
        "    \"BALTIC SEA\",\n",
        "    \"China\",\n",
        "    \"United Nations\",\n",
        "    \"climate change\",\n",
        "    \"european Union\",\n",
        "    \"european\",\n",
        "    \"Albania\"\n",
        "]\n",
        "\n",
        "print(\"Testing Enhanced Wikidata Search:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for term in test_terms:\n",
        "    print(f\"\\nSearching for: '{term}'\")\n",
        "    result = enhanced_wikidata_search(term, target_languages=['es', 'fr'], min_score=70)\n",
        "    \n",
        "    if result['found']:\n",
        "        print(f\"Found {len(result['entities'])} entities:\")\n",
        "        for i, entity in enumerate(result['entities'], 1):\n",
        "            print(f\"  {i}. {entity['label']} (Score: {entity['score']}, Exact: {entity['exact_match']})\")\n",
        "            print(f\"     Description: {entity['description']}\")\n",
        "            print(f\"     Translations: {entity['multilingual_labels']}\")\n",
        "            print(f\"     URL: {entity['url']}\")\n",
        "    else:\n",
        "        print(\"No matches found\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'etymology': '',\n",
              "  'definitions': [],\n",
              "  'pronunciations': {'text': [], 'audio': []}}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Russian column lowercase transformation applied (excluding geonames).\n",
            "Processing Spanish corrections...\n",
            "es_corrections_low has 7276 rows\n",
            "Spanish corrections applied.\n",
            "Processing French corrections...\n",
            "fr_corrections_low has 7276 rows\n",
            "French corrections applied.\n",
            "Updated glossary saved with 5980 rows\n",
            "Columns: ['Keyword', 'Category', 'English', 'Arabic', 'French', 'Spanish', 'Chinese', 'Russian', 'Portuguese', 'Swahili']\n"
          ]
        }
      ],
      "source": [
        "# set all to lowercase excepting Geonames\n",
        "\n",
        "#load geonames\n",
        "geonames_file_path = \"data/M49_countries.xlsx\"\n",
        "geonames_df = pl.read_excel(geonames_file_path)\n",
        "# Load the master glossary\n",
        "master_glossary_df = pl.read_excel(\"data/glossary_UNEP_202505.xlsx\")\n",
        "\n",
        "# Get the list of Spanish geonames for exclusion\n",
        "spanish_geonames = geonames_df.select(\"Spanish\").filter(\n",
        "    pl.col(\"Spanish\").is_not_null()\n",
        ").to_series().to_list()\n",
        "\n",
        "french_geonames = geonames_df.select(\"French\").filter(\n",
        "    pl.col(\"French\").is_not_null()\n",
        ").to_series().to_list()\n",
        "\n",
        "russian_geonames = geonames_df.select(\"Russian\").filter(\n",
        "    pl.col(\"Russian\").is_not_null()\n",
        ").to_series().to_list()\n",
        "\n",
        "# For each value in 'corrected' column: if the first character is uppercase but the second is not, convert to lowercase\n",
        "# BUT exclude geonames (don't convert them to lowercase)\n",
        "\n",
        "# Check if es_corrections exists before creating es_corrections_low\n",
        "if 'es_corrections' in locals():\n",
        "    es_corrections_low = es_corrections.clone().with_columns(\n",
        "        pl.when(\n",
        "            (pl.col(\"corrected\").str.slice(0, 1) == pl.col(\"corrected\").str.slice(0, 1).str.to_uppercase()) &\n",
        "            (pl.col(\"corrected\").str.slice(1, 1) == pl.col(\"corrected\").str.slice(1, 1).str.to_lowercase()) &\n",
        "            (pl.col(\"corrected\").str.len_chars() >= 2) &\n",
        "            (~pl.col(\"corrected\").is_in(spanish_geonames))  # Exclude geonames\n",
        "        )\n",
        "        .then(pl.col(\"corrected\").str.to_lowercase())\n",
        "        .otherwise(pl.col(\"corrected\"))\n",
        "        .alias(\"corrected\")\n",
        "    )\n",
        "else:\n",
        "    print(\"es_corrections not found. Skipping Spanish corrections.\")\n",
        "\n",
        "es_corrections_low = es_corrections_low.with_columns([\n",
        "        pl.col(\"corrected\").str.replace_all(\"Africa \", \"África \").str.replace_all(\"^Africa$\", \"África\").alias(\"corrected\")\n",
        "    ])\n",
        "\n",
        "# Process French corrections (this should work since fr_corrections exists)\n",
        "fr_corrections_low = fr_corrections.clone().with_columns(\n",
        "    pl.when(\n",
        "        (pl.col(\"corrected\").str.slice(0, 1) == pl.col(\"corrected\").str.slice(0, 1).str.to_uppercase()) &\n",
        "        (pl.col(\"corrected\").str.slice(1, 1) == pl.col(\"corrected\").str.slice(1, 1).str.to_lowercase()) &\n",
        "        (pl.col(\"corrected\").str.len_chars() >= 2) &\n",
        "        (~pl.col(\"corrected\").is_in(french_geonames))  # Exclude geonames\n",
        "    )\n",
        "    .then(pl.col(\"corrected\").str.to_lowercase())\n",
        "    .otherwise(pl.col(\"corrected\"))\n",
        "    .alias(\"corrected\")\n",
        ")\n",
        "#master_glossary_df = glossary_df\n",
        "\n",
        "master_glossary_df = master_glossary_df.with_columns(\n",
        "    pl.when(\n",
        "        (pl.col(\"Russian\").str.slice(0, 1) == pl.col(\"Russian\").str.slice(0, 1).str.to_uppercase()) &\n",
        "        (pl.col(\"Russian\").str.slice(1, 1) == pl.col(\"Russian\").str.slice(1, 1).str.to_lowercase()) &\n",
        "        (pl.col(\"Russian\").str.len_chars() >= 2) &\n",
        "        (pl.col(\"Russian\").is_not_null()) &  # Ensure not null\n",
        "        (~pl.col(\"Russian\").is_in(russian_geonames))  # Exclude geonames\n",
        "    )\n",
        "    .then(pl.col(\"Russian\").str.to_lowercase())\n",
        "    .otherwise(pl.col(\"Russian\"))\n",
        "    .alias(\"Russian\")\n",
        ")\n",
        "\n",
        "print(\"Russian column lowercase transformation applied (excluding geonames).\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Replace the original language columns with corrected ones by matching original values\n",
        "\n",
        "# For Spanish corrections (if es_corrections_low exists)\n",
        "if 'es_corrections_low' in locals():\n",
        "    print(\"Processing Spanish corrections...\")\n",
        "    print(f\"es_corrections_low has {es_corrections_low.shape[0]} rows\")\n",
        "    \n",
        "    # Create a mapping from original to corrected values\n",
        "    spanish_corrections_map = dict(zip(\n",
        "        es_corrections_low.get_column(\"original\").to_list(),\n",
        "        es_corrections_low.get_column(\"corrected\").to_list()\n",
        "    ))\n",
        "    \n",
        "    # Apply corrections using map_elements\n",
        "    master_glossary_df = master_glossary_df.with_columns([\n",
        "        pl.col(\"Spanish\").map_elements(\n",
        "            lambda x: spanish_corrections_map.get(x, x) if x is not None else x,\n",
        "            return_dtype=pl.Utf8\n",
        "        ).alias(\"Spanish\")\n",
        "    ])\n",
        "    print(\"Spanish corrections applied.\")\n",
        "else:\n",
        "    print(\"Skipping Spanish corrections - es_corrections_low not available.\")\n",
        "\n",
        "# For French corrections\n",
        "if 'fr_corrections_low' in locals():\n",
        "    print(\"Processing French corrections...\")\n",
        "    print(f\"fr_corrections_low has {fr_corrections_low.shape[0]} rows\")\n",
        "    \n",
        "    # Create a mapping from original to corrected values\n",
        "    french_corrections_map = dict(zip(\n",
        "        fr_corrections_low.get_column(\"original\").to_list(),\n",
        "        fr_corrections_low.get_column(\"corrected\").to_list()\n",
        "    ))\n",
        "    \n",
        "    # Apply corrections using map_elements\n",
        "    master_glossary_df = master_glossary_df.with_columns([\n",
        "        pl.col(\"French\").map_elements(\n",
        "            lambda x: french_corrections_map.get(x, x) if x is not None else x,\n",
        "            return_dtype=pl.Utf8\n",
        "        ).alias(\"French\")\n",
        "    ])\n",
        "    print(\"French corrections applied.\")\n",
        "\n",
        "# Save the updated glossary\n",
        "master_glossary_df.write_excel(\"data/glossaryUNEP_corrected.xlsx\")\n",
        "\n",
        "\n",
        "print(f\"Updated glossary saved with {master_glossary_df.shape[0]} rows\")\n",
        "print(\"Columns:\", master_glossary_df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not determine dtype for column 21, falling back to string\n",
            "Could not determine dtype for column 24, falling back to string\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Spanish corrections...\n",
            "es_corrections_low has 7276 rows\n",
            "Spanish corrections applied.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not determine dtype for column 21, falling back to string\n",
            "Could not determine dtype for column 24, falling back to string\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing French corrections...\n",
            "fr_corrections_low has 7276 rows\n",
            "French corrections applied.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not determine dtype for column 21, falling back to string\n",
            "Could not determine dtype for column 24, falling back to string\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<xlsxwriter.workbook.Workbook at 0x127108026c0>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "master_glossary_es = pl.read_excel(\"data/Keywords language versions in separate sheets.xlsx\", sheet_name=\"ES\")\n",
        "\n",
        "# For Spanish corrections (if es_corrections_low exists)\n",
        "if 'es_corrections_low' in locals():\n",
        "    print(\"Processing Spanish corrections...\")\n",
        "    print(f\"es_corrections_low has {es_corrections_low.shape[0]} rows\")\n",
        "    \n",
        "    # Create a mapping from original to corrected values\n",
        "    spanish_corrections_map = dict(zip(\n",
        "        es_corrections_low.get_column(\"original\").to_list(),\n",
        "        es_corrections_low.get_column(\"corrected\").to_list()\n",
        "    ))\n",
        "    \n",
        "    # Apply corrections using map_elements\n",
        "    master_glossary_es = master_glossary_es.with_columns([\n",
        "        pl.col(\"UNBIS\").map_elements(\n",
        "            lambda x: spanish_corrections_map.get(x, x) if x is not None else x,\n",
        "            return_dtype=pl.Utf8\n",
        "        ).alias(\"UNBIS\")\n",
        "    ])\n",
        "    print(\"Spanish corrections applied.\")\n",
        "else:\n",
        "    print(\"Skipping Spanish corrections - es_corrections_low not available.\")\n",
        "\n",
        "master_glossary_fr = pl.read_excel(\"data/Keywords language versions in separate sheets.xlsx\", sheet_name=\"FR\")\n",
        "# For French corrections\n",
        "if 'fr_corrections_low' in locals():\n",
        "    print(\"Processing French corrections...\")\n",
        "    print(f\"fr_corrections_low has {fr_corrections_low.shape[0]} rows\")\n",
        "    \n",
        "    # Create a mapping from original to corrected values\n",
        "    french_corrections_map = dict(zip(\n",
        "        fr_corrections_low.get_column(\"original\").to_list(),\n",
        "        fr_corrections_low.get_column(\"corrected\").to_list()\n",
        "    ))\n",
        "    \n",
        "    # Apply corrections using map_elements\n",
        "    master_glossary_fr = master_glossary_fr.with_columns([\n",
        "        pl.col(\"UNBIS\").map_elements(\n",
        "            lambda x: french_corrections_map.get(x, x) if x is not None else x,\n",
        "            return_dtype=pl.Utf8\n",
        "        ).alias(\"UNBIS\")\n",
        "    ])\n",
        "    print(\"French corrections applied.\")\n",
        "\n",
        "\n",
        "\n",
        "master_glossary_ru = pl.read_excel(\"data/Keywords language versions in separate sheets.xlsx\", sheet_name=\"RU\")\n",
        "master_glossary_ru = master_glossary_ru.with_columns(\n",
        "    pl.when(\n",
        "        (pl.col(\"UNBIS\").str.slice(0, 1) == pl.col(\"UNBIS\").str.slice(0, 1).str.to_uppercase()) &\n",
        "        (pl.col(\"UNBIS\").str.slice(1, 1) == pl.col(\"UNBIS\").str.slice(1, 1).str.to_lowercase()) &\n",
        "        (pl.col(\"UNBIS\").str.len_chars() >= 2) &\n",
        "        (pl.col(\"UNBIS\").is_not_null()) &  # Ensure not null\n",
        "        (~pl.col(\"UNBIS\").is_in(russian_geonames))  # Exclude geonames\n",
        "    )\n",
        "    .then(pl.col(\"UNBIS\").str.to_lowercase())\n",
        "    .otherwise(pl.col(\"UNBIS\"))\n",
        "    .alias(\"UNBIS\")\n",
        ")\n",
        "master_glossary_es.write_excel(\"data/Keywords language versions in separate sheets_corrected_ES.xlsx\", worksheet=\"ES\")\n",
        "master_glossary_fr.write_excel(\"data/Keywords language versions in separate sheets_corrected_FR.xlsx\", worksheet=\"FR\")\n",
        "master_glossary_ru.write_excel(\"data/Keywords language versions in separate sheets_corrected_RU.xlsx\", worksheet=\"RU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in fr_corrections_low: ['original', 'corrected', 'changed', 'language_code', 'column']\n",
            "\n",
            "First few rows:\n",
            "shape: (5, 5)\n",
            "┌────────────────────┬────────────────────┬─────────┬───────────────┬────────┐\n",
            "│ original           ┆ corrected          ┆ changed ┆ language_code ┆ column │\n",
            "│ ---                ┆ ---                ┆ ---     ┆ ---           ┆ ---    │\n",
            "│ str                ┆ str                ┆ bool    ┆ str           ┆ str    │\n",
            "╞════════════════════╪════════════════════╪═════════╪═══════════════╪════════╡\n",
            "│ ABACA              ┆ abaca              ┆ true    ┆ fr            ┆ French │\n",
            "│ ENFANTS ABANDONNES ┆ enfants abandonnés ┆ true    ┆ fr            ┆ French │\n",
            "│ ABREVIATIONS       ┆ abréviations       ┆ true    ┆ fr            ┆ French │\n",
            "│ ENLEVEMENT         ┆ enlèvement         ┆ true    ┆ fr            ┆ French │\n",
            "│ AVORTEMENT         ┆ avortement         ┆ true    ┆ fr            ┆ French │\n",
            "└────────────────────┴────────────────────┴─────────┴───────────────┴────────┘\n"
          ]
        }
      ],
      "source": [
        "# Debug: Check what columns exist in fr_corrections_low\n",
        "print(\"Columns in fr_corrections_low:\", fr_corrections_low.columns)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(fr_corrections_low.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EznpQIHQhGo",
        "outputId": "f59dc124-cbeb-4107-b808-28737d33ba20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Período de validez después de abierto el envase: 10 horas.\n",
            "Shelf life after first opening the container: 10 hours.\n"
          ]
        }
      ],
      "source": [
        "# Load translation memory\n",
        "\n",
        "source_test_file = \"all-filtered.es.real.test\"\n",
        "target_test_file = \"all-filtered.en.real.test\"\n",
        "\n",
        "with open(source_test_file, encoding=\"utf-8\") as source, open(target_test_file, encoding=\"utf-8\") as target:\n",
        "  source_sentences = [sent.strip() for sent in source.readlines()]\n",
        "  target_sentences = [sent.strip() for sent in target.readlines()]\n",
        "\n",
        "print(source_sentences[0])\n",
        "print(target_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzsxxv1tQj3b",
        "outputId": "250610b0-fcb3-4363-f7a3-0c9153cff6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Período de validez después de abierto el envase: 4 semanas\n",
            "Período de validez después de abierto el envase: 10 horas.\n",
            "Shelf life after opening the immediate packaging: 4 weeks.\n"
          ]
        }
      ],
      "source": [
        "# Load fuzzy matches from the Context Dataset\n",
        "\n",
        "online_test_file = \"all-filtered.esen.ms-multi-12.online.test\"\n",
        "\n",
        "src_lang = \"spa_Latn\"\n",
        "tgt_lang = \"eng_Latn\"\n",
        "\n",
        "with open(online_test_file, encoding=\"utf-8\") as online:\n",
        "  lines = [line.strip().split(\" ||| \") for line in online.readlines()]\n",
        "  scores = [float(line[0].strip()) for line in lines]\n",
        "  fuzzy_source_sentences = [line[1].strip() for line in lines]\n",
        "  online_source_sentences = [line[2].strip() for line in lines]\n",
        "  fuzzy_target_prefixes = [line[3].strip() for line in lines]\n",
        "\n",
        "print(fuzzy_source_sentences[0])\n",
        "print(online_source_sentences[0])\n",
        "print(fuzzy_target_prefixes[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_HHEGmtQ1i3"
      },
      "source": [
        "# Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H2PesVzEz-T",
        "outputId": "94801fef-d2ec-413c-dcd5-75d1da7ffc7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install ctranslate2 sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9JBaMTPE2q0",
        "outputId": "cfe29d90-9766-4ce6-a80e-46af10c1a520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/models/ct2-nllb-200-3.3B-int8': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /content/models/ct2-nllb-200-3.3B-int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3w3CifEWZn7",
        "outputId": "614944bc-0b81-4284-9e78-ceaf6a92c390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rconfig.json:   0% 0.00/808 [00:00<?, ?B/s]\rconfig.json: 100% 808/808 [00:00<00:00, 6.21MB/s]\n",
            "2025-07-13 21:29:58.635083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752442198.897765    7060 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752442198.964322    7060 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-13 21:29:59.546839: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "pytorch_model.bin: 100% 5.48G/5.48G [01:35<00:00, 57.3MB/s]\n",
            "model.safetensors:  25% 1.37G/5.48G [00:33<01:10, 58.5MB/s]\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 1.10MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 564/564 [00:00<00:00, 2.57MB/s]\n",
            "\n",
            "sentencepiece.bpe.model:   0% 0.00/4.85M [00:00<?, ?B/s]\u001b[A\n",
            "sentencepiece.bpe.model: 100% 4.85M/4.85M [00:02<00:00, 1.80MB/s]\n",
            "\n",
            "model.safetensors:  32% 1.75G/5.48G [00:43<01:02, 59.8MB/s]\n",
            "tokenizer.json: 100% 17.3M/17.3M [00:05<00:00, 3.39MB/s]\n",
            "model.safetensors:  33% 1.82G/5.48G [00:43<00:55, 66.0MB/s]\n",
            "special_tokens_map.json: 3.55kB [00:00, 10.0MB/s]\n",
            "model.safetensors: 100% 5.48G/5.48G [02:41<00:00, 33.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Example of converting an NLLB model to CTranslate2 with int8 quantization\n",
        "\n",
        "#!ct2-transformers-converter --model facebook/nllb-200-1.3B --quantization int8 --output_dir /content/models/ct2-nllb-200-1.3B-int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbk1jtdCYamS",
        "outputId": "51799859-dc49-42e8-b3ba-3f1a3a2792bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-13 21:16:43--  https://s3.amazonaws.com/opennmt-models/nllb-200/flores200_sacrebleu_tokenizer_spm.model\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.0.37, 52.217.87.206, 52.216.140.174, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.0.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4852054 (4.6M) [binary/octet-stream]\n",
            "Saving to: ‘flores200_sacrebleu_tokenizer_spm.model’\n",
            "\n",
            "flores200_sacrebleu 100%[===================>]   4.63M  9.30MB/s    in 0.5s    \n",
            "\n",
            "2025-07-13 21:16:44 (9.30 MB/s) - ‘flores200_sacrebleu_tokenizer_spm.model’ saved [4852054/4852054]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the SentencePiece model\n",
        "\n",
        "#!wget https://s3.amazonaws.com/opennmt-models/nllb-200/flores200_sacrebleu_tokenizer_spm.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KT7FdCrySL6T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# [Modify] Set paths to the CTranslate2 and SentencePiece models\n",
        "#!mkdir -p /content/models\n",
        "#!cp -r /content/ct2-nllb* /content/models\n",
        "drive = \"../models\"\n",
        "\n",
        "ct_model_path = os.path.join(drive, \"ct2-nllb-200-1.3B-int8\")\n",
        "sp_model_path = os.path.join(drive, \"flores200_sacrebleu_tokenizer_spm.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xt2rEaKPQ4WC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import ctranslate2\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the source SentecePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(sp_model_path)\n",
        "\n",
        "# Load the CTranslate2 model\n",
        "translator = ctranslate2.Translator(ct_model_path, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dg4f3qc-exg3"
      },
      "outputs": [],
      "source": [
        "sp.encode_as_pieces(\"English:\")\n",
        "\n",
        "src_lang = \"eng_Latn\"\n",
        "tgt_lang = \"spa_Latn\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crGs90d-UQj6"
      },
      "source": [
        "# Translate (source sentences only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q9DfomHujcG",
        "outputId": "e80d3899-5c27-41e7-e52e-f7aa0c47501c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.\n",
            "These initiatives are restoring almost five million hectares of marine ecosystems – an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.\n",
            "\n",
            "The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.\n",
            "The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.\n",
            "\n",
            "“After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.\n",
            "But the challenge ahead of us is significant and we need everyone to play their part,” said Inger Andersen, Executive Director of UNEP.\n",
            "“These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.\n",
            "To deliver our restoration goals, our ambition must be as big as the ocean we must protect.”\n",
            "\n",
            "FAO Director-General QU Dongyu said: “The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.\n",
            "These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"\n",
            "\n",
            "The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration – led by UNEP and FAO – which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.\n",
            "The awards track notable initiatives that support global commitments to restore one billion hectares – an area larger than China – by 2030.\n"
          ]
        }
      ],
      "source": [
        "source = \"\"\"The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.\n",
        "These initiatives are restoring almost five million hectares of marine ecosystems – an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.\n",
        "\n",
        "The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.\n",
        "The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.\n",
        "\n",
        "“After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.\n",
        "But the challenge ahead of us is significant and we need everyone to play their part,” said Inger Andersen, Executive Director of UNEP.\n",
        "“These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.\n",
        "To deliver our restoration goals, our ambition must be as big as the ocean we must protect.”\n",
        "\n",
        "FAO Director-General QU Dongyu said: “The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.\n",
        "These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"\n",
        "\n",
        "The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration – led by UNEP and FAO – which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.\n",
        "The awards track notable initiatives that support global commitments to restore one billion hectares – an area larger than China – by 2030.\"\"\"\n",
        "\n",
        "source_sents = [sent.strip() for sent in source.split(\"\\n\")]\n",
        "print(*source_sents, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s33AtXObE5u4",
        "outputId": "1d712a7c-9ff0-44c1-8237-73238265f5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado los primeros buques insignia de restauración mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área de aproximadamente el tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre los Océanos.\n",
            "Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del canal de Mozambique, rica en corales, más de 60 de las islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "Las iniciativas ganadoras se anunciaron en un evento durante la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para el apoyo de las Naciones Unidas.\n",
            "\"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "Pero el desafío que tenemos por delante es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, directora ejecutiva del PNUMA.\n",
            "\"Estos buques insignia de la restauración mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "Para lograr nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenible y la reducción de los recursos naturales están afectando nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de subsistencia de las comunidades dependientes.\n",
            "Estos nuevos buques insignia de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n"
          ]
        }
      ],
      "source": [
        "src_lang = \"eng_Latn\"\n",
        "tgt_lang = \"spa_Latn\"\n",
        "\n",
        "beam_size = 2\n",
        "\n",
        "# Replace special characters in source_sents, like “,«, –\n",
        "source_sents = [sent.replace(\"“\", '\"').replace(\"”\", '\"') for sent in source_sents]\n",
        "source_sents = [sent.replace(\"–\", \"-\") for sent in source_sents]\n",
        "\n",
        "# Remove empty string from source_sents\n",
        "source_sents = [sent.strip() for sent in source_sents if sent.strip()]\n",
        "target_prefix = [[tgt_lang]] * len(source_sents)\n",
        "\n",
        "# Subword the source sentences\n",
        "source_sents_subworded = sp.encode_as_pieces(source_sents)\n",
        "source_sents_subworded = [[src_lang] + sent + [\"</s>\"] for sent in source_sents_subworded]\n",
        "\n",
        "# Translate the source sentences\n",
        "translations = translator.translate_batch(source_sents_subworded,\n",
        "                                          batch_type=\"tokens\",\n",
        "                                          max_batch_size=2024,\n",
        "                                          beam_size=beam_size,\n",
        "                                          target_prefix=target_prefix)\n",
        "translations = [translation.hypotheses[0] for translation in translations]\n",
        "\n",
        "# Desubword the target sentences\n",
        "translations_desubword = sp.decode(translations)\n",
        "translations_desubword = [sent[len(tgt_lang):].strip() for sent in translations_desubword]\n",
        "\n",
        "print(*translations_desubword[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DWX5oeOTXTNK"
      },
      "outputs": [],
      "source": [
        "# Save the translations\n",
        "with open(\"testUNEP.en\", \"w+\") as output:\n",
        "  for translation in translations_desubword:\n",
        "    output.write(translation + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKmQ-y-IxGQZ"
      },
      "source": [
        "# Fuzzy search indexer TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best match result: {'best_fuzzy': 'UN Environment Programme', 'score': 95.65217391304348, 'result': {'French': '', 'Spanish': 'Programa ONU Medio Ambiente', 'Arabic': ''}}\n",
            "All matches in text (no overlaps), 2 results in total: [{'found_in_text': 'ore', 'best_fuzzy': 'ore', 'score': 100.0, 'result': {'French': 'minerais', 'Spanish': 'yacimientos minerales'}}, {'found_in_text': 'UN Environment Programme is', 'best_fuzzy': 'United Nations Environment Programme', 'score': 80.0, 'result': {'French': '', 'Spanish': 'Programa de las Naciones Unidas para el Medio Ambiente'}}]\n",
            "Nearly-exact English matches, 3 in total: [{'found_in_text': 'UN Environment Programme', 'best_fuzzy': 'UN Environment Programme', 'score': 100.0, 'result': {'French': '', 'Spanish': 'Programa ONU Medio Ambiente'}}, {'found_in_text': 'UNEA', 'best_fuzzy': 'UNEA', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'International Day of Women Judge', 'best_fuzzy': 'International Day of Women Judges', 'score': 98.46153846153847, 'result': {'French': 'Journée internationale des femmes juges', 'Spanish': 'Día Internacional de las Juezas'}}]\n",
            "Complex text matches RESULT #4, 9 results in total: [{'found_in_text': 'United Nations Environment Programme', 'best_fuzzy': 'United Nations Environment Programme', 'score': 100.0, 'result': {'French': '', 'Spanish': 'Programa de las Naciones Unidas para el Medio Ambiente'}}, {'found_in_text': 'UNEP', 'best_fuzzy': 'UNEP', 'score': 100.0, 'result': {'French': '', 'Spanish': 'PNUMA'}}, {'found_in_text': 'South-South cooperation', 'best_fuzzy': 'South-South cooperation', 'score': 100.0, 'result': {'French': 'coopération sud-sud', 'Spanish': 'Cooperación Sur-Sur'}}, {'found_in_text': 'Climate Change', 'best_fuzzy': 'climate change', 'score': 100.0, 'result': {'French': 'changements climatiques', 'Spanish': 'cambio climático'}}, {'found_in_text': 'persistent organic pollutants', 'best_fuzzy': 'Persistent Organic Pollutants', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'ore', 'best_fuzzy': 'ore', 'score': 100.0, 'result': {'French': 'minerais', 'Spanish': 'yacimientos minerales'}}, {'found_in_text': 'UNEA', 'best_fuzzy': 'UNEA', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': '1. total greenhouse gas emissions per year', 'best_fuzzy': '1. Total greenhouse gas emissions per year', 'score': 100.0, 'result': {'French': '1. Total des émissions annuelles de gaz à effet de serre', 'Spanish': '1.  Emisiones totales de gases de efecto invernadero por año'}}, {'found_in_text': 'indicator', 'best_fuzzy': 'indicator', 'score': 100.0, 'result': {'French': 'indicateur', 'Spanish': 'indicador'}}]\n",
            "Nearly-exact English matches: 5 matches in total:  [{'found_in_text': 'United Nations Environment Programme', 'best_fuzzy': 'United Nations Environment Programme', 'score': 100.0, 'result': {'French': '', 'Spanish': 'Programa de las Naciones Unidas para el Medio Ambiente'}}, {'found_in_text': '(UNEP)', 'best_fuzzy': 'UNEP', 'score': 100.0, 'result': {'French': '', 'Spanish': 'PNUMA'}}, {'found_in_text': '(South-South cooperation)', 'best_fuzzy': 'South-South cooperation', 'score': 100.0, 'result': {'French': 'coopération sud-sud', 'Spanish': 'Cooperación Sur-Sur'}}, {'found_in_text': 'persistent organic pollutants', 'best_fuzzy': 'persistent organic pollutant', 'score': 98.24561403508773, 'result': {'French': 'polluant organique persistant', 'Spanish': 'contaminante orgánico persistente'}}, {'found_in_text': 'greenhouse gas emissions per year indicator', 'best_fuzzy': '1. Total greenhouse gas emissions per year', 'score': 91.66666666666667, 'result': {'French': '1. Total des émissions annuelles de gaz à effet de serre', 'Spanish': '1.  Emisiones totales de gases de efecto invernadero por año'}}]\n",
            "Bilingual pairs (best_fuzzy, Spanish translation):\n",
            "('United Nations Environment Programme', 'Programa de las Naciones Unidas para el Medio Ambiente')\n",
            "('UNEP', 'PNUMA')\n",
            "('South-South cooperation', 'Cooperación Sur-Sur')\n",
            "('persistent organic pollutant', 'contaminante orgánico persistente')\n",
            "('1. Total greenhouse gas emissions per year', '1.  Emisiones totales de gases de efecto invernadero por año')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Dict, Optional, Union, Callable, Any, Tuple\n",
        "from rapidfuzz import fuzz, process, utils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MultilingualGlossaryProcessor:\n",
        "    \"\"\"\n",
        "    A class for processing multilingual glossaries using RapidFuzz for fuzzy string matching.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, glossary_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the processor with a glossary file.\n",
        "        \n",
        "        Args:\n",
        "            glossary_path: Path to CSV/Excel file with columns:\n",
        "                          Keyword, Category, English, Arabic, French, Spanish, Chinese, Russian, Portuguese, Swahili\n",
        "        \"\"\"\n",
        "        if glossary_path.endswith('.xlsx') or glossary_path.endswith('.xls'):\n",
        "            self.glossary = pd.read_excel(glossary_path)\n",
        "        else:\n",
        "            self.glossary = pd.read_csv(glossary_path)\n",
        "        \n",
        "        # Define available languages\n",
        "        self.languages = ['English', 'Arabic', 'French', 'Spanish', 'Chinese', 'Russian', 'Portuguese', 'Swahili']\n",
        "        \n",
        "        # Validate glossary structure\n",
        "        required_columns = ['Keyword', 'Category'] + self.languages\n",
        "        missing_columns = [col for col in required_columns if col not in self.glossary.columns]\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "    \n",
        "    def find_best_fuzzy_match(\n",
        "        self,\n",
        "        query: str,\n",
        "        source_language: str,\n",
        "        target_languages: List[str],\n",
        "        scorer: Callable = fuzz.WRatio,\n",
        "        processor: Optional[Callable] = None,\n",
        "        score_cutoff: Optional[float] = 60.0,\n",
        "        process_method: str = \"extractOne\"\n",
        "    ) -> Dict[str, Union[str, Dict[str, str]]]:\n",
        "        \"\"\"\n",
        "        Find the best fuzzy match in the glossary for a given query.\n",
        "        \n",
        "        Args:\n",
        "            query: The text to search for\n",
        "            source_language: Language of the query\n",
        "            target_languages: List of target languages to return translations\n",
        "            scorer: RapidFuzz scorer function (default: fuzz.WRatio)\n",
        "            processor: Text preprocessing function (default: None)\n",
        "            score_cutoff: Minimum similarity score (default: 60.0)\n",
        "            process_method: RapidFuzz process method (\"extractOne\", \"extract\", \"cdist\", \"cpdist\")\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with best_fuzzy match and translations in target languages\n",
        "        \"\"\"\n",
        "        if source_language not in self.languages:\n",
        "            raise ValueError(f\"Source language '{source_language}' not supported. Available: {self.languages}\")\n",
        "        \n",
        "        invalid_targets = [lang for lang in target_languages if lang not in self.languages]\n",
        "        if invalid_targets:\n",
        "            raise ValueError(f\"Invalid target languages: {invalid_targets}. Available: {self.languages}\")\n",
        "        \n",
        "        # Get all terms in source language (excluding NaN values)\n",
        "        source_terms = self.glossary[source_language].dropna().tolist()\n",
        "        \n",
        "        if not source_terms:\n",
        "            return {\"best_fuzzy\": \"\", \"result\": {}}\n",
        "        \n",
        "        # Find best match using specified process method\n",
        "        if process_method == \"extractOne\":\n",
        "            result = process.extractOne(\n",
        "                query, \n",
        "                source_terms, \n",
        "                scorer=scorer, \n",
        "                processor=processor, \n",
        "                score_cutoff=score_cutoff\n",
        "            )\n",
        "        elif process_method == \"extract\":\n",
        "            results = process.extract(\n",
        "                query, \n",
        "                source_terms, \n",
        "                scorer=scorer, \n",
        "                processor=processor, \n",
        "                limit=1, \n",
        "                score_cutoff=score_cutoff\n",
        "            )\n",
        "            result = results[0] if results else None\n",
        "        elif process_method == \"cdist\":\n",
        "            # Using cdist for single query\n",
        "            distances = process.cdist(\n",
        "                [query], \n",
        "                source_terms, \n",
        "                scorer=scorer, \n",
        "                processor=processor, \n",
        "                score_cutoff=score_cutoff\n",
        "            )\n",
        "            if distances.size > 0:\n",
        "                best_idx = np.argmax(distances[0])\n",
        "                if distances[0][best_idx] >= (score_cutoff or 0):\n",
        "                    result = (source_terms[best_idx], distances[0][best_idx], best_idx)\n",
        "                else:\n",
        "                    result = None\n",
        "            else:\n",
        "                result = None\n",
        "        elif process_method == \"cpdist\":\n",
        "            # cpdist requires equal length arrays, so we'll use the query repeated\n",
        "            if len(source_terms) > 0:\n",
        "                distances = process.cpdist(\n",
        "                    [query] * len(source_terms), \n",
        "                    source_terms, \n",
        "                    scorer=scorer, \n",
        "                    processor=processor, \n",
        "                    score_cutoff=score_cutoff\n",
        "                )\n",
        "                if distances.size > 0:\n",
        "                    best_idx = np.argmax(distances)\n",
        "                    if distances[best_idx] >= (score_cutoff or 0):\n",
        "                        result = (source_terms[best_idx], distances[best_idx], best_idx)\n",
        "                    else:\n",
        "                        result = None\n",
        "                else:\n",
        "                    result = None\n",
        "            else:\n",
        "                result = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported process method: {process_method}\")\n",
        "        \n",
        "        if not result:\n",
        "            return {\"best_fuzzy\": \"\", \"result\": {}}\n",
        "        \n",
        "        best_match, score, index = result\n",
        "        \n",
        "        # Find the row containing this match\n",
        "        match_row = self.glossary[self.glossary[source_language] == best_match].iloc[0]\n",
        "        \n",
        "        # Get translations for target languages\n",
        "        translations = {}\n",
        "        for lang in target_languages:\n",
        "            translation = match_row[lang]\n",
        "            if pd.notna(translation):\n",
        "                translations[lang] = str(translation)\n",
        "            else:\n",
        "                translations[lang] = \"\"\n",
        "        \n",
        "        return {\n",
        "            \"best_fuzzy\": best_match,\n",
        "            \"score\": score,\n",
        "            \"result\": translations\n",
        "        }\n",
        "    \n",
        "    def _remove_overlapping_matches(self, matches: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Remove overlapping matches, keeping the longest/highest scoring ones.\n",
        "        \n",
        "        Args:\n",
        "            matches: List of match dictionaries with 'start', 'end', 'score', etc.\n",
        "            \n",
        "        Returns:\n",
        "            Filtered list with non-overlapping matches\n",
        "        \"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # Sort by length (descending) then by score (descending)\n",
        "        sorted_matches = sorted(matches, \n",
        "                              key=lambda x: (x['end'] - x['start'], x['score']), \n",
        "                              reverse=True)\n",
        "        \n",
        "        final_matches = []\n",
        "        used_positions = set()\n",
        "        \n",
        "        for match in sorted_matches:\n",
        "            # Check if this match overlaps with any already selected match\n",
        "            match_positions = set(range(match['start'], match['end']))\n",
        "            \n",
        "            if not match_positions.intersection(used_positions):\n",
        "                # No overlap, add this match\n",
        "                final_matches.append(match)\n",
        "                used_positions.update(match_positions)\n",
        "        \n",
        "        # Sort final matches by position in text\n",
        "        final_matches.sort(key=lambda x: x['start'])\n",
        "        return final_matches\n",
        "    \n",
        "    def find_all_fuzzy_matches_in_text(\n",
        "        self,\n",
        "        text: str,\n",
        "        source_language: str,\n",
        "        target_languages: List[str],\n",
        "        scorer: Callable = fuzz.partial_ratio,\n",
        "        processor: Optional[Callable] = None,\n",
        "        score_cutoff: Optional[float] = 80.0,\n",
        "        min_word_length: int = 1,\n",
        "        limit: int = None\n",
        "    ) -> List[Dict[str, Union[str, Dict[str, str]]]]:\n",
        "        \"\"\"\n",
        "        Find all glossary terms that fuzzy match within a given text using extract method and token_set_ratio.\n",
        "        Efficiently searches for glossary entries in the text and handles overlapping matches.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text to search within\n",
        "            source_language: Language of the input text\n",
        "            target_languages: List of target languages to return translations\n",
        "            scorer: RapidFuzz scorer function (default: fuzz.partial_ratio)\n",
        "            processor: Text preprocessing function (default: None)\n",
        "            score_cutoff: Minimum similarity score (default: 80.0)\n",
        "            min_word_length: Minimum length of words to consider (default: 2)\n",
        "            limit: Maximum number of matches to return (default: None for all matches)\n",
        "            \n",
        "        Returns:\n",
        "            List of dictionaries with found matches and their translations\n",
        "        \"\"\"\n",
        "        if source_language not in self.languages:\n",
        "            raise ValueError(f\"Source language '{source_language}' not supported. Available: {self.languages}\")\n",
        "        \n",
        "        invalid_targets = [lang for lang in target_languages if lang not in self.languages]\n",
        "        if invalid_targets:\n",
        "            raise ValueError(f\"Invalid target languages: {invalid_targets}. Available: {self.languages}\")\n",
        "        \n",
        "        # Get all terms in source language (excluding NaN values)\n",
        "        source_terms = self.glossary[source_language].dropna().tolist()\n",
        "        \n",
        "        if not source_terms:\n",
        "            return []\n",
        "        \n",
        "        # Filter terms by minimum word length\n",
        "        filtered_terms = [term for term in source_terms if len(str(term).strip()) >= min_word_length]\n",
        "        \n",
        "        if not filtered_terms:\n",
        "            return []\n",
        "        \n",
        "        # Sort glossary terms by length (longest first) for better matching\n",
        "        source_terms_sorted = sorted(filtered_terms, key=len, reverse=True)\n",
        "        \n",
        "        all_matches = []\n",
        "        \n",
        "        # Use extract method with token_set_ratio as the only scorer\n",
        "        extract_results = process.extract(\n",
        "            text,\n",
        "            source_terms_sorted,\n",
        "            scorer=scorer,\n",
        "            processor=processor,\n",
        "            score_cutoff=score_cutoff,\n",
        "            limit=limit\n",
        "        )\n",
        "        \n",
        "        if not extract_results:\n",
        "            return []\n",
        "        \n",
        "        #print(f\"Found {len(extract_results)} matches in text using extract method with token_set_ratio\")\n",
        "        \n",
        "        # Process each match result\n",
        "        for match_term, similarity, _ in extract_results:\n",
        "            # Find the row in glossary containing this match\n",
        "            match_rows = self.glossary[self.glossary[source_language] == match_term]\n",
        "            \n",
        "            if match_rows.empty:\n",
        "                continue\n",
        "                \n",
        "            match_row = match_rows.iloc[0]\n",
        "            \n",
        "            # Get translations for target languages\n",
        "            translations = {}\n",
        "            for lang in target_languages:\n",
        "                translation = match_row[lang] if lang in match_row else None\n",
        "                if pd.notna(translation):\n",
        "                    translations[lang] = str(translation)\n",
        "                else:\n",
        "                    translations[lang] = \"\"\n",
        "            \n",
        "            # Find approximate positions in text for overlap detection\n",
        "            # Using case-insensitive search to find the term in text\n",
        "            text_lower = text.lower()\n",
        "            term_lower = match_term.lower()\n",
        "            \n",
        "            # Try to find the exact match position\n",
        "            start_pos = text_lower.find(term_lower)\n",
        "            if start_pos != -1:\n",
        "                end_pos = start_pos + len(match_term)\n",
        "            else:\n",
        "                # If exact match not found, use fuzzy position estimation\n",
        "                # Split text into words and try to find approximate position\n",
        "                words = text.split()\n",
        "                best_match_idx = 0\n",
        "                best_score = 0\n",
        "                \n",
        "                term_words = match_term.split()\n",
        "                term_length = len(term_words)\n",
        "                \n",
        "                # Search for best matching position using sliding window\n",
        "                for i in range(len(words) - term_length + 1):\n",
        "                    window_text = \" \".join(words[i:i + term_length])\n",
        "                    window_score = fuzz.token_set_ratio(window_text.lower(), term_lower)\n",
        "                    if window_score > best_score:\n",
        "                        best_score = window_score\n",
        "                        best_match_idx = i\n",
        "                \n",
        "                # Calculate approximate positions based on best match\n",
        "                if best_match_idx < len(words):\n",
        "                    words_before = \" \".join(words[:best_match_idx])\n",
        "                    start_pos = len(words_before) + (1 if words_before else 0)\n",
        "                    \n",
        "                    matched_words = words[best_match_idx:best_match_idx + term_length]\n",
        "                    end_pos = start_pos + len(\" \".join(matched_words))\n",
        "                else:\n",
        "                    start_pos = 0\n",
        "                    end_pos = len(match_term)\n",
        "            \n",
        "            # Extract the actual text segment that was matched\n",
        "            if start_pos >= 0 and end_pos <= len(text):\n",
        "                found_text = text[start_pos:end_pos]\n",
        "            else:\n",
        "                found_text = match_term  # Fallback to the glossary term\n",
        "            \n",
        "            all_matches.append({\n",
        "                \"found_in_text\": found_text,\n",
        "                \"best_fuzzy\": match_term,\n",
        "                \"score\": similarity,\n",
        "                \"result\": translations,\n",
        "                \"start\": start_pos,\n",
        "                \"end\": end_pos\n",
        "            })\n",
        "        \n",
        "        # Remove overlapping matches (prefer longer and higher scoring matches)\n",
        "        final_matches = self._remove_overlapping_matches(all_matches)\n",
        "        \n",
        "        # Remove position information from final output and sort by score\n",
        "        result_matches = []\n",
        "        for match in final_matches:\n",
        "            result_match = {k: v for k, v in match.items() if k not in ['start', 'end']}\n",
        "            result_matches.append(result_match)\n",
        "        \n",
        "        # Sort by score (highest first)\n",
        "        result_matches.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        \n",
        "        return result_matches\n",
        "\n",
        "    def find_nearly_exact_english_matches(\n",
        "        self,\n",
        "        text: str,\n",
        "        target_languages: List[str],\n",
        "        score_cutoff: float = 95.0,\n",
        "        normalize_text: bool = True,\n",
        "        remove_overlaps: bool = True\n",
        "    ) -> List[Dict[str, Union[str, Dict[str, str]]]]:\n",
        "        \"\"\"\n",
        "        Find nearly-exact matches for English glossary terms in the given text.\n",
        "        Optionally normalizes the text and glossary terms before matching.\n",
        "        \n",
        "        Args:\n",
        "            text: Input English text\n",
        "            target_languages: List of target languages to return translations\n",
        "            score_cutoff: Minimum similarity score (default: 95.0)\n",
        "            normalize_text: Whether to normalize text and terms (default: True)\n",
        "            remove_overlaps: Whether to remove overlapping matches (default: True)\n",
        "        \n",
        "        Returns:\n",
        "            List of dictionaries with found matches and their translations\n",
        "        \"\"\"\n",
        "        def normalize(s):\n",
        "            s = str(s).strip()\n",
        "            # Remove numerical substring at the start and its trailing space and punctuation\n",
        "            s = re.sub(r'^\\d+\\s*', '', s) # Example: \"123 term\" -> \"term\", \"123. term\" -> \"term\"\n",
        "            s = re.sub(r'[^\\w\\s]', '', s) # Remove punctuation\n",
        "            s = re.sub(r'\\s+', ' ', s) # Normalize whitespace\n",
        "            return s\n",
        "        \n",
        "        if \"English\" not in self.languages:\n",
        "            raise ValueError(\"English language not available in glossary.\")\n",
        "        \n",
        "        invalid_targets = [lang for lang in target_languages if lang not in self.languages]\n",
        "        if invalid_targets:\n",
        "            raise ValueError(f\"Invalid target languages: {invalid_targets}. Available: {self.languages}\")\n",
        "        \n",
        "        # Get all English terms\n",
        "        english_terms = self.glossary[\"English\"].dropna().tolist()\n",
        "        if not english_terms:\n",
        "            return []\n",
        "        \n",
        "        # Sort terms by length (longest first) for better matching priority\n",
        "        english_terms_sorted = sorted(english_terms, key=len, reverse=True)\n",
        "        \n",
        "        all_matches = []\n",
        "        \n",
        "        # For each glossary term, try to find it in the text\n",
        "        for orig_term in english_terms_sorted:\n",
        "            # Normalize term if requested\n",
        "            if normalize_text:\n",
        "                search_term = normalize(orig_term)\n",
        "                search_text = normalize(text)\n",
        "            else:\n",
        "                search_term = orig_term.strip()\n",
        "                search_text = text.strip()\n",
        "            \n",
        "            # Split into words for position tracking\n",
        "            term_words = search_term.split()\n",
        "            if not term_words:\n",
        "                continue\n",
        "            \n",
        "            # Find all possible matches in the text\n",
        "            text_words = search_text.split()\n",
        "            term_length = len(term_words)\n",
        "            \n",
        "            for i in range(len(text_words) - term_length + 1):\n",
        "                # Get n-gram from text\n",
        "                ngram_words = text_words[i:i + term_length]\n",
        "                ngram_text = \" \".join(ngram_words)\n",
        "                \n",
        "                # Calculate similarity using token_set_ratio for better subset matching\n",
        "                score = fuzz.token_set_ratio(ngram_text, search_term)\n",
        "                \n",
        "                if score >= score_cutoff:\n",
        "                    # Find positions in original text\n",
        "                    # This is approximate since we're working with normalized text\n",
        "                    original_words = text.split()\n",
        "                    if i < len(original_words) and i + term_length <= len(original_words):\n",
        "                        # Get the original text segment\n",
        "                        original_segment = \" \".join(original_words[i:i + term_length])\n",
        "                        \n",
        "                        # Estimate positions (approximate)\n",
        "                        start_pos = text.lower().find(original_segment.lower())\n",
        "                        if start_pos == -1:\n",
        "                            # Fallback: use word-based estimation\n",
        "                            words_before = \" \".join(original_words[:i])\n",
        "                            start_pos = len(words_before) + (1 if words_before else 0)\n",
        "                        end_pos = start_pos + len(original_segment)\n",
        "                        \n",
        "                        # Get translations\n",
        "                        match_row = self.glossary[self.glossary[\"English\"] == orig_term].iloc[0]\n",
        "                        translations = {}\n",
        "                        for lang in target_languages:\n",
        "                            translation = match_row[lang]\n",
        "                            if pd.notna(translation):\n",
        "                                translations[lang] = str(translation)\n",
        "                            else:\n",
        "                                translations[lang] = \"\"\n",
        "                        \n",
        "                        all_matches.append({\n",
        "                            \"found_in_text\": original_segment,\n",
        "                            \"best_fuzzy\": orig_term,\n",
        "                            \"score\": score,\n",
        "                            \"result\": translations,\n",
        "                            \"start\": start_pos,\n",
        "                            \"end\": end_pos\n",
        "                        })\n",
        "        \n",
        "        # Remove overlapping matches if requested\n",
        "        if remove_overlaps:\n",
        "            final_matches = self._remove_overlapping_matches(all_matches)\n",
        "        else:\n",
        "            final_matches = all_matches\n",
        "        \n",
        "        # Remove position information from final output and sort by score\n",
        "        result_matches = []\n",
        "        for match in final_matches:\n",
        "            result_match = {k: v for k, v in match.items() if k not in ['start', 'end']}\n",
        "            result_matches.append(result_match)\n",
        "        \n",
        "        # Sort by score (highest first)\n",
        "        result_matches.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        \n",
        "        return result_matches\n",
        "\n",
        "\n",
        "def create_processor_function(processor_type: str) -> Optional[Callable]:\n",
        "    \"\"\"\n",
        "    Create a processor function based on the specified type.\n",
        "    \n",
        "    Args:\n",
        "        processor_type: Type of processor (\"none\", \"default\", \"custom\")\n",
        "        \n",
        "    Returns:\n",
        "        Processor function or None\n",
        "    \"\"\"\n",
        "    if processor_type == \"none\":\n",
        "        return None\n",
        "    elif processor_type == \"default\":\n",
        "        return utils.default_process\n",
        "    elif processor_type == \"custom\":\n",
        "        # Custom processor that handles special cases\n",
        "        def custom_processor(text):\n",
        "            if not text:\n",
        "                return \"\"\n",
        "            # Remove extra whitespace, keep alphanumeric and spaces\n",
        "            processed = re.sub(r'^\\d+\\s*', '', str(text).strip()) # Example: \"123 term\" -> \"term\", \"123. term\" -> \"term\"\n",
        "            #processed = re.sub(r'[^\\w\\s]', '', processed) # Remove punctuation\n",
        "            processed = re.sub(r'\\s+', ' ', processed)\n",
        "            return processed\n",
        "        return custom_processor\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown processor type: {processor_type}\")\n",
        "\n",
        "\n",
        "# Example usage and testing functions\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example usage of the MultilingualGlossaryProcessor.\n",
        "    \"\"\"\n",
        "    # Initialize processor (assuming you have a glossary file)\n",
        "    processor = MultilingualGlossaryProcessor(\"data/glossaryUNEP_corrected.xlsx\")\n",
        "    \n",
        "    # Example 1: Find best fuzzy match\n",
        "    result1 = processor.find_best_fuzzy_match(\n",
        "        query=\"UN Environment Program\",\n",
        "        source_language=\"English\",\n",
        "        target_languages=[\"French\", \"Spanish\", \"Arabic\"],\n",
        "        scorer=fuzz.WRatio,\n",
        "        processor=utils.default_process,\n",
        "        score_cutoff=70.0,\n",
        "        process_method=\"extractOne\"\n",
        "    )\n",
        "    print(\"Best match result:\", result1)\n",
        "    \n",
        "    # Example 2: Find all matches in text with overlap handling\n",
        "    text = \"UN Environment Programme is sponsored by UNESCO.\"\n",
        "    result2 = processor.find_all_fuzzy_matches_in_text(\n",
        "        text=text,\n",
        "        source_language=\"English\",\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        processor=utils.default_process,\n",
        "        score_cutoff=80.0\n",
        "    )\n",
        "    print(f\"All matches in text (no overlaps), {len(result2)} results in total:\", result2)\n",
        "    \n",
        "    # Example 3: Nearly-exact English matches with normalization\n",
        "    english_text = \"The UN Environment Programme and UNEA are working on the International Day of Women Judge with organizations and developing new policies.\"\n",
        "    result3 = processor.find_nearly_exact_english_matches(\n",
        "        text=english_text,\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        score_cutoff=95.0,\n",
        "        normalize_text=True,\n",
        "        remove_overlaps=True\n",
        "    )\n",
        "    print(f\"Nearly-exact English matches, {len(result3)} in total:\", result3)\n",
        "    \n",
        "    # Example 4: More complex text\n",
        "    complex_text = \"This year, the United Nations Environment Programme (UNEP) and the SSC (South-South cooperation) are presiding the COP on Climate Change to address persistent organic pollutants before the UNEA7 with FAO and UNESCO, where the 1. total greenhouse gas emissions per year indicator is expected to be reduced by 50%.\"\n",
        "    result4 = processor.find_all_fuzzy_matches_in_text(\n",
        "        text=complex_text,\n",
        "        source_language=\"English\",\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        processor=utils.default_process,\n",
        "        score_cutoff=95.0\n",
        "    )\n",
        "    print(f\"Complex text matches RESULT #4, {len(result4)} results in total:\", result4)\n",
        "\n",
        "    # Example 5: Nearly-exact English matches with normalization\n",
        "    english_text = complex_text\n",
        "    result5 = processor.find_nearly_exact_english_matches(\n",
        "        text=english_text,\n",
        "        target_languages=[\"French\", \"Spanish\"],\n",
        "        score_cutoff=90.0,\n",
        "        normalize_text=True,\n",
        "        remove_overlaps=True\n",
        "    )\n",
        "    print(f\"Nearly-exact English matches: {len(result5)} matches in total: \", result5)\n",
        "    # print set of best_fuzzy and result['Spanish'] of result5\n",
        "    bilingual_pairs = [(match['best_fuzzy'], match['result'].get('Spanish', '')) for match in result5]\n",
        "    print(\"Bilingual pairs (best_fuzzy, Spanish translation):\")\n",
        "    for pair in bilingual_pairs:\n",
        "        print(pair)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_usage()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75.67567567567568\n",
            "75.67567567567568\n",
            "79.06976744186046\n",
            "100.0\n",
            "100.0\n",
            "69.23076923076923\n",
            "100.0\n"
          ]
        }
      ],
      "source": [
        "text1 = \"This year, the World Restoration Flagships of United Nations Environment Programme (UNEP) and the SSC (South-South cooperation) are presiding the COP on Climate Change where the Member States will address persistent organic pollutants before the UNEA7 with FAO and UNESCO, where the 1. Total greenhouse gas emissions per year indicator is expected to be reduced by 50%.\"    \n",
        "text2 = \"greenhouse gas emission\"\n",
        "text3 = \"World Restoration Flagship\"\n",
        "text4 = \"United States\"\n",
        "\n",
        "# Compare token ratio fuzzy\n",
        "print(fuzz.token_set_ratio(text1, text2))\n",
        "print(fuzz.token_set_ratio(text2, text1))\n",
        "print(fuzz.token_set_ratio(text3, text1))\n",
        "print(fuzz.token_set_ratio(text4, text1))\n",
        "print(fuzz.partial_ratio(text1, text2))\n",
        "print(fuzz.partial_ratio(text1, text4))\n",
        "print(fuzz.partial_token_set_ratio(text1, text4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complex text matches RESULT: 67 \n",
            "\n",
            " [{'found_in_text': 'South Africa', 'best_fuzzy': 'South Africa', 'score': 100.0, 'result': {'French': 'Afrique du Sud', 'Spanish': 'Sudáfrica'}}, {'found_in_text': 'factory', 'best_fuzzy': 'factory', 'score': 100.0, 'result': {'French': 'usine', 'Spanish': 'fábrica'}}, {'found_in_text': '.  \\n\\nThe ', 'best_fuzzy': 'plant growth', 'score': 100.0, 'result': {'French': 'croissance de la plante', 'Spanish': 'Crecimiento de planta'}}, {'found_in_text': 'plant', 'best_fuzzy': 'plant', 'score': 100.0, 'result': {'French': 'plantes', 'Spanish': 'plantas'}}, {'found_in_text': 'environmental footprint', 'best_fuzzy': 'environmental footprint', 'score': 100.0, 'result': {'French': 'empreinte environnementale', 'Spanish': 'huella ambiental'}}, {'found_in_text': 'United Nations', 'best_fuzzy': 'United Nations', 'score': 100.0, 'result': {'French': 'Nations Unies', 'Spanish': 'naciones Unidas'}}, {'found_in_text': 'Programme', 'best_fuzzy': 'programme', 'score': 100.0, 'result': {'French': 'programme', 'Spanish': 'Programa'}}, {'found_in_text': 'ater and e', 'best_fuzzy': 'energy demand', 'score': 100.0, 'result': {'French': \"demande d'énergie\", 'Spanish': 'demanda de energía'}}, {'found_in_text': 'resources', 'best_fuzzy': 'resources', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'life cycle', 'best_fuzzy': 'life cycle', 'score': 100.0, 'result': {'French': 'cycle biologique', 'Spanish': 'ciclo vital'}}, {'found_in_text': 'assessment', 'best_fuzzy': 'assessment', 'score': 100.0, 'result': {'French': 'évaluation', 'Spanish': 'evaluación'}}, {'found_in_text': 'ent of a jacket’s environmental', 'best_fuzzy': 'environmental impact of industry', 'score': 100.0, 'result': {'French': \"impact environnemental de l'industrie\", 'Spanish': 'impacto ambiental de la industria'}}, {'found_in_text': 'om the produc', 'best_fuzzy': 'production standard', 'score': 100.0, 'result': {'French': 'normes de production', 'Spanish': 'normas de producción'}}, {'found_in_text': 'material', 'best_fuzzy': 'material', 'score': 100.0, 'result': {'French': 'matériaux', 'Spanish': 'materiales'}}, {'found_in_text': 'environmental standard', 'best_fuzzy': 'environmental standard', 'score': 100.0, 'result': {'French': 'normes écologiques', 'Spanish': 'normas ecológicas'}}, {'found_in_text': 'for more sustainable', 'best_fuzzy': 'sustainable production', 'score': 100.0, 'result': {'French': 'production durable', 'Spanish': 'producción sostenible'}}, {'found_in_text': 'an that, bu', 'best_fuzzy': 'business management', 'score': 100.0, 'result': {'French': \"gestion d'entreprises\", 'Spanish': 'administración de empresas'}}, {'found_in_text': 'planet', 'best_fuzzy': 'planet', 'score': 100.0, 'result': {'French': 'planètes', 'Spanish': 'planetas'}}, {'found_in_text': 'life cycle ass', 'best_fuzzy': 'impact assessment', 'score': 100.0, 'result': {'French': \"évaluation de l'impact\", 'Spanish': 'Evaluación de impacto'}}, {'found_in_text': 'textile', 'best_fuzzy': 'textile', 'score': 100.0, 'result': {'French': 'textiles', 'Spanish': 'productos textiles'}}, {'found_in_text': 'Tunisia', 'best_fuzzy': 'Tunisia', 'score': 100.0, 'result': {'French': 'Tunisie', 'Spanish': 'Túnez'}}, {'found_in_text': 'greenhouse gas', 'best_fuzzy': 'greenhouse gas', 'score': 100.0, 'result': {'French': 'gaz à effet de serre', 'Spanish': 'gas de efecto invernadero'}}, {'found_in_text': ' phase out toxic ch', 'best_fuzzy': 'chemicals and waste', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'micals and r', 'best_fuzzy': 'water resource', 'score': 100.0, 'result': {'French': 'ressources en eau', 'Spanish': 'recursos hídricos'}}, {'found_in_text': 'water use', 'best_fuzzy': 'water use', 'score': 100.0, 'result': {'French': \"utilisation de l'eau\", 'Spanish': 'Uso del agua'}}, {'found_in_text': 'he fashion industr', 'best_fuzzy': 'clothing industry', 'score': 100.0, 'result': {'French': 'industrie du vêtement', 'Spanish': 'industria del vestido'}}, {'found_in_text': 'ction is a driver of clim', 'best_fuzzy': 'impact of climate change', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': ' voracious ', 'best_fuzzy': 'consumer waste', 'score': 100.0, 'result': {'French': 'déchets de consommation', 'Spanish': 'desechos posteriores al consumo'}}, {'found_in_text': 'consumer', 'best_fuzzy': 'consumer', 'score': 100.0, 'result': {'French': 'consommateurs', 'Spanish': 'consumidores'}}, {'found_in_text': 'raw material', 'best_fuzzy': 'raw material', 'score': 100.0, 'result': {'French': 'matières premières', 'Spanish': 'materias primas'}}, {'found_in_text': 'ignifican', 'best_fuzzy': 'energy source', 'score': 100.0, 'result': {'French': \"source d'énergie\", 'Spanish': 'fuente de energía'}}, {'found_in_text': 'pollution', 'best_fuzzy': 'pollution', 'score': 100.0, 'result': {'French': 'pollution', 'Spanish': 'contaminación'}}, {'found_in_text': 'ef of UNEP’s ', 'best_fuzzy': 'plant resource', 'score': 100.0, 'result': {'French': 'ressources végétales', 'Spanish': 'recursos vegetales'}}, {'found_in_text': 'design', 'best_fuzzy': 'design', 'score': 100.0, 'result': {'French': 'design', 'Spanish': 'diseño'}}, {'found_in_text': ' the effects', 'best_fuzzy': 'chemical pollution', 'score': 100.0, 'result': {'French': 'pollution chimique', 'Spanish': 'contaminación química'}}, {'found_in_text': 'waste', 'best_fuzzy': 'waste', 'score': 100.0, 'result': {'French': 'déchets', 'Spanish': 'desechos'}}, {'found_in_text': 'Nature', 'best_fuzzy': 'Nature', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'nched the In', 'best_fuzzy': 'programme management', 'score': 100.0, 'result': {'French': 'gestion des programmes', 'Spanish': 'gestión de programas'}}, {'found_in_text': 'European Union', 'best_fuzzy': 'European Union', 'score': 100.0, 'result': {'French': 'Union européenne', 'Spanish': 'Unión Europea'}}, {'found_in_text': 'India', 'best_fuzzy': 'India', 'score': 100.0, 'result': {'French': 'Inde', 'Spanish': 'India'}}, {'found_in_text': 'textile industry', 'best_fuzzy': 'textile industry', 'score': 100.0, 'result': {'French': 'industrie textile', 'Spanish': 'industria textil'}}, {'found_in_text': 'e made to l', 'best_fuzzy': 'material management', 'score': 100.0, 'result': {'French': 'gestion des matériaux', 'Spanish': 'gestión de materiales'}}, {'found_in_text': 'ed, and the us', 'best_fuzzy': 'water resources management', 'score': 100.0, 'result': {'French': 'gestion des ressources en eau', 'Spanish': 'gestión de recursos de agua'}}, {'found_in_text': 'esources', 'best_fuzzy': 'water supply', 'score': 100.0, 'result': {'French': 'alimentation en eau', 'Spanish': 'abastecimiento de agua'}}, {'found_in_text': 'a minimum.', 'best_fuzzy': 'chemical process', 'score': 100.0, 'result': {'French': 'procédé chimique', 'Spanish': 'proceso químico'}}, {'found_in_text': 'process', 'best_fuzzy': 'process', 'score': 100.0, 'result': {'French': 'processus', 'Spanish': 'Proceso'}}, {'found_in_text': '. He’s the head o', 'best_fuzzy': 'environmental sustainability', 'score': 100.0, 'result': {'French': 'durabilité environnementale', 'Spanish': 'sostenibilidad ambiental'}}, {'found_in_text': 'sustainability', 'best_fuzzy': 'sustainability', 'score': 100.0, 'result': {'French': 'durabilité', 'Spanish': 'Sostenibilidad'}}, {'found_in_text': 'the co', 'best_fuzzy': 'deep water', 'score': 100.0, 'result': {'French': 'eaux profondes', 'Spanish': 'aguas profundas'}}, {'found_in_text': 'Group dev', 'best_fuzzy': 'energy resource', 'score': 100.0, 'result': {'French': 'ressources énergétiques', 'Spanish': 'recursos energéticos'}}, {'found_in_text': 'loped an energy manage', 'best_fuzzy': 'toxic waste management', 'score': 100.0, 'result': {'French': 'gestion des déchets toxiques', 'Spanish': 'administración de desperdicios tóxicos'}}, {'found_in_text': 'aste and beco', 'best_fuzzy': 'resource management', 'score': 100.0, 'result': {'French': 'gestion des ressources', 'Spanish': 'Ordenación de recursos'}}, {'found_in_text': 'ficient, ', 'best_fuzzy': 'developing country', 'score': 100.0, 'result': {'French': 'pays en développement', 'Spanish': 'país en desarrollo'}}, {'found_in_text': 't the factor', 'best_fuzzy': 'chemical industry', 'score': 100.0, 'result': {'French': 'industrie chimique', 'Spanish': 'industria química'}}, {'found_in_text': 'l 300 ki', 'best_fuzzy': 'solar system', 'score': 100.0, 'result': {'French': 'système solaire', 'Spanish': 'sistema solar'}}, {'found_in_text': 'roof', 'best_fuzzy': 'roof', 'score': 100.0, 'result': {'French': 'toits', 'Spanish': 'techos'}}, {'found_in_text': 'which would cut el', 'best_fuzzy': 'electricity supply industry', 'score': 100.0, 'result': {'French': 'secteur de l’approvisionnement en électricité', 'Spanish': 'industria de suministro de electricidad'}}, {'found_in_text': 'children', 'best_fuzzy': 'children', 'score': 100.0, 'result': {'French': 'enfants', 'Spanish': 'niños'}}, {'found_in_text': 'emicals it uses meet ', 'best_fuzzy': 'environmental management system', 'score': 100.0, 'result': {'French': 'système de gestion environnementale', 'Spanish': 'sistema de gestión ambiental'}}, {'found_in_text': 's in the entire supply c', 'best_fuzzy': 'supply and demand', 'score': 100.0, 'result': {'French': 'offre et demande', 'Spanish': 'oferta y demanda'}}, {'found_in_text': 'environmental impacts', 'best_fuzzy': 'environmental impacts', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': ' improve acc', 'best_fuzzy': 'business financing', 'score': 100.0, 'result': {'French': \"financement de l'entreprise\", 'Spanish': 'financiamiento de empresas'}}, {'found_in_text': 'financing', 'best_fuzzy': 'financing', 'score': 100.0, 'result': {'French': 'financement', 'Spanish': 'financiación'}}, {'found_in_text': 'ole in building a ', 'best_fuzzy': 'sustainable resource use', 'score': 100.0, 'result': {'French': '', 'Spanish': ''}}, {'found_in_text': 'funds', 'best_fuzzy': 'funds', 'score': 100.0, 'result': {'French': 'fonds', 'Spanish': 'fondo'}}, {'found_in_text': 'vative solutions t', 'best_fuzzy': 'climate change impact', 'score': 100.0, 'result': {'French': 'impact du changement climatique', 'Spanish': 'efectos del cambio climático'}}, {'found_in_text': 'biodiversity', 'best_fuzzy': 'biodiversity', 'score': 100.0, 'result': {'French': 'biodiversité', 'Spanish': 'biodiversidad'}}]\n"
          ]
        }
      ],
      "source": [
        "story1 = \"\"\"\n",
        "\n",
        "Nairobi, 10 July 2025 – As extreme heat grips many countries and becomes “the new normal”, the UN Environment Programme (UNEP) warns of heightened health risks for older persons in the Frontiers 2025 Report published today. Other highlighted impacts of climate change include the melting of glaciers that reawaken ancient pathogens and floods that risk releasing dangerous chemicals. \n",
        "\n",
        "The 7th edition of the Frontiers Report, The Weight of Time - Facing a new age of challenges for people and ecosystems, is part of UNEP’s Foresight Trajectory initiative and highlights emerging environmental issues as well as potential solutions. The first edition in 2016, warned of the growing risk of zoonotic diseases, four years before the COVID-19 pandemic. This report is released as communities across China, Japan, India, Europe, USA and elsewhere face weeks of extreme heat and flooding. \n",
        "\n",
        "“Heat waves are among the most frequent and deadly impacts of climate change, along with floods and shrinking ice cover,” said Inger Andersen, Executive Director of UNEP. “We must be prepared for the risks these impacts pose, especially for society’s most vulnerable, including older persons. Yet as this year’s Frontiers Report shows, solutions exist that can help protect communities and restore ecosystems long-thought to have been lost.” \n",
        "\n",
        "Adults aged 65 and above now form an increasingly dominant part of the world population, particularly in urban areas of low- and middle-income countries. The report notes that annual heat-related deaths among older persons have risen by an estimated 85% since the 1990s. Additional risks arise from deteriorating air quality and floods in low-lying coastal cities where older persons live. \n",
        "\n",
        "Older persons — especially those with chronic illnesses, limited mobility, or frailty — are particularly vulnerable to heat-related health issues, including respiratory, cardiovascular, and metabolic diseases, as well as increased mortality. \n",
        "\n",
        "The report recommends making cities pollution-free, resilient, and accessible spaces with expansive vegetation. Key strategies include better urban planning, community-based disaster risk management, and improved access to climate information for older populations. \n",
        "\n",
        "Earlier this year, the UN Human Rights Council adopted a new resolution to develop an “international legally binding instrument on the human rights of older persons,” a possible path to add safety to those most exposed to climate change. \n",
        "\n",
        "Zombie microbes \n",
        "\n",
        "Beyond the risks to older persons, the report also warns of ancient microbes awakening. Should global temperatures rise more than 2˚C above pre-industrial levels, this would significantly reduce the cryosphere in mass, which includes glaciers, seasonal snow, ice sheets and shelves, sea ice, seasonally frozen ground, and permafrost. Cryospheric regions are home to 670 million people as well as to billions more who live in areas with water originating from those frozen areas. \n",
        "\n",
        "Dormant fungi, bacteria, and viruses in these frozen regions could reactivate, raising the risk of antimicrobial resistance. To slow down the decline of the cryosphere, the Frontiers 2025 Report recommends cutting greenhouse gas emissions – including black carbon emissions from diesel engines, open-field agricultural burning, and wildfires – and limiting tourism in fragile frozen regions. The report also recommends accelerating scientific research into the diversity of cryospheric microorganisms that will not survive the cryosphere’s decline. \n",
        "\n",
        "The return of banned chemicals through floods \n",
        "\n",
        "The report also identifies risks from the remobilization of chemicals that were banned and phased-out decades ago. Floods can bring such chemicals to the surface, after having accumulated in sediment over centuries. \n",
        "\n",
        "As floodwaters stir up sediment and debris, toxic chemicals may be released and re-enter urban areas or food systems. The report lists effective measures to reduce this growing risk: traditional control measures like polders, dikes and retention basins, improved drainage systems, nature-based solutions (e.g., sponge-city approaches), regular monitoring of pollutants in diverse locations and products, and economic impact studies about this type of pollution. \n",
        "\n",
        "The risk of ageing dams \n",
        "\n",
        "Another emerging threat the Frontiers 2025 Report addresses is the risk of ageing dams. Alongside many benefits, dams can harm indigenous and fishing-dependent communities, as well as degrade ecosystems. Removal of large, older dams that have become unsafe, obsolete, or economically unviable is increasingly happening in Europe and North America. \n",
        "\n",
        "The report highlights potential benefits of the removal of dams and barriers in restoring natural river connectivity for biodiversity and ecosystems. Reversing river fragmentation and restoring natural processes support the implementation of the UN’s principles for ecosystem-restoration initiatives . \n",
        "\n",
        " \n",
        "\n",
        "NOTES TO EDITORS \n",
        "\n",
        "About the UN Environment Programme (UNEP) \n",
        "\n",
        "UNEP is the leading global voice on the environment. It provides leadership and encourages partnership in caring for the environment by inspiring, informing and enabling nations and peoples to improve their quality of life without compromising that of future generations. \n",
        "\n",
        "For more information, please contact: \n",
        "\n",
        "News and Media Unit, UN Environment Programme \n",
        "\"\"\"\n",
        "\n",
        "story1 = \"\"\"\n",
        "\n",
        "On the outskirts of Cape Town, South Africa, sits a bustling garment factory owned by Cape Union Mart, one of the country’s best-known outdoor apparel retailers.  \n",
        "\n",
        "The plant produces about 190,000 puffer jackets a year and Cape Union Mart had been searching for ways to reduce its environmental footprint. So, in 2021 the company partnered with the United Nations Environment Programme (UNEP). The goal was to chart how much water and energy were going into each jacket, and figure out how to use fewer resources. \n",
        "\n",
        "This life cycle assessment revealed that more than 80 per cent of a jacket’s environmental impact comes from the production of the fabric it’s made from. That led Cape Union Mart to review its procurement criteria, said Pre-Production Manager Michelle Goddard. The company now only buys material from suppliers that meet stringent environmental standards. \n",
        "\n",
        "“Customers are definitely looking for more sustainable products and, being environmentally responsible makes sense for our brand,” says Goddard. “But even more than that, business as usual is no longer an option. Human activities have caused the Earth’s systems to exceed six of the nine planetary boundaries required for a healthy planet and there is an urgent need for companies to integrate sustainable practices.” \n",
        "\n",
        "The life cycle assessment was part of a larger UNEP effort to support small- and medium-sized textile manufacturers reduce their environmental impact. From Tunisia to South Africa, this UNEP programme has worked with clothing makers to cut greenhouse gas emissions, phase out toxic chemicals and reduce water use. The programme comes at a time when a growing number of experts raise the alarm about the environmental fallout from the fashion industry. Clothing production is a driver of climate change, a voracious consumer of raw materials, and in some places, a significant source of pollution. \n",
        "\n",
        "“The world is realizing that we need to change our approach to fashion and textiles,” says Elisa Tonda, Chief of UNEP’s Resources and Markets branch. “We need to design products to be reuseable, durable and recyclable in order to protect the planet from the effects of pollution and waste.”  \n",
        "\n",
        "On the outskirts of Cape Town, South Africa, sits a bustling garment factory owned by Cape Union Mart, one of the country’s best-known outdoor apparel retailers. The plant produces about 190,000 puffer jackets a year and Cape Union Mart had been searching for ways to reduce its environmental footprint. So, in 2021 the company partnered with the United Nations Environment Programme (UNEP). The goal was to chart how much water and energy were going into each jacket, and figure out how to use fewer resources. This life cycle assessment revealed that more than 80 per cent of a jacket’s environmental impact comes from the production of the fabric it’s made from. That led Cape Union Mart to review its procurement criteria, said Pre-Production Manager Michelle Goddard. The company now only buys material from suppliers that meet stringent environmental standards. “Customers are definitely looking for more sustainable products and, being environmentally responsible makes sense for our brand,” says Goddard. “But even more than that, business as usual is no longer an option. Human activities have caused the Earth’s systems to exceed six of the nine planetary boundaries required for a healthy planet and there is an urgent need for companies to integrate sustainable practices.” The life cycle assessment was part of a larger UNEP effort to support small- and medium-sized textile manufacturers reduce their environmental impact. From Tunisia to South Africa, this UNEP programme has worked with clothing makers to cut greenhouse gas emissions, phase out toxic chemicals and reduce water use. The programme comes at a time when a growing number of experts raise the alarm about the environmental fallout from the fashion industry. Clothing production is a driver of climate change, a voracious consumer of raw materials, and in some places, a significant source of pollution. “The world is realizing that we need to change our approach to fashion and textiles,” says Elisa Tonda, Chief of UNEP’s Resources and Markets branch. “We need to design products to be reuseable, durable and recyclable in order to protect the planet from the effects of pollution and waste.” Name: b5afa1be-ffdf-4b21-97ab-51b5435f1686 Credit: Courtesy Cape Union Mart Description: Women using sewing machines Link: NA Caption: With support from UNEP, South Africa’s Cape Union Mart updated its procurement criteria and now only buys material from suppliers that meet strict environmental standards. Between 2000 and 2015, clothing production doubled globally, according to the Ellen MacArthur Foundation. This rapid growth has expanded the industry’s impact on the environment. Experts say the pain is felt more acutely in developing countries. One study published in the journal Springer Nature found that more than 15,000 chemicals are used in making textiles. As well, producing just 1 kilogramme of textiles takes over 0.5 kilogrammes of chemicals. To help change that, UNEP launched the InTex programme in 2020. Officially called the Innovative Business Practices and Economic Models in the Textile Value Chain, InText is funded by the European Union and Denmark. It focuses on small- and medium-sized enterprises, which are the vast majority of the world’s textile manufacturers, in five nations: India, Indonesia, Kenya, South Africa and Tunisia. InText is part of UNEP’s broader Textile Initiative, which aims to create a cleaner, more sustainable textile industry. At the core of InTex is the idea that the textile industry must move off its linear business model, where resources are gobbled up to make flimsy, disposable clothes. InTex touts a model where garments are made to last, material is recycled, and the use of resources – from water to chemicals – is kept to a minimum. This process is known as circularity and along with sparing the environment, it could generate up to US$700 billion in business opportunities by 2030, found the Ellen MacArthur Foundation. That potential is something Bilel Ben Miled knows well. He’s the head of sustainability at Tunisia’s Gonser Group, a clothing maker. With UNEP’s help, the company did a deep dive into the environmental footprint of the denim it produces in a factory outside of Tunis, the Tunisian capital. The factory produces 600,000 garments a year. Following the assessment, Gonser Group developed an energy management platform to track in real time the use of water, steam and gas in the factory. Ben Miled says the system, expected to launch later this year, will help the company spot waste and become more resource efficient, key in a country facing severe water shortages. Name: 20231019_111932 Credit: Courtesy Gonser Group Description: A factory floor with large washing machines Link: NA Caption: Tunisia’s Gonser Group, with support from UNEP, is evaluating an automated system that could cut the factory’s chemical use by 25 per cent. Gonser Group also plans to install 300 kilowatts of solar panels on the factory’s roof by 2026, which would cut electricity use by 40 per cent. And it’s evaluating an automated chemical dosing system that could cut the factory’s chemical use by 25 per cent. “We need to reduce our environmental footprint if we’re going to leave a better world for our children and the following generations,” says Ben Miled. The work with InTex is part of a broader sustainability push by the Gonser Group. Ben Miled says the company recycles up to 90 per cent of its water. The firm has also certified that more than 90 per cent of the chemicals it uses meet an environmental standard developed for apparel makers. Gonser Group is a major supplier to European brands, many of which are facing stricter environmental rules, especially on chemicals use. One of the big beneftis of UNEP’s InTex programme is that it helps companies improve efficiency, cut costs, and meet the growing demand from sustainable markets in the EU. “Brands are so engaged now when it comes to the environment,” Ben Miled says. “If we want to continue our business activities and expand our markets, we need to invest in sustainability.” UNEP’s Tonda agrees. “By considering sustainability and circularity in their procurement criteria, international brands can influence changes in the entire supply chain, decreasing the overall environmental impacts of production”. Since its launch in 2020, InTex has supported 32 small and medium enterprises develop roadmaps for reducing their environmental and socio-economic impact. Some 230 company representatives have been trained in circularity. The programme is now expanding to India and Indonesia, aiming to work with 60 more business and improve access to financing for small textile makers, which often struggle to get loans. The European Union has recognized InTex for its role in building a more sustainable global textiles industry. “This is an important programme because it shows clothes makers that it is possible to both protect the planet and bolster the bottom line,” says Tonda. “And it also demonstrates that investing in sustainability can give smaller businesses a competitive edge.” ### UNEP's work is made possible by flexible contributions from Member States and other partners to the Environment Fund  and UNEP Climate, Nature and Pollution funds. These funds enable agile, innovative solutions to climate change, nature and biodiversity loss, and pollution and waste. Learn how to support UNEP to invest in people and planet.\n",
        "With support from UNEP, South Africa’s Cape Union Mart updated its procurement criteria and now only buys material from suppliers that meet strict environmental standards. Courtesy Cape Union Mart \n",
        "\n",
        "Between 2000 and 2015, clothing production doubled globally, according to the Ellen MacArthur Foundation. This rapid growth has expanded the industry’s impact on the environment. Experts say the pain is felt more acutely in developing countries. One study published in the journal Springer Nature found that more than 15,000 chemicals are used in making textiles. As well, producing just 1 kilogramme of textiles takes over 0.5 kilogrammes of chemicals. \n",
        "\n",
        "To help change that, UNEP launched the InTex programme in 2020. Officially called the Innovative Business Practices and Economic Models in the Textile Value Chain, InText is funded by the European Union and Denmark. It focuses on small- and medium-sized enterprises, which are the vast majority of the world’s textile manufacturers, in five nations: India, Indonesia, Kenya, South Africa and Tunisia. InText is part of UNEP’s broader Textile Initiative, which aims to create a cleaner, more sustainable textile industry.  \n",
        "\n",
        "At the core of InTex is the idea that the textile industry must move off its linear business model, where resources are gobbled up to make flimsy, disposable clothes. InTex touts a model where garments are made to last, material is recycled, and the use of resources – from water to chemicals – is kept to a minimum. This process is known as circularity and along with sparing the environment, it could generate up to US$700 billion in business opportunities by 2030, found the Ellen MacArthur Foundation. \n",
        "\n",
        "That potential is something Bilel Ben Miled knows well. He’s the head of sustainability at Tunisia’s Gonser Group, a clothing maker. With UNEP’s help, the company did a deep dive into the environmental footprint of the denim it produces in a factory outside of Tunis, the Tunisian capital. The factory produces 600,000 garments a year. \n",
        "\n",
        "Following the assessment, Gonser Group developed an energy management platform to track in real time the use of water, steam and gas in the factory. Ben Miled says the system, expected to launch later this year, will help the company spot waste and become more resource efficient, key in a country facing severe water shortages.   \n",
        "\n",
        "A factory floor with large washing machines \n",
        "Tunisia’s Gonser Group, with support from UNEP, is evaluating an automated system that could cut the factory’s chemical use by 25 per cent. Courtesy Gonser Group \n",
        "\n",
        "Gonser Group also plans to install 300 kilowatts of solar panels on the factory’s roof by 2026, which would cut electricity use by 40 per cent. And it’s evaluating an automated chemical dosing system that could cut the factory’s chemical use by 25 per cent. \n",
        "\n",
        "“We need to reduce our environmental footprint if we’re going to leave a better world for our children and the following generations,” says Ben Miled. \n",
        "\n",
        "The work with InTex is part of a broader sustainability push by the Gonser Group. Ben Miled says the company recycles up to 90 per cent of its water. The firm has also certified that more than 90 per cent of the chemicals it uses meet an environmental standard developed for apparel makers. \n",
        "\n",
        "Gonser Group is a major supplier to European brands, many of which are facing stricter environmental rules, especially on chemicals use. One of the big beneftis of UNEP’s InTex programme is that it helps companies improve efficiency, cut costs, and meet the growing demand from sustainable markets in the EU. \n",
        "\n",
        "“Brands are so engaged now when it comes to the environment,” Ben Miled says. “If we want to continue our business activities and expand our markets, we need to invest in sustainability.” \n",
        "\n",
        "UNEP’s Tonda agrees.  \n",
        "\n",
        "“By considering sustainability and circularity in their procurement criteria, international brands can influence changes in the entire supply chain, decreasing the overall environmental impacts of production.” \n",
        "\n",
        "Since its launch in 2020, InTex has supported 32 small and medium enterprises develop roadmaps for reducing their environmental and socio-economic impact. Some 230 company representatives have been trained in circularity. The programme is now expanding to India and Indonesia, aiming to work with 60 more business and improve access to financing for small textile makers, which often struggle to get loans.  \n",
        "\n",
        "The European Union has recognized InTex for its role in building a more sustainable global textiles industry. \n",
        "\n",
        "“This is an important programme because it shows clothes makers that it is possible to both protect the planet and bolster the bottom line,” says Tonda. “And it also demonstrates that investing in sustainability can give smaller businesses a competitive edge.” \n",
        "\n",
        " \n",
        "\n",
        "UNEP's work is made possible by flexible contributions from Member States and other partners to the Environment Fund  and UNEP Climate, Nature and Pollution funds. These funds enable agile, innovative solutions to climate change, nature and biodiversity loss, and pollution and waste. Learn how to support UNEP to invest in people and planet. \n",
        "\"\"\"\n",
        "\n",
        "# Example 6: More complex text\n",
        "processor = MultilingualGlossaryProcessor(\"data/glossaryUNEP_corrected.xlsx\")\n",
        "result7 = processor.find_all_fuzzy_matches_in_text(\n",
        "    text=story1,\n",
        "    source_language=\"English\",\n",
        "    target_languages=[\"French\", \"Spanish\"],\n",
        "    scorer=fuzz.token_set_ratio,\n",
        "    processor=create_processor_function(\"custom\"),\n",
        "    score_cutoff=95.0\n",
        ")\n",
        "print(\"Complex text matches RESULT:\", len(result7),\"\\n\\n\", result7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['South Africa', 'factory', 'plant growth', 'plant', 'environmental footprint', 'United Nations', 'programme', 'energy demand', 'resources', 'life cycle', 'assessment', 'environmental impact of industry', 'production standard', 'material', 'environmental standard', 'sustainable production', 'business management', 'planet', 'impact assessment', 'textile', 'Tunisia', 'greenhouse gas', 'chemicals and waste', 'water resource', 'water use', 'clothing industry', 'impact of climate change', 'consumer waste', 'consumer', 'raw material', 'energy source', 'pollution', 'plant resource', 'design', 'chemical pollution', 'waste', 'Nature', 'programme management', 'European Union', 'India', 'textile industry', 'material management', 'water resources management', 'water supply', 'chemical process', 'process', 'environmental sustainability', 'sustainability', 'deep water', 'energy resource', 'toxic waste management', 'resource management', 'developing country', 'chemical industry', 'solar system', 'roof', 'electricity supply industry', 'children', 'environmental management system', 'supply and demand', 'environmental impacts', 'business financing', 'financing', 'sustainable resource use', 'funds', 'climate change impact', 'biodiversity']\n"
          ]
        }
      ],
      "source": [
        "# Get list of str of key best_fuzzy\n",
        "result7 = [match['best_fuzzy'] for match in result7 if 'best_fuzzy' in match]\n",
        "\n",
        "print(result7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('surface water management', 66.66666666666667, 0)]\n",
            "Matches found: []\n"
          ]
        }
      ],
      "source": [
        "from rapidfuzz import fuzz, process\n",
        "two_words = \"surface water management\"\n",
        "print(process.extract(story1, [two_words], scorer=fuzz.partial_ratio, limit=5))\n",
        "\n",
        "# pretty print of result6\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "#pp.pprint(result6)\n",
        "\n",
        "\n",
        "import re\n",
        "# Create a regex pattern to match first token in two_words until second token\n",
        "re_pattern = rf\"{re.escape(two_words.split()[0])}.*{re.escape(two_words.split()[-1])}\"\n",
        "re_patinverted = rf\"{re.escape(two_words.split()[-1])}.*{re.escape(two_words.split()[0])}\"\n",
        "# Find all matches in the story\n",
        "matches = re.findall(re_pattern, story1, flags=re.IGNORECASE)\n",
        "matches += re.findall(re_patinverted, story1, flags=re.IGNORECASE)\n",
        "matches = list(set(matches))\n",
        "print(\"Matches found:\", matches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of matches found for token_set_radio: 81\n",
            "Number of matches found for partial_ratio: 81\n",
            "Both methods found the same matches.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of matches found for token_set_radio: {len(result6)}\")\n",
        "print(f\"Number of matches found for partial_ratio: {len(result7)}\")\n",
        "if result7==result6:\n",
        "    print(\"Both methods found the same matches.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking if best_fuzzy values from result6 are contained in story1 (WITH STEMMING):\n",
            "======================================================================\n",
            "\n",
            "Term: 'South Africa'\n",
            "Found in story: True\n",
            "Original words: {'first': 'South', 'last': 'Africa'}\n",
            "Stemmed words: {'first': 'south', 'last': 'africa'}\n",
            "Stemmed matches: 1 - ['South Africa']\n",
            "Exact matches: 1 - ['South Africa']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'factory'\n",
            "Found in story: False\n",
            "Original word: factory\n",
            "Stemmed word: factori\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'plant growth'\n",
            "Found in story: False\n",
            "Original words: {'first': 'plant', 'last': 'growth'}\n",
            "Stemmed words: {'first': 'plant', 'last': 'growth'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'plant'\n",
            "Found in story: True\n",
            "Original word: plant\n",
            "Stemmed word: plant\n",
            "Matches: 2 - ['plant', 'plant']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'environmental footprint'\n",
            "Found in story: True\n",
            "Original words: {'first': 'environmental', 'last': 'footprint'}\n",
            "Stemmed words: {'first': 'environment', 'last': 'footprint'}\n",
            "Stemmed matches: 1 - ['environmental footprint']\n",
            "Exact matches: 1 - ['environmental footprint']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'United Nations'\n",
            "Found in story: True\n",
            "Original words: {'first': 'United', 'last': 'Nations'}\n",
            "Stemmed words: {'first': 'unit', 'last': 'nation'}\n",
            "Stemmed matches: 1 - ['United Nations']\n",
            "Exact matches: 1 - ['United Nations']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'programme'\n",
            "Found in story: True\n",
            "Original word: programme\n",
            "Stemmed word: programm\n",
            "Matches: 14 - ['Programme', 'programme', 'programme', 'Programme', 'programme', 'programme', 'programme', 'programme', 'programme', 'programme', 'programme', 'programme', 'programme', 'programme']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'energy demand'\n",
            "Found in story: False\n",
            "Original words: {'first': 'energy', 'last': 'demand'}\n",
            "Stemmed words: {'first': 'energi', 'last': 'demand'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'resources'\n",
            "Found in story: True\n",
            "Original word: resources\n",
            "Stemmed word: resourc\n",
            "Matches: 10 - ['resources', 'Resources', 'resources', 'Resources', 'resources', 'resources', 'resource', 'resources', 'resources', 'resource']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'life cycle'\n",
            "Found in story: True\n",
            "Original words: {'first': 'life', 'last': 'cycle'}\n",
            "Stemmed words: {'first': 'life', 'last': 'cycl'}\n",
            "Stemmed matches: 1 - ['life cycle']\n",
            "Exact matches: 1 - ['life cycle']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'assessment'\n",
            "Found in story: True\n",
            "Original word: assessment\n",
            "Stemmed word: assess\n",
            "Matches: 6 - ['assessment', 'assessment', 'assessment', 'assessment', 'assessment', 'assessment']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'environmental impact of industry'\n",
            "Found in story: False\n",
            "Original words: {'first': 'environmental', 'last': 'industry'}\n",
            "Stemmed words: {'first': 'environment', 'last': 'industri'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'production standard'\n",
            "Found in story: False\n",
            "Original words: {'first': 'production', 'last': 'standard'}\n",
            "Stemmed words: {'first': 'product', 'last': 'standard'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'material'\n",
            "Found in story: True\n",
            "Original word: material\n",
            "Stemmed word: materi\n",
            "Matches: 8 - ['material', 'materials', 'material', 'materials', 'material', 'material', 'material', 'material']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'environmental standard'\n",
            "Found in story: True\n",
            "Original words: {'first': 'environmental', 'last': 'standard'}\n",
            "Stemmed words: {'first': 'environment', 'last': 'standard'}\n",
            "Stemmed matches: 2 - ['environmental standard', 'environmental standards']\n",
            "Exact matches: 1 - ['environmental standard']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'sustainable production'\n",
            "Found in story: True\n",
            "Original words: {'first': 'sustainable', 'last': 'production'}\n",
            "Stemmed words: {'first': 'sustain', 'last': 'product'}\n",
            "Stemmed matches: 1 - ['sustainable products']\n",
            "Exact matches: 0 - []\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'business management'\n",
            "Found in story: False\n",
            "Original words: {'first': 'business', 'last': 'management'}\n",
            "Stemmed words: {'first': 'busi', 'last': 'manag'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'planet'\n",
            "Found in story: True\n",
            "Original word: planet\n",
            "Stemmed word: planet\n",
            "Matches: 10 - ['planetary', 'planet', 'planet', 'planetary', 'planet', 'planet', 'planet', 'planet', 'planet', 'planet']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'impact assessment'\n",
            "Found in story: False\n",
            "Original words: {'first': 'impact', 'last': 'assessment'}\n",
            "Stemmed words: {'first': 'impact', 'last': 'assess'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'textile'\n",
            "Found in story: True\n",
            "Original word: textile\n",
            "Stemmed word: textil\n",
            "Matches: 22 - ['textile', 'textiles', 'textile', 'textiles', 'textiles', 'textiles', 'Textile', 'textile', 'Textile', 'textile', 'textile', 'textile', 'textiles', 'textiles', 'textiles', 'Textile', 'textile', 'Textile', 'textile', 'textile', 'textile', 'textiles']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'Tunisia'\n",
            "Found in story: True\n",
            "Original word: Tunisia\n",
            "Stemmed word: tunisia\n",
            "Matches: 10 - ['Tunisia', 'Tunisia', 'Tunisia', 'Tunisia', 'Tunisian', 'Tunisia', 'Tunisia', 'Tunisia', 'Tunisian', 'Tunisia']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'greenhouse gas'\n",
            "Found in story: True\n",
            "Original words: {'first': 'greenhouse', 'last': 'gas'}\n",
            "Stemmed words: {'first': 'greenhous', 'last': 'ga'}\n",
            "Stemmed matches: 1 - ['greenhouse gas']\n",
            "Exact matches: 1 - ['greenhouse gas']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'chemicals and waste'\n",
            "Found in story: False\n",
            "Original words: {'first': 'chemicals', 'last': 'waste'}\n",
            "Stemmed words: {'first': 'chemic', 'last': 'wast'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'water resource'\n",
            "Found in story: False\n",
            "Original words: {'first': 'water', 'last': 'resource'}\n",
            "Stemmed words: {'first': 'water', 'last': 'resourc'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'water use'\n",
            "Found in story: True\n",
            "Original words: {'first': 'water', 'last': 'use'}\n",
            "Stemmed words: {'first': 'water', 'last': 'use'}\n",
            "Stemmed matches: 2 - ['use of water', 'water use']\n",
            "Exact matches: 2 - ['use of water', 'water use']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'clothing industry'\n",
            "Found in story: False\n",
            "Original words: {'first': 'clothing', 'last': 'industry'}\n",
            "Stemmed words: {'first': 'cloth', 'last': 'industri'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'impact of climate change'\n",
            "Found in story: False\n",
            "Original words: {'first': 'impact', 'last': 'change'}\n",
            "Stemmed words: {'first': 'impact', 'last': 'chang'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'consumer waste'\n",
            "Found in story: False\n",
            "Original words: {'first': 'consumer', 'last': 'waste'}\n",
            "Stemmed words: {'first': 'consum', 'last': 'wast'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'consumer'\n",
            "Found in story: True\n",
            "Original word: consumer\n",
            "Stemmed word: consum\n",
            "Matches: 2 - ['consumer', 'consumer']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'raw material'\n",
            "Found in story: True\n",
            "Original words: {'first': 'raw', 'last': 'material'}\n",
            "Stemmed words: {'first': 'raw', 'last': 'materi'}\n",
            "Stemmed matches: 1 - ['raw materials']\n",
            "Exact matches: 0 - []\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'energy source'\n",
            "Found in story: False\n",
            "Original words: {'first': 'energy', 'last': 'source'}\n",
            "Stemmed words: {'first': 'energi', 'last': 'sourc'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'pollution'\n",
            "Found in story: True\n",
            "Original word: pollution\n",
            "Stemmed word: pollut\n",
            "Matches: 8 - ['pollution', 'pollution', 'pollution', 'pollution', 'Pollution', 'pollution', 'Pollution', 'pollution']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'plant resource'\n",
            "Found in story: False\n",
            "Original words: {'first': 'plant', 'last': 'resource'}\n",
            "Stemmed words: {'first': 'plant', 'last': 'resourc'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'design'\n",
            "Found in story: True\n",
            "Original word: design\n",
            "Stemmed word: design\n",
            "Matches: 2 - ['design', 'design']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'chemical pollution'\n",
            "Found in story: False\n",
            "Original words: {'first': 'chemical', 'last': 'pollution'}\n",
            "Stemmed words: {'first': 'chemic', 'last': 'pollut'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'waste'\n",
            "Found in story: True\n",
            "Original word: waste\n",
            "Stemmed word: wast\n",
            "Matches: 6 - ['waste', 'waste', 'waste', 'waste', 'waste', 'waste']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'Nature'\n",
            "Found in story: True\n",
            "Original word: Nature\n",
            "Stemmed word: natur\n",
            "Matches: 6 - ['Nature', 'Nature', 'nature', 'Nature', 'Nature', 'nature']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'programme management'\n",
            "Found in story: False\n",
            "Original words: {'first': 'programme', 'last': 'management'}\n",
            "Stemmed words: {'first': 'programm', 'last': 'manag'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'European Union'\n",
            "Found in story: True\n",
            "Original words: {'first': 'European', 'last': 'Union'}\n",
            "Stemmed words: {'first': 'european', 'last': 'union'}\n",
            "Stemmed matches: 1 - ['European Union']\n",
            "Exact matches: 1 - ['European Union']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'India'\n",
            "Found in story: True\n",
            "Original word: India\n",
            "Stemmed word: india\n",
            "Matches: 4 - ['India', 'India', 'India', 'India']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'textile industry'\n",
            "Found in story: True\n",
            "Original words: {'first': 'textile', 'last': 'industry'}\n",
            "Stemmed words: {'first': 'textil', 'last': 'industri'}\n",
            "Stemmed matches: 0 - []\n",
            "Exact matches: 1 - ['textile industry']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'material management'\n",
            "Found in story: False\n",
            "Original words: {'first': 'material', 'last': 'management'}\n",
            "Stemmed words: {'first': 'materi', 'last': 'manag'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'water resources management'\n",
            "Found in story: False\n",
            "Original words: {'first': 'water', 'last': 'management'}\n",
            "Stemmed words: {'first': 'water', 'last': 'manag'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'water supply'\n",
            "Found in story: False\n",
            "Original words: {'first': 'water', 'last': 'supply'}\n",
            "Stemmed words: {'first': 'water', 'last': 'suppli'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'chemical process'\n",
            "Found in story: False\n",
            "Original words: {'first': 'chemical', 'last': 'process'}\n",
            "Stemmed words: {'first': 'chemic', 'last': 'process'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'process'\n",
            "Found in story: True\n",
            "Original word: process\n",
            "Stemmed word: process\n",
            "Matches: 2 - ['process', 'process']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'environmental sustainability'\n",
            "Found in story: False\n",
            "Original words: {'first': 'environmental', 'last': 'sustainability'}\n",
            "Stemmed words: {'first': 'environment', 'last': 'sustain'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'sustainability'\n",
            "Found in story: True\n",
            "Original word: sustainability\n",
            "Stemmed word: sustain\n",
            "Matches: 20 - ['sustainable', 'sustainable', 'sustainable', 'sustainable', 'sustainable', 'sustainability', 'sustainability', 'sustainable', 'sustainability', 'sustainability', 'sustainable', 'sustainability', 'sustainable', 'sustainability', 'sustainability', 'sustainable', 'sustainability', 'sustainability', 'sustainable', 'sustainability']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'deep water'\n",
            "Found in story: False\n",
            "Original words: {'first': 'deep', 'last': 'water'}\n",
            "Stemmed words: {'first': 'deep', 'last': 'water'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'energy resource'\n",
            "Found in story: False\n",
            "Original words: {'first': 'energy', 'last': 'resource'}\n",
            "Stemmed words: {'first': 'energi', 'last': 'resourc'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'toxic waste management'\n",
            "Found in story: False\n",
            "Original words: {'first': 'toxic', 'last': 'management'}\n",
            "Stemmed words: {'first': 'toxic', 'last': 'manag'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'resource management'\n",
            "Found in story: False\n",
            "Original words: {'first': 'resource', 'last': 'management'}\n",
            "Stemmed words: {'first': 'resourc', 'last': 'manag'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'developing country'\n",
            "Found in story: True\n",
            "Original words: {'first': 'developing', 'last': 'country'}\n",
            "Stemmed words: {'first': 'develop', 'last': 'countri'}\n",
            "Stemmed matches: 1 - ['developing countries']\n",
            "Exact matches: 0 - []\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'chemical industry'\n",
            "Found in story: False\n",
            "Original words: {'first': 'chemical', 'last': 'industry'}\n",
            "Stemmed words: {'first': 'chemic', 'last': 'industri'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'solar system'\n",
            "Found in story: False\n",
            "Original words: {'first': 'solar', 'last': 'system'}\n",
            "Stemmed words: {'first': 'solar', 'last': 'system'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'roof'\n",
            "Found in story: True\n",
            "Original word: roof\n",
            "Stemmed word: roof\n",
            "Matches: 2 - ['roof', 'roof']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'electricity supply industry'\n",
            "Found in story: False\n",
            "Original words: {'first': 'electricity', 'last': 'industry'}\n",
            "Stemmed words: {'first': 'electr', 'last': 'industri'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'children'\n",
            "Found in story: True\n",
            "Original word: children\n",
            "Stemmed word: children\n",
            "Matches: 2 - ['children', 'children']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'environmental management system'\n",
            "Found in story: False\n",
            "Original words: {'first': 'environmental', 'last': 'system'}\n",
            "Stemmed words: {'first': 'environment', 'last': 'system'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'supply and demand'\n",
            "Found in story: False\n",
            "Original words: {'first': 'supply', 'last': 'demand'}\n",
            "Stemmed words: {'first': 'suppli', 'last': 'demand'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'environmental impacts'\n",
            "Found in story: True\n",
            "Original words: {'first': 'environmental', 'last': 'impacts'}\n",
            "Stemmed words: {'first': 'environment', 'last': 'impact'}\n",
            "Stemmed matches: 3 - ['impact on the environment', 'environmental impact', 'environmental impacts']\n",
            "Exact matches: 1 - ['environmental impacts']\n",
            "Patterns used: ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'business financing'\n",
            "Found in story: False\n",
            "Original words: {'first': 'business', 'last': 'financing'}\n",
            "Stemmed words: {'first': 'busi', 'last': 'financ'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'financing'\n",
            "Found in story: True\n",
            "Original word: financing\n",
            "Stemmed word: financ\n",
            "Matches: 2 - ['financing', 'financing']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'sustainable resource use'\n",
            "Found in story: False\n",
            "Original words: {'first': 'sustainable', 'last': 'use'}\n",
            "Stemmed words: {'first': 'sustain', 'last': 'use'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'funds'\n",
            "Found in story: True\n",
            "Original word: funds\n",
            "Stemmed word: fund\n",
            "Matches: 8 - ['funded', 'Fund', 'funds', 'funds', 'funded', 'Fund', 'funds', 'funds']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'climate change impact'\n",
            "Found in story: False\n",
            "Original words: {'first': 'climate', 'last': 'impact'}\n",
            "Stemmed words: {'first': 'climat', 'last': 'impact'}\n",
            "No matches found\n",
            "--------------------------------------------------\n",
            "\n",
            "Term: 'biodiversity'\n",
            "Found in story: True\n",
            "Original word: biodiversity\n",
            "Stemmed word: biodivers\n",
            "Matches: 2 - ['biodiversity', 'biodiversity']\n",
            "Patterns used: ['stemmed_word_boundary']\n",
            "--------------------------------------------------\n",
            "\n",
            "Total terms found in story: 34 out of 67\n",
            "Found terms: ['South Africa', 'plant', 'environmental footprint', 'United Nations', 'programme', 'resources', 'life cycle', 'assessment', 'material', 'environmental standard', 'sustainable production', 'planet', 'textile', 'Tunisia', 'greenhouse gas', 'water use', 'consumer', 'raw material', 'pollution', 'design', 'waste', 'Nature', 'European Union', 'India', 'textile industry', 'process', 'sustainability', 'developing country', 'roof', 'children', 'environmental impacts', 'financing', 'funds', 'biodiversity']\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK if needed and import stemming functionality\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.stem import PorterStemmer\n",
        "    # Download required NLTK data if not present\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "except ImportError:\n",
        "    print(\"Installing NLTK...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
        "    import nltk\n",
        "    from nltk.stem import PorterStemmer\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def check_best_fuzzy_in_story_with_stemming(result_list, story_text):\n",
        "    \"\"\"\n",
        "    Check if the 'best_fuzzy' values from the input result list are contained in story1.\n",
        "    Uses regex patterns to match both normal and inverted token order with stemming applied.\n",
        "    \n",
        "    Args:\n",
        "        result_list: List of dictionaries containing 'best_fuzzy' keys\n",
        "        story_text: The story text to search within\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with match results for each best_fuzzy term\n",
        "    \"\"\"\n",
        "    if not result_list:\n",
        "        return {}\n",
        "    \n",
        "    match_results = {}\n",
        "    \n",
        "    for item in result_list:\n",
        "        if 'best_fuzzy' not in item:\n",
        "            continue\n",
        "            \n",
        "        best_fuzzy = item['best_fuzzy']\n",
        "        words = best_fuzzy.split()\n",
        "        \n",
        "        if len(words) < 2:\n",
        "            # For single words, apply stemming and do case-insensitive search\n",
        "            original_word = words[0]\n",
        "            stemmed_word = stemmer.stem(original_word.lower())\n",
        "            \n",
        "            # Create pattern that matches the stem with possible suffixes\n",
        "            pattern = rf\"\\b{re.escape(stemmed_word)}\\w*\\b\"\n",
        "            matches = re.findall(pattern, story_text, flags=re.IGNORECASE)\n",
        "            \n",
        "            match_results[best_fuzzy] = {\n",
        "                'found': bool(matches),\n",
        "                'matches': matches,\n",
        "                'patterns_used': ['stemmed_word_boundary'],\n",
        "                'original_word': original_word,\n",
        "                'stemmed_word': stemmed_word,\n",
        "                'pattern_used': pattern\n",
        "            }\n",
        "        else:\n",
        "            # For multi-word terms, apply stemming to first and last words\n",
        "            first_word = words[0]\n",
        "            last_word = words[-1]\n",
        "            \n",
        "            # Apply stemming\n",
        "            stemmed_first = stemmer.stem(first_word.lower())\n",
        "            stemmed_last = stemmer.stem(last_word.lower())\n",
        "            \n",
        "            # Create patterns using stemmed words with possible suffixes\n",
        "            # Normal pattern: stemmed_first + suffixes ... (max 2 words) ... stemmed_last + suffixes\n",
        "            re_pattern = rf\"\\b{re.escape(stemmed_first)}\\w*(?:\\s+\\w+){{0,2}}\\s+\\b{re.escape(stemmed_last)}\\w*\\b\"\n",
        "            # Inverted pattern: stemmed_last + suffixes ... (max 2 words) ... stemmed_first + suffixes\n",
        "            re_par_inverted = rf\"\\b{re.escape(stemmed_last)}\\w*(?:\\s+\\w+){{0,2}}\\s+\\b{re.escape(stemmed_first)}\\w*\\b\"\n",
        "\n",
        "            # Also create exact word patterns for comparison with same word limit\n",
        "            exact_pattern = rf\"\\b{re.escape(first_word)}(?:\\s+\\w+){{0,2}}\\s+\\b{re.escape(last_word)}\\b\"\n",
        "            exact_inverted = rf\"\\b{re.escape(last_word)}(?:\\s+\\w+){{0,2}}\\s+\\b{re.escape(first_word)}\\b\"\n",
        "            \n",
        "            # Find matches with stemmed patterns\n",
        "            stemmed_normal_matches = re.findall(re_pattern, story_text, flags=re.IGNORECASE)\n",
        "            stemmed_inverted_matches = re.findall(re_par_inverted, story_text, flags=re.IGNORECASE)\n",
        "            \n",
        "            # Find matches with exact patterns for comparison\n",
        "            exact_normal_matches = re.findall(exact_pattern, story_text, flags=re.IGNORECASE)\n",
        "            exact_inverted_matches = re.findall(exact_inverted, story_text, flags=re.IGNORECASE)\n",
        "            \n",
        "            # Combine all matches and deduplicate\n",
        "            all_stemmed_matches = list(set(stemmed_normal_matches + stemmed_inverted_matches))\n",
        "            all_exact_matches = list(set(exact_normal_matches + exact_inverted_matches))\n",
        "            \n",
        "            match_results[best_fuzzy] = {\n",
        "                'found': bool(all_stemmed_matches or all_exact_matches),\n",
        "                'stemmed_matches': all_stemmed_matches,\n",
        "                'exact_matches': all_exact_matches,\n",
        "                'stemmed_normal_matches': stemmed_normal_matches,\n",
        "                'stemmed_inverted_matches': stemmed_inverted_matches,\n",
        "                'exact_normal_matches': exact_normal_matches,\n",
        "                'exact_inverted_matches': exact_inverted_matches,\n",
        "                'patterns_used': ['stemmed_normal', 'stemmed_inverted', 'exact_normal', 'exact_inverted'],\n",
        "                'original_words': {'first': first_word, 'last': last_word},\n",
        "                'stemmed_words': {'first': stemmed_first, 'last': stemmed_last},\n",
        "                'stemmed_patterns': {'normal': re_pattern, 'inverted': re_par_inverted},\n",
        "                'exact_patterns': {'normal': exact_pattern, 'inverted': exact_inverted}\n",
        "            }\n",
        "    \n",
        "    return match_results\n",
        "\n",
        "# Test the enhanced function with result6 and story1\n",
        "print(\"Checking if best_fuzzy values from result6 are contained in story1 (WITH STEMMING):\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "fuzzy_check_results_stemmed = check_best_fuzzy_in_story_with_stemming(result7, story1)\n",
        "\n",
        "for term, result in fuzzy_check_results_stemmed.items():\n",
        "    print(f\"\\nTerm: '{term}'\")\n",
        "    print(f\"Found in story: {result['found']}\")\n",
        "    \n",
        "    if 'stemmed_words' in result:\n",
        "        print(f\"Original words: {result['original_words']}\")\n",
        "        print(f\"Stemmed words: {result['stemmed_words']}\")\n",
        "    elif 'stemmed_word' in result:\n",
        "        print(f\"Original word: {result['original_word']}\")\n",
        "        print(f\"Stemmed word: {result['stemmed_word']}\")\n",
        "    \n",
        "    if result['found']:\n",
        "        if 'stemmed_matches' in result:\n",
        "            print(f\"Stemmed matches: {len(result['stemmed_matches'])} - {result['stemmed_matches']}\")\n",
        "            print(f\"Exact matches: {len(result['exact_matches'])} - {result['exact_matches']}\")\n",
        "        else:\n",
        "            print(f\"Matches: {len(result['matches'])} - {result['matches']}\")\n",
        "        print(f\"Patterns used: {result['patterns_used']}\")\n",
        "    else:\n",
        "        print(\"No matches found\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Print totals\n",
        "total_found = sum(1 for result in fuzzy_check_results_stemmed.values() if result['found'])\n",
        "print(f\"\\nTotal terms found in story: {total_found} out of {len(result7)}\")\n",
        "\n",
        "# list of found terms in fuzzy_check_results_stemmed\n",
        "found_terms = [term for term, result in fuzzy_check_results_stemmed.items() if result['found']]\n",
        "print(f\"Found terms: {found_terms}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxqtfbGuUY2b"
      },
      "source": [
        "# Translate (with fuzzy matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXaOfYnwxO1S",
        "outputId": "70e5b823-d217-4db7-fc53-6e4649d6b15a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of fuzzy source and fuzzy target prefixes:\n",
            "10\n",
            "10\n",
            "The United Nations Environment Programme (UNEP) and the Food and Agriculture Organization (FAO) have named the first World Restoration Flagships for this year, which address ecosystem degradation across the globe.\n",
            "El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la degradación de los ecosistemas en todo el planeta.\n"
          ]
        }
      ],
      "source": [
        "similar_text = \"\"\"El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la degradación de los ecosistemas en todo el planeta.\n",
        "Estas iniciativas han estado restaurando alrededor tres millones de hectáreas de ecosistemas marinos, un área del tamaño de El Salvador.\n",
        "Las siete nuevas Iniciativas Emblemáticas comprenden iniciativas de restauración en Ecuador, Colombia, Kenya e Indonesia.\n",
        ".\n",
        "\"Por mucho tiempo se ha dado por sentado el poder de los bosques, tan esenciales para la restauración.\n",
        "Cada persona debe cumplir su parte\", afirmó Inger Andersen, Directora Ejecutiva del PNUMA.\n",
        "\"Las Iniciativas Emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
        "Para lograr nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
        "El Director General de la FAO, QU Dongyu, manifestó: \"La crisis climática, las prácticas de explotación insostenible y la reducción de los recursos naturales están afectando nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades.\n",
        "Estas nuevas 7 Iniciativas Emblemáticas muestran que detener y revertir la degradación es posible y beneficioso para el planeta y las personas\".\"\"\"\n",
        "fuzzy_sents = [sent.strip() for sent in similar_text.split(\"\\n\")]\n",
        "fuzzy_target_prefixes = [sent.strip() for sent in fuzzy_sents if sent.strip()]\n",
        "\n",
        "similar_text_en = \"\"\"The United Nations Environment Programme (UNEP) and the Food and Agriculture Organization (FAO) have named the first World Restoration Flagships for this year, which address ecosystem degradation across the globe.\n",
        "These initiatives have been restoring around three million hectares of marine ecosystems, an area the size of El Salvador.\n",
        "The seven new flagships include restoration initiatives in Ecuador, Colombia, Kenya and Indonesia.\n",
        ".\n",
        "\"The power of forests, so essential to restoration, has long been taken for granted.\n",
        "Everyone must do their part,\" said Inger Andersen, Executive Director of UNEP.\n",
        "\"The World Restoration Flagships show how biodiversity protection, climate action and economic development are deeply interconnected.\n",
        "To achieve our restoration goals, our ambition must be as big as the ocean we must protect.\"\n",
        "FAO Director-General QU Dongyu said, \"The climate crisis, unsustainable exploitation practices and depletion of natural resources are affecting our blue ecosystems, damaging marine life and threatening the livelihoods of communities.\n",
        "These new 7 flagships show that halting and reversing degradation is possible and beneficial for the planet and people.\"\n",
        "\"\"\"\n",
        "\n",
        "#glossary_entry = [\"World Restoration Flagships\", \"Iniciativas Emblemáticas de la Restauración Mundial\"] #plural\n",
        "#glossary_entry = [\"World Restoration Flagship\", \"Iniciativa Emblemática de la Restauración Mundial\"] #singular\n",
        "#glossary_entry = [\"world restoration flagship\", \"iniciativa emblemática de la restauración mundial\"] #singular_lowercase\n",
        "\n",
        "fuzzy_src_sents = [sent.strip() for sent in similar_text_en.split(\"\\n\")]\n",
        "fuzzy_source_sentences = [sent.strip() for sent in fuzzy_src_sents if sent.strip()]\n",
        "\n",
        "# Replace first and second elements of source and target texts with the glossary entry\n",
        "#fuzzy_source_sentences[0] = glossary_entry[0]\n",
        "#fuzzy_target_prefixes[0] = glossary_entry[1]\n",
        "\n",
        "print(\"Length of fuzzy source and fuzzy target prefixes:\")\n",
        "print(len(fuzzy_source_sentences))\n",
        "print(len(fuzzy_target_prefixes))\n",
        "\n",
        "print(fuzzy_source_sentences[0])\n",
        "print(fuzzy_target_prefixes[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative of fuzzy entries ready for xl8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matches found for source sentence 'The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.': 4 - [('pollution', 'contaminación'), ('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial'), ('UN Environment Programme', 'Programa ONU Medio Ambiente'), ('invasive alien species', 'Especie exótica invasiva')]\n",
            "Matches found for source sentence 'These initiatives are restoring almost five million hectares of marine ecosystems - an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.': 4 - [('marine ecosystems', 'ecosistemas marinos'), ('Costa Rica', 'Costa Rica'), ('France', 'Francia'), ('conference', 'conferencias')]\n",
            "Matches found for source sentence 'The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.': 7 - [('restoration', 'Restauración'), ('Mozambique', 'Mozambique'), ('Spain', 'España'), ('Mexico', 'México'), ('island', 'islas'), ('Europe', 'Europa'), ('forest ecosystem', 'ecosistemas forestales')]\n",
            "Matches found for source sentence 'The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.': 2 - [('France', 'Francia'), ('conference', 'conferencias')]\n",
            "Matches found for source sentence '\"After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.': 2 - [('ocean', 'océanos'), ('restoration', 'Restauración')]\n",
            "Matches found for source sentence 'But the challenge ahead of us is significant and we need everyone to play their part,\" said Inger Andersen, Executive Director of UNEP.': 2 - [('Executive Director', 'Director Ejecutivo'), ('UNEP', 'PNUMA')]\n",
            "Matches found for source sentence '\"These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.': 4 - [('biodiversity protection', 'protección de la diversidad biológica'), ('climate', 'clima'), ('economic development', 'desarrollo económico'), ('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial')]\n",
            "Matches found for source sentence 'To deliver our restoration goals, our ambition must be as big as the ocean we must protect.\"': 2 - [('restoration', 'Restauración'), ('ocean', 'océanos')]\n",
            "Matches found for source sentence 'FAO Director-General QU Dongyu said: \"The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.': 4 - [('climate', 'clima'), ('ecosystems', 'ecosistemas'), ('life', 'vida'), ('livelihood', 'Medio de vida')]\n",
            "Matches found for source sentence 'These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"': 3 - [('degradation', 'degradación'), ('planet', 'planetas'), ('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial')]\n",
            "Matches found for source sentence 'The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration - led by UNEP and FAO - which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.': 7 - [('World Restoration Flagship', 'Iniciativa Emblemática de la Restauración Mundial'), ('UNEP', 'PNUMA'), ('degradation', 'degradación'), ('ecosystems', 'ecosistemas'), ('ocean', 'océanos'), ('award', 'premios'), ('ecosystem restoration', 'restauración de los ecosistemas')]\n",
            "Matches found for source sentence 'The awards track notable initiatives that support global commitments to restore one billion hectares - an area larger than China - by 2030.': 2 - [('China', 'China'), ('award', 'premios')]\n"
          ]
        }
      ],
      "source": [
        "processor = MultilingualGlossaryProcessor(\"data/glossaryUNEP_corrected.xlsx\")\n",
        "\n",
        "glossary_matches = []\n",
        "\n",
        "for src_sent in source_sents:\n",
        "    sent_matches = processor.find_nearly_exact_english_matches(\n",
        "        text=src_sent,\n",
        "        target_languages=[\"Spanish\"],\n",
        "        score_cutoff=90.0,\n",
        "        normalize_text=True,\n",
        "        remove_overlaps=True\n",
        "    )\n",
        "\n",
        "    # filter sent_matches as a list of str of best_fuzzy and results['Spanish'] if both are not empty\n",
        "    sent_matches = [(match['best_fuzzy'], match['result'].get('Spanish', '')) for match in sent_matches if 'best_fuzzy' in match and 'Spanish' in match['result']]\n",
        "    # remove set in sent_matches if best_fuzzy is empty or Spanish translation is empty\n",
        "    sent_matches = [match for match in sent_matches if match[0] and match[1]]\n",
        "    if sent_matches:\n",
        "        print(f\"Matches found for source sentence '{src_sent}': {len(sent_matches)}\"\n",
        "              f\" - {sent_matches}\")\n",
        "        glossary_matches.append(sent_matches)\n",
        "    else:\n",
        "        # add empty tuple if no matches found\n",
        "        print(f\"No matches found for source sentence '{src_sent}'\")\n",
        "        glossary_matches.append((\"\", \"\"))\n",
        "\n",
        "# Transform glossary_matches into a list of tuples (A, B) where A is joined string from first elements of each tuple in glossary_matches and B is joined string from second elements of each tuple in glossary_matches\n",
        "glossary_matches = [(\", \".join([match[0] for match in matches]),\n",
        "                                 \", \".join([match[1] for match in matches])) for matches in glossary_matches]\n",
        "\n",
        "# separate the glossary_matches into two lists: first elements and second elements\n",
        "glossary_matches_src = [match[0] for match in glossary_matches]\n",
        "glossary_matches_tgt = [match[1] for match in glossary_matches]\n",
        "\n",
        "fuzzy_source_sentences = glossary_matches_src\n",
        "fuzzy_target_prefixes = glossary_matches_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translation inserting matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpxNHl8GF4F8",
        "outputId": "e6ee9b4b-5b0e-4d36-e0a1-819fa7cc1214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['eng_Latn', '▁pollu', 'tion', ',', '▁World', '▁Rest', 'oration', '▁Flag', 'ship', ',', '▁UN', '▁Environment', '▁Programme', ',', '▁invasi', 've', '▁alien', '▁species', 'eng_Latn', '▁•', '▁The', '▁UN', '▁Environment', '▁Programme', '▁(', 'UN', 'EP', ')', '▁and', '▁the', '▁Food', '▁and', '▁Agric', 'ulture', '▁Organization', '▁of', '▁the', '▁UN', '▁(', 'FA', 'O', ')', '▁have', '▁named', '▁the', '▁first', '▁World', '▁Rest', 'oration', '▁Flag', 'shi', 'ps', '▁for', '▁this', '▁year', ',', '▁tack', 'ling', '▁pollu', 'tion', ',', '▁uns', 'usta', 'inable', '▁explo', 'itation', ',', '▁and', '▁invasi', 've', '▁species', '▁in', '▁three', '▁contin', 'ents', '.', '</s>']\n",
            "['spa_Latn', '▁contamina', 'ción', ',', '▁Inici', 'ativa', '▁Emb', 'lem', 'ática', '▁de', '▁la', '▁Resta', 'uración', '▁Mundial', ',', '▁Programa', '▁ONU', '▁Medio', '▁Ambiente', ',', '▁Es', 'pe', 'cie', '▁ex', 'ó', 'tica', '▁invasi', 'va', 'spa_Latn', '▁•']\n",
            "\n",
            "Translations:\n",
            "contaminación, Iniciativa Emblemática de la Restauración Mundial, Programa ONU Medio Ambiente, Especie exótica invasivaspa_Latn • El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "ecosistemas marinos, Costa Rica, Francia, conferenciasspa_Latn • Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área del tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre el Océano.\n",
            "Restauración, Mozambique, España, México, islas, Europa, ecosistemas forestalesspa_Latn • Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del Canal de Mozambique del Norte, rica en corales, más de 60 islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "Francia, conferenciasspa_Latn • Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para recibir el apoyo de las Naciones Unidas.\n",
            "océanos, Restauraciónspa_Latn • \"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "Director Ejecutivo, PNUMAspa_Latn • Pero el desafío que nos espera es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "protección de la diversidad biológica, clima, desarrollo económico, Iniciativa Emblemática de la Restauración Mundialspa_Latn • \"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "Restauración, océanosspa_Latn • Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "clima, ecosistemas, vida, Medio de vidaspa_Latn • El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "degradación, planetas, Iniciativa Emblemática de la Restauración Mundialspa_Latn • Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n",
            "\n",
            "Translations only:\n",
            "• El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "• Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área del tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre el Océano.\n",
            "• Los tres nuevos buques insignia incluyen iniciativas de restauración en la región del Canal de Mozambique del Norte, rica en corales, más de 60 islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "• Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre el Océano en Niza, Francia, y ahora son elegibles para recibir el apoyo de las Naciones Unidas.\n",
            "• \"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "• Pero el desafío que nos espera es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "• \"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "• Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "• El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "• Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n"
          ]
        }
      ],
      "source": [
        "import ctranslate2\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "\n",
        "src_lang = \"eng_Latn\"\n",
        "tgt_lang = \"spa_Latn\"\n",
        "\n",
        "beam_size = 2\n",
        "\n",
        "# Load the source SentecePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(sp_model_path)\n",
        "\n",
        "\n",
        "# Subword the source sentences\n",
        "fuzzy_source_sentences_subworded = sp.encode_as_pieces(fuzzy_source_sentences)\n",
        "real_source_sentences_subworded = sp.encode_as_pieces(source_sents)\n",
        "fuzzy_real_subworded = zip(fuzzy_source_sentences_subworded, real_source_sentences_subworded)\n",
        "\n",
        "separator = sp.encode_as_pieces(\"•\")  # tokenize \"•\" -- output is \"▁•\"\n",
        "\n",
        "source_sents_subworded = [[src_lang] + fuzzy_src + [src_lang] + separator + real_src + [\"</s>\"]\n",
        "                          for fuzzy_src, real_src in fuzzy_real_subworded]\n",
        "#source_sents_subworded = [[src_lang] + fuzzy_src + [src_lang] + separator + [\"</s>\"]\n",
        "                          #for fuzzy_src in fuzzy_source_sentences_subworded]\n",
        "print(source_sents_subworded[0])\n",
        "\n",
        "prefixes_subworded = sp.encode_as_pieces(fuzzy_target_prefixes)\n",
        "target_prefixes = [[tgt_lang] + sent + [tgt_lang] + separator for sent in prefixes_subworded]\n",
        "print(target_prefixes[0])\n",
        "\n",
        "# Translate the source sentences\n",
        "translator = ctranslate2.Translator(ct_model_path, device=device)\n",
        "translations = translator.translate_batch(source_sents_subworded,\n",
        "                                          batch_type=\"tokens\",\n",
        "                                          max_batch_size=2024,\n",
        "                                          beam_size=beam_size,\n",
        "                                          min_decoding_length=2,\n",
        "                                          max_decoding_length=512,\n",
        "                                          target_prefix=target_prefixes)\n",
        "translations = [translation.hypotheses[0] for translation in translations]\n",
        "\n",
        "# Desubword the target sentences\n",
        "translations_desubword = sp.decode(translations)\n",
        "translations_desubword = [sent[len(tgt_lang):].strip() for sent in translations_desubword]\n",
        "\n",
        "translations_only = [sent.split(tgt_lang)[1].strip() for sent in translations_desubword]\n",
        "\n",
        "print(\"\\nTranslations:\", *translations_desubword[:10], sep=\"\\n\")\n",
        "print(\"\\nTranslations only:\", *translations_only[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7jGt_Ij6J5Xn"
      },
      "outputs": [],
      "source": [
        "translations_only = [sent.split(tgt_lang)[1].strip() for sent in translations_desubword]\n",
        "translations_only = [sent[1:].strip() if sent.startswith(\"•\") else sent.strip() for sent in translations_only]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "6p9mxL3WL0pZ",
        "outputId": "a572b653-cd35-48b5-bd01-c43092a02631"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translations_only[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "fQEk9087q0Ml"
      },
      "outputs": [],
      "source": [
        "# Save the translations\n",
        "\n",
        "translations_file_name = \"testUNEP.es\"\n",
        "\n",
        "with open(translations_file_name, \"w+\") as output:\n",
        "  for translation in translations_only:\n",
        "    output.write(translation + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJyoXuaY8qqe"
      },
      "source": [
        "# Show in parallel print each line of testUNEP.en and testUNEP.es\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cunliud8-7S",
        "outputId": "44b49a1e-1b6f-4284-c125-d149ab7e9f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "12\n",
            "contaminación , Iniciativa Emblemática de la Restauración Mundial , Programa ONU Medio Ambiente , Especie exótica invasivaspa_Latn • El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "El Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Alimentación y la Agricultura (FAO) han nombrado las primeras Iniciativas Emblemáticas de la Restauración Mundial para este año, que abordan la contaminación, la explotación insostenible y las especies invasoras en tres continentes.\n",
            "The UN Environment Programme (UNEP) and the Food and Agriculture Organization of the UN (FAO) have named the first World Restoration Flagships for this year, tackling pollution, unsustainable exploitation, and invasive species in three continents.\n",
            "\n",
            "ecosistemas marinos , Costa Rica , Francia , conferenciasspa_Latn • Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área de aproximadamente el tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre los Océanos.\n",
            "Estas iniciativas están restaurando casi cinco millones de hectáreas de ecosistemas marinos, un área de aproximadamente el tamaño de Costa Rica, que, junto con Francia, acoge la 3a Conferencia de las Naciones Unidas sobre los Océanos.\n",
            "These initiatives are restoring almost five million hectares of marine ecosystems - an area about the size of Costa Rica, which, together with France, is hosting the 3rd UN Ocean Conference.\n",
            "\n",
            "Restauración , Mozambique , España , México , islas , Europa , ecosistemas forestalesspa_Latn • Los tres nuevos buques insignia comprenden iniciativas de restauración en la región del Canal de Mozambique, rica en corales, más de 60 de las islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "Los tres nuevos buques insignia comprenden iniciativas de restauración en la región del Canal de Mozambique, rica en corales, más de 60 de las islas de México y el Mar Menor en España, el primer ecosistema europeo con personalidad jurídica.\n",
            "The three new flagships comprise restoration initiatives in the coral-rich Northern Mozambique Channel Region, more than 60 of Mexico’s islands and the Mar Menor in Spain, Europe’s first ecosystem with legal personhood.\n",
            "\n",
            "Francia , conferenciasspa_Latn • Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre los Océanos en Niza, Francia, y ahora son elegibles para recibir apoyo de las Naciones Unidas.\n",
            "Las iniciativas ganadoras se anunciaron en un evento de la Conferencia de las Naciones Unidas sobre los Océanos en Niza, Francia, y ahora son elegibles para recibir apoyo de las Naciones Unidas.\n",
            "The winning initiatives were announced at an event during the UN Ocean Conference in Nice, France, and are now eligible for UN support.\n",
            "\n",
            "océanos , Restauraciónspa_Latn • \"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "\"Después de décadas de dar el océano por sentado, estamos presenciando un gran cambio hacia la restauración.\n",
            "\"After decades of taking the ocean for granted, we are witnessing a great shift towards restoration.\n",
            "\n",
            "Director Ejecutivo , PNUMAspa_Latn • Pero el desafío que tenemos por delante es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "Pero el desafío que tenemos por delante es significativo y necesitamos que todos desempeñen su papel\", dijo Inger Andersen, Directora Ejecutiva del PNUMA.\n",
            "But the challenge ahead of us is significant and we need everyone to play their part,\" said Inger Andersen, Executive Director of UNEP.\n",
            "\n",
            "protección de la diversidad biológica , clima , desarrollo económico , Iniciativa Emblemática de la Restauración Mundialspa_Latn • \"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "\"Estas Iniciativas emblemáticas de la Restauración Mundial muestran cómo la protección de la biodiversidad, la acción climática y el desarrollo económico están profundamente interconectados.\n",
            "\"These World Restoration Flagships show how biodiversity protection, climate action, and economic development are deeply interconnected.\n",
            "\n",
            "Restauración , océanosspa_Latn • Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "Para alcanzar nuestros objetivos de restauración, nuestra ambición debe ser tan grande como el océano que debemos proteger\".\n",
            "To deliver our restoration goals, our ambition must be as big as the ocean we must protect.\"\n",
            "\n",
            "clima , ecosistemas , vida , Medio de vidaspa_Latn • El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "El Director General de la FAO, QU Dongyu, dijo: \"La crisis climática, las prácticas de explotación insostenibles y la disminución de los recursos naturales están afectando a nuestros ecosistemas azules, dañando la vida marina y amenazando los medios de vida de las comunidades dependientes.\n",
            "FAO Director-General QU Dongyu said: \"The climate crisis, unsustainable exploitation practices and nature resources shrinking are affecting our blue ecosystems, harming marine life and threatening the livelihoods of dependent communities.\n",
            "\n",
            "degradación , planetas , Iniciativa Emblemática de la Restauración Mundialspa_Latn • Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n",
            "Estas nuevas Iniciativas Emblemáticas de la Restauración Mundial muestran que detener y revertir la degradación no sólo es posible, sino que también es beneficioso para el planeta y las personas\".\n",
            "These new World Restoration Flagships show that halting and reversing degradation is not only possible, but also beneficial to planet and people.\"\n",
            "\n",
            "Iniciativa Emblemática de la Restauración Mundial , PNUMA , degradación , ecosistemas , océanos , premios , restauración de los ecosistemasspa_Latn • Los Premios Emblemáticos de la Restauración Mundial forman parte de la Década de las Naciones Unidas para la Restauración de los Ecosistemas - liderada por el PNUMA y la FAO - que tiene como objetivo prevenir, detener y revertir la degradación de los ecosistemas en todos los continentes y en todos los océanos.\n",
            "Los Premios Emblemáticos de la Restauración Mundial forman parte de la Década de las Naciones Unidas para la Restauración de los Ecosistemas - liderada por el PNUMA y la FAO - que tiene como objetivo prevenir, detener y revertir la degradación de los ecosistemas en todos los continentes y en todos los océanos.\n",
            "The World Restoration Flagship awards are part of the UN Decade on Ecosystem Restoration - led by UNEP and FAO - which aims to prevent, halt, and reverse the degradation of ecosystems on every continent and in every ocean.\n",
            "\n",
            "China , premiosspa_Latn • Los premios se centran en iniciativas notables que respaldan los compromisos globales para restaurar mil millones de hectáreas - un área más grande que China - para 2030.\n",
            "Los premios se centran en iniciativas notables que respaldan los compromisos globales para restaurar mil millones de hectáreas - un área más grande que China - para 2030.\n",
            "The awards track notable initiatives that support global commitments to restore one billion hectares - an area larger than China - by 2030.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# translations_desubword and translations_only\n",
        "print(len(translations_desubword))\n",
        "print(len(translations_only))\n",
        "\n",
        "for i in range(len(translations_desubword)):\n",
        "  print(translations_desubword[i])\n",
        "  if i > len(translations_only) - 1:\n",
        "    print()\n",
        "  else:\n",
        "    print(translations_only[i])\n",
        "    print(source_sents[i])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "m8rsx_DO-m_k",
        "outputId": "3b8d1c50-a71b-4109-c6fc-3fdc4b783a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/models/flores200_sacrebleu_tokenizer_spm.model (deflated 51%)\n",
            "  adding: content/models/ct2-nllb-200-1.3B-int8/ (stored 0%)\n",
            "  adding: content/models/ct2-nllb-200-1.3B-int8/model.bin (deflated 8%)\n",
            "  adding: content/models/ct2-nllb-200-1.3B-int8/config.json (deflated 44%)\n",
            "  adding: content/models/ct2-nllb-200-1.3B-int8/shared_vocabulary.json (deflated 72%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ab63dd8f-fbbb-4c52-b72e-f205d3806491\", \"models.zip\", 1279554870)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# zip and download folder \"/content/models\" to avoid conversion at every run\n",
        "\n",
        "#!zip -r /content/models.zip /content/models\n",
        "\n",
        "#download zip file from Google Colab\n",
        "#from google.colab import files\n",
        "#files.download(\"/content/models.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Glossary simple basic replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded with encoding: latin1\n",
            "Test replacement:\n",
            "Prismaradona\n",
            "This is Prismaradona and other text\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 74\u001b[0m\n\u001b[0;32m     70\u001b[0m                 child\u001b[38;5;241m.\u001b[39mreplace_with(new_text)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Process the HTML content\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Uncomment this when ready to apply changes\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m process_text_content(soup\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;28;01melse\u001b[39;00m soup)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Alternative: Process specific elements (paragraphs, table cells, etc.)\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Uncomment to process paragraphs\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# for para in soup.find_all('p'):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# with open(\"/content/Cronología_Krosmoz_glossary_corrected.htm\", \"w\", encoding=\"utf-8\") as file:\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m#     file.write(str(soup))\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHTML processing setup complete. Uncomment the processing sections when ready to apply changes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mprocess_text_content\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m element\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(child, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 66\u001b[0m         process_text_content(child)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(child, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace_with\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# Process text nodes\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         new_text \u001b[38;5;241m=\u001b[39m exact_replace(\u001b[38;5;28mstr\u001b[39m(child), glossary_dict)\n",
            "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mprocess_text_content\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m element\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(child, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 66\u001b[0m         process_text_content(child)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(child, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace_with\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# Process text nodes\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         new_text \u001b[38;5;241m=\u001b[39m exact_replace(\u001b[38;5;28mstr\u001b[39m(child), glossary_dict)\n",
            "    \u001b[1;31m[... skipping similar frames: process_text_content at line 66 (1 times)]\u001b[0m\n",
            "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mprocess_text_content\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m element\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(child, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 66\u001b[0m         process_text_content(child)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(child, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace_with\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# Process text nodes\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         new_text \u001b[38;5;241m=\u001b[39m exact_replace(\u001b[38;5;28mstr\u001b[39m(child), glossary_dict)\n",
            "Cell \u001b[1;32mIn[2], line 60\u001b[0m, in \u001b[0;36mprocess_text_content\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m element\u001b[38;5;241m.\u001b[39mstring:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# If element has direct text content, process it\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     new_text \u001b[38;5;241m=\u001b[39m exact_replace(element\u001b[38;5;241m.\u001b[39mstring, glossary_dict)\n\u001b[0;32m     61\u001b[0m     element\u001b[38;5;241m.\u001b[39mstring\u001b[38;5;241m.\u001b[39mreplace_with(new_text)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# If element has children, process them recursively\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[2], line 17\u001b[0m, in \u001b[0;36mexact_replace\u001b[1;34m(text, glossary_dict)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Replace exact matches in text while preserving case and punctuation\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m english_term, spanish_term \u001b[38;5;129;01min\u001b[39;00m glossary_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Use word boundaries to match complete words only\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m re\u001b[38;5;241m.\u001b[39mescape(english_term) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, spanish_term, text)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
            "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\re\\__init__.py:255\u001b[0m, in \u001b[0;36mescape\u001b[1;34m(pattern)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# SPECIAL_CHARS\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# closing ')', '}' and ']'\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# '-' (a range in character set)\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# '&', '~', (extended character set operations)\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# '#' (comment) and WHITESPACE (ignored) in verbose mode\u001b[39;00m\n\u001b[0;32m    253\u001b[0m _special_chars_map \u001b[38;5;241m=\u001b[39m {i: \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mchr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m()[]\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m?*+-|^$\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m.&~# \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\v\u001b[39;00m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m--> 255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mescape\u001b[39m(pattern):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Escape special characters in a string.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pattern, \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Load glossary\n",
        "glossary = pd.read_csv(r\"C:\\Users\\Nelso\\Downloads\\Dofus_names_translations.csv\")\n",
        "glossary_dict = dict(zip(glossary['en'].astype(str), glossary['es'].astype(str)))\n",
        "\n",
        "# Add \"Prysmarodoth:Prismarodona\" to dict\n",
        "glossary_dict[\"Prysmarodoth\"] = \"Prismaradona\"\n",
        "\n",
        "# Exact replace function (no fuzzy matching, preserves case)\n",
        "def exact_replace(text, glossary_dict):\n",
        "    \"\"\"Replace exact matches in text while preserving case and punctuation\"\"\"\n",
        "    for english_term, spanish_term in glossary_dict.items():\n",
        "        # Use word boundaries to match complete words only\n",
        "        pattern = r'\\b' + re.escape(english_term) + r'\\b'\n",
        "        text = re.sub(pattern, spanish_term, text)\n",
        "    return text\n",
        "\n",
        "# Try different encodings\n",
        "file_path = r\"C:\\Users\\Nelso\\Downloads\\Cronología Krosmoz.htm\"\n",
        "encodings_to_try = ['latin1', 'iso-8859-1', 'cp1252', 'utf-8']\n",
        "\n",
        "html_content = None\n",
        "used_encoding = None\n",
        "\n",
        "for encoding in encodings_to_try:\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=encoding) as file:\n",
        "            html_content = file.read()\n",
        "            used_encoding = encoding\n",
        "            print(f\"Successfully loaded with encoding: {encoding}\")\n",
        "            break\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Failed with encoding: {encoding}\")\n",
        "        continue\n",
        "\n",
        "if html_content is None:\n",
        "    print(\"Could not decode the file with any of the tried encodings\")\n",
        "    exit()\n",
        "\n",
        "# Parse HTML with BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Test for exact_replace\n",
        "print(\"Test replacement:\")\n",
        "print(exact_replace(\"Prysmarodoth\", glossary_dict))\n",
        "print(exact_replace(\"This is Prysmarodoth and other text\", glossary_dict))\n",
        "\n",
        "# Function to process text nodes while preserving HTML structure\n",
        "def process_text_content(element):\n",
        "    \"\"\"Recursively process text content in HTML elements\"\"\"\n",
        "    if element.name in ['script', 'style']:\n",
        "        # Skip script and style tags\n",
        "        return\n",
        "    \n",
        "    if element.string:\n",
        "        # If element has direct text content, process it\n",
        "        new_text = exact_replace(element.string, glossary_dict)\n",
        "        element.string.replace_with(new_text)\n",
        "    else:\n",
        "        # If element has children, process them recursively\n",
        "        for child in element.children:\n",
        "            if hasattr(child, 'name'):\n",
        "                process_text_content(child)\n",
        "            elif hasattr(child, 'replace_with'):\n",
        "                # Process text nodes\n",
        "                new_text = exact_replace(str(child), glossary_dict)\n",
        "                child.replace_with(new_text)\n",
        "\n",
        "# Process the HTML content\n",
        "# Uncomment this when ready to apply changes\n",
        "process_text_content(soup.body if soup.body else soup)\n",
        "\n",
        "# Alternative: Process specific elements (paragraphs, table cells, etc.)\n",
        "# Uncomment to process paragraphs\n",
        "# for para in soup.find_all('p'):\n",
        "#     if para.get_text(strip=True):\n",
        "#         process_text_content(para)\n",
        "\n",
        "# Uncomment to process table cells\n",
        "# for cell in soup.find_all(['td', 'th']):\n",
        "#     if cell.get_text(strip=True):\n",
        "#         process_text_content(cell)\n",
        "\n",
        "# Uncomment to process headings\n",
        "# for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
        "#     if heading.get_text(strip=True):\n",
        "#         process_text_content(heading)\n",
        "\n",
        "# Save updated HTML document\n",
        "# Uncomment when ready to save\n",
        "# with open(\"/content/Cronología_Krosmoz_glossary_corrected.htm\", \"w\", encoding=\"utf-8\") as file:\n",
        "#     file.write(str(soup))\n",
        "\n",
        "print(\"HTML processing setup complete. Uncomment the processing sections when ready to apply changes.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
