{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37534f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Function to correct geoname casing with optimized rules\n",
    "def correct_geoname_casing(corrected_term):\n",
    "    if not corrected_term or not isinstance(corrected_term, str):\n",
    "        return corrected_term\n",
    "    \n",
    "    # EARLY EXIT 1: If term is already properly capitalized (first letter uppercase, not all caps)\n",
    "    if corrected_term and corrected_term[0].isupper() and not corrected_term.isupper():\n",
    "        # Check if it contains mixed casing errors (like \"asSyrians\" or \"balanites aEgyptiaca\")\n",
    "        has_mixed_case_error = False\n",
    "        words = corrected_term.split()\n",
    "        for word in words:\n",
    "            # Look for patterns like \"asSyrians\" (lowercase start + uppercase in middle)\n",
    "            if len(word) > 1 and word[0].islower() and any(c.isupper() for c in word[1:]):\n",
    "                has_mixed_case_error = True\n",
    "                break\n",
    "            # Look for patterns like \"aEgyptiaca\" (lowercase + uppercase in middle)\n",
    "            if len(word) > 2 and any(word[i].islower() and word[i+1].isupper() for i in range(len(word)-1)):\n",
    "                has_mixed_case_error = True\n",
    "                break\n",
    "        \n",
    "        # If no mixed case errors and not all uppercase, likely already correct\n",
    "        if not has_mixed_case_error:\n",
    "            # Still check language patterns and geoname corrections\n",
    "            result = corrected_term\n",
    "            \n",
    "            # Apply language pattern correction\n",
    "            match = language_pattern.search(result)\n",
    "            if match:\n",
    "                prefix = match.group(1)\n",
    "                suffix = match.group(2)\n",
    "                # Capitalize the prefix (language name), handling hyphens\n",
    "                words = prefix.replace('-', ' - ').split()\n",
    "                capitalized_words = []\n",
    "                for word in words:\n",
    "                    if word == '-':\n",
    "                        capitalized_words.append('-')\n",
    "                    else:\n",
    "                        capitalized_words.append(word.capitalize())\n",
    "                new_prefix = \"\".join(capitalized_words).replace(' - ', '-')\n",
    "                result = result.replace(match.group(0), f\"{new_prefix} {suffix}\")\n",
    "            \n",
    "            # Apply geoname corrections only if the term might contain geonames\n",
    "            # BUT with stricter exact matching only\n",
    "            result_lower = result.lower()\n",
    "            if result_lower in geoname_case_mapping:\n",
    "                return geoname_case_mapping[result_lower]\n",
    "            \n",
    "            # For multi-word terms, apply word-by-word corrections with proper boundary checking\n",
    "            words = result.split()\n",
    "            corrected_words = []\n",
    "            for word in words:\n",
    "                clean_word = re.sub(r'[^\\w\\-]', '', word)\n",
    "                punctuation = re.sub(r'[\\w\\-]', '', word)\n",
    "                clean_word_lower = clean_word.lower()\n",
    "                # Only match complete words, never substrings\n",
    "                if clean_word_lower in geoname_case_mapping and clean_word_lower == clean_word.lower():\n",
    "                    corrected_words.append(geoname_case_mapping[clean_word_lower] + punctuation)\n",
    "                elif clean_word_lower in geoname_plurals and clean_word_lower == clean_word.lower():\n",
    "                    corrected_words.append(geoname_plurals[clean_word_lower] + punctuation)\n",
    "                else:\n",
    "                    # Hyphenated compounds\n",
    "                    if '-' in clean_word and len(clean_word.split('-')) == 2:\n",
    "                        parts = clean_word.split('-')\n",
    "                        corrected_parts = []\n",
    "                        for part in parts:\n",
    "                            part_lower = part.lower()\n",
    "                            if part_lower in geoname_case_mapping and part_lower == part.lower():\n",
    "                                corrected_parts.append(geoname_case_mapping[part_lower])\n",
    "                            else:\n",
    "                                corrected_parts.append(part)\n",
    "                        corrected_words.append('-'.join(corrected_parts) + punctuation)\n",
    "                    else:\n",
    "                        corrected_words.append(word)\n",
    "            return \" \".join(corrected_words)\n",
    "        else:\n",
    "            # If mixed case error, DO NOT attempt substring replacement!\n",
    "            # Just return the original term, or apply only strict word-boundary corrections\n",
    "            words = corrected_term.split()\n",
    "            corrected_words = []\n",
    "            for word in words:\n",
    "                clean_word = re.sub(r'[^\\w\\-]', '', word)\n",
    "                punctuation = re.sub(r'[\\w\\-]', '', word)\n",
    "                clean_word_lower = clean_word.lower()\n",
    "                if clean_word_lower in geoname_case_mapping and clean_word_lower == clean_word.lower():\n",
    "                    corrected_words.append(geoname_case_mapping[clean_word_lower] + punctuation)\n",
    "                elif clean_word_lower in geoname_plurals and clean_word_lower == clean_word.lower():\n",
    "                    corrected_words.append(geoname_plurals[clean_word_lower] + punctuation)\n",
    "                else:\n",
    "                    if '-' in clean_word and len(clean_word.split('-')) == 2:\n",
    "                        parts = clean_word.split('-')\n",
    "                        corrected_parts = []\n",
    "                        for part in parts:\n",
    "                            part_lower = part.lower()\n",
    "                            if part_lower in geoname_case_mapping and part_lower == part.lower():\n",
    "                                corrected_parts.append(geoname_case_mapping[part_lower])\n",
    "                            else:\n",
    "                                corrected_parts.append(part)\n",
    "                        corrected_words.append('-'.join(corrected_parts) + punctuation)\n",
    "                    else:\n",
    "                        corrected_words.append(word)\n",
    "            return \" \".join(corrected_words)\n",
    "    \n",
    "    # EARLY EXIT 2: For simple single-word lowercase terms\n",
    "    if ' ' not in corrected_term and corrected_term.islower():\n",
    "        # Check if it's a simple geoname replacement (exact match only)\n",
    "        lower_term = corrected_term.lower()\n",
    "        if lower_term in geoname_case_mapping:\n",
    "            return geoname_case_mapping[lower_term]\n",
    "        elif lower_term in geoname_plurals:\n",
    "            return geoname_plurals[lower_term]\n",
    "        else:\n",
    "            return corrected_term  # <-- This ensures \"formalities\" stays \"formalities\"\n",
    "\n",
    "    \n",
    "    result = corrected_term\n",
    "    \n",
    "    # Apply language pattern correction (handles hyphens now)\n",
    "    match = language_pattern.search(result)\n",
    "    if match:\n",
    "        prefix = match.group(1)\n",
    "        suffix = match.group(2)\n",
    "        # Capitalize the prefix (language name), handling hyphens properly\n",
    "        words = prefix.replace('-', ' - ').split()\n",
    "        capitalized_words = []\n",
    "        for word in words:\n",
    "            if word == '-':\n",
    "                capitalized_words.append('-')\n",
    "            else:\n",
    "                capitalized_words.append(word.capitalize())\n",
    "        new_prefix = \"\".join(capitalized_words).replace(' - ', '-')\n",
    "        result = result.replace(match.group(0), f\"{new_prefix} {suffix}\")\n",
    "    \n",
    "    # Apply geoname replacements if likely to contain geonames\n",
    "    result_lower = result.lower()\n",
    "    \n",
    "    # Check for exact matches first (entire term)\n",
    "    if result_lower in geoname_case_mapping:\n",
    "        return geoname_case_mapping[result_lower]\n",
    "    \n",
    "    # For multi-word terms, apply word-by-word corrections ONLY\n",
    "    # This prevents substring matching within words\n",
    "    words = result.split()\n",
    "    if len(words) == 1 and words[0].islower() and words[0].lower() not in geoname_case_mapping and words[0].lower() not in geoname_plurals:\n",
    "        return corrected_term\n",
    "    corrected_words = []\n",
    "    word_changed = False\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove punctuation for matching but preserve it in output\n",
    "        clean_word = re.sub(r'[^\\w\\-]', '', word)\n",
    "        punctuation = re.sub(r'[\\w\\-]', '', word)\n",
    "        \n",
    "        clean_word_lower = clean_word.lower()\n",
    "        \n",
    "        # Only match complete words, never substrings\n",
    "        if clean_word_lower in geoname_case_mapping:\n",
    "            corrected_words.append(geoname_case_mapping[clean_word_lower] + punctuation)\n",
    "            word_changed = True\n",
    "        elif clean_word_lower in geoname_plurals:\n",
    "            corrected_words.append(geoname_plurals[clean_word_lower] + punctuation)\n",
    "            word_changed = True\n",
    "        else:\n",
    "            # Check for hyphenated compounds (like \"french-syrian\")\n",
    "            if '-' in clean_word and len(clean_word.split('-')) == 2:\n",
    "                parts = clean_word.split('-')\n",
    "                corrected_parts = []\n",
    "                for part in parts:\n",
    "                    part_lower = part.lower()\n",
    "                    if part_lower in geoname_case_mapping:\n",
    "                        corrected_parts.append(geoname_case_mapping[part_lower])\n",
    "                        word_changed = True\n",
    "                    else:\n",
    "                        corrected_parts.append(part)\n",
    "                corrected_words.append('-'.join(corrected_parts) + punctuation)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "    \n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def fix_middle_cased_word(word):\n",
    "            # If all caps, keep as is\n",
    "            if word.isupper():\n",
    "                return word\n",
    "            # If first char is uppercase and rest are lowercase, keep as is\n",
    "            if len(word) > 1 and word[0].isupper() and word[1:].islower():\n",
    "                return word\n",
    "            # If any uppercase in the middle, set them to lowercase\n",
    "            chars = [word[0]]\n",
    "            for c, prev in zip(word[1:], word[:-1]):\n",
    "                if c.isupper() and not prev == ' ':\n",
    "                    chars.append(c.lower())\n",
    "                else:\n",
    "                    chars.append(c)\n",
    "            return ''.join(chars)\n",
    "\n",
    "def fix_middle_cased_text(text):\n",
    "    # Only fix words with mixed casing, not all caps\n",
    "    def fix_word(word):\n",
    "        # Remove punctuation for checking\n",
    "        clean = re.sub(r'[^\\w\\-]', '', word)\n",
    "        if len(clean) > 1 and any(c.isupper() for c in clean[1:]) and not clean.isupper():\n",
    "            # Fix only the clean part, then re-attach punctuation\n",
    "            fixed = fix_middle_cased_word(clean)\n",
    "            return word.replace(clean, fixed)\n",
    "        return word\n",
    "    return ' '.join([fix_word(w) for w in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4a4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total geographical names for exclusion: 586\n",
      "Sample geonames: ['Israeli', 'Kyrgyzstani', 'France', 'Croat', 'Egypt', 'Czech', 'Futunan', 'Montserratian', 'Myanmarese', 'Cambodia']\n",
      "Processing English corrections...\n",
      "en_corrections2 has 7279 rows\n",
      "Starting geoname case correction on 7279 terms...\n",
      "Attempting vectorized processing...\n",
      "Vectorized processing completed in 0.18 seconds\n",
      "English corrections saved to data/en_corrections_processed2.xlsx with 7279 rows\n",
      "Total corrections with changes: 97\n",
      "\n",
      "First few rows of processed corrections:\n",
      "shape: (5, 6)\n",
      "┌────────────────────┬────────────────────┬─────────┬───────────────┬─────────┬────────────────────┐\n",
      "│ original           ┆ corrected          ┆ changed ┆ language_code ┆ column  ┆ lemmatized         │\n",
      "│ ---                ┆ ---                ┆ ---     ┆ ---           ┆ ---     ┆ ---                │\n",
      "│ str                ┆ str                ┆ bool    ┆ str           ┆ str     ┆ str                │\n",
      "╞════════════════════╪════════════════════╪═════════╪═══════════════╪═════════╪════════════════════╡\n",
      "│ NATURAL RESOURCES  ┆ natural resource   ┆ true    ┆ en            ┆ English ┆ natural resources  │\n",
      "│ AND THE ENVI…      ┆ and the envir…     ┆         ┆               ┆         ┆ and the envi…      │\n",
      "│ SOCIAL CONDITIONS  ┆ social condition   ┆ true    ┆ en            ┆ English ┆ social conditions  │\n",
      "│ AND EQUITY         ┆ and equity         ┆         ┆               ┆         ┆ and equity         │\n",
      "│ ECONOMIC           ┆ economic           ┆ true    ┆ en            ┆ English ┆ economic           │\n",
      "│ DEVELOPMENT AND    ┆ development and    ┆         ┆               ┆         ┆ development and    │\n",
      "│ DEVEL…             ┆ devel…             ┆         ┆               ┆         ┆ devel…             │\n",
      "│ CULTURE            ┆ culture            ┆ true    ┆ en            ┆ English ┆ culture            │\n",
      "│ INDUSTRY           ┆ industry           ┆ true    ┆ en            ┆ English ┆ industry           │\n",
      "└────────────────────┴────────────────────┴─────────┴───────────────┴─────────┴────────────────────┘\n",
      "\n",
      "Testing specific cases:\n",
      "'French' -> 'French'\n",
      "'Argentinians' -> 'Argentinians'\n",
      "'Baltic Sea' -> 'Baltic Sea'\n",
      "'Indian Sea' -> 'Indian Sea'\n",
      "'french' -> 'French'\n",
      "'argentinians' -> 'Argentinians'\n",
      "'baltic sea' -> 'baltic sea'\n",
      "'indian sea' -> 'Indian sea'\n",
      "'ASSYRIANS' -> 'ASSYRIANS'\n",
      "'Assyrians' -> 'Assyrians'\n",
      "'asSyrians' -> 'asSyrians'\n",
      "'syrian' -> 'Syrian'\n",
      "'syrians' -> 'Syrians'\n",
      "'french-syrian' -> 'french-syrian'\n",
      "'transfrench' -> 'transfrench'\n",
      "'puerto ricans' -> 'puerto ricans'\n",
      "'United States' -> 'United States'\n",
      "'slavic languages' -> 'Slavic languages'\n",
      "'budukh language' -> 'Budukh language'\n",
      "'english language' -> 'English language'\n",
      "'ural-Altaic languages' -> 'Ural-Altaic languages'\n",
      "'asian countries' -> 'Asian countries'\n",
      "'european union' -> 'European union'\n",
      "'balanites aegyptiaca' -> 'balanites aegyptiaca'\n",
      "'Latin America' -> 'Latin America'\n",
      "'Latin American' -> 'Latin American'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load geonames for exclusion\n",
    "geonames_file_path = \"data/M49_countries.xlsx\"\n",
    "geonames_df = pl.read_excel(geonames_file_path)\n",
    "en_corrections2 = pl.read_excel(\"data/en_corrections_processed_lemmatized2.xlsx\")\n",
    "# Check if en_corrections2 exists in local variables\n",
    "if 'en_corrections2' not in locals():\n",
    "    en_corrections2 = pl.read_excel(\"data/en_corrections_processed_lemmatized2.xlsx\")\n",
    "    \n",
    "\n",
    "# Load the emoji country nationality list to enrich the geonames data\n",
    "emoji_countries_df = pl.read_csv(\"data/emoji_country_nationality_list.csv\")\n",
    "\n",
    "# Create a comprehensive list of all geographical names from both sources\n",
    "english_geonames = []\n",
    "\n",
    "# Get existing English geonames from M49 countries\n",
    "if \"English\" in geonames_df.columns:\n",
    "    existing_english = geonames_df.select(\"English\").filter(\n",
    "        pl.col(\"English\").is_not_null()\n",
    "    ).to_series().to_list()\n",
    "    english_geonames.extend(existing_english)\n",
    "\n",
    "# Add country names from emoji list\n",
    "country_names = emoji_countries_df.select(\"Name\").filter(\n",
    "    pl.col(\"Name\").is_not_null()\n",
    ").to_series().to_list()\n",
    "english_geonames.extend(country_names)\n",
    "\n",
    "# Add all demonyms from emoji list (Demonym 1, 2, 3)\n",
    "for demonym_col in [\"Demonym 1\", \"Demonym 2\", \"Demonym 3\"]:\n",
    "    demonyms = emoji_countries_df.select(demonym_col).filter(\n",
    "        (pl.col(demonym_col).is_not_null()) &\n",
    "        (pl.col(demonym_col).str.strip_chars() != \"\")\n",
    "    ).to_series().to_list()\n",
    "    english_geonames.extend(demonyms)\n",
    "\n",
    "# Remove duplicates and empty strings\n",
    "english_geonames = list(set([name for name in english_geonames if name and str(name).strip()]))\n",
    "\n",
    "print(f\"Total geographical names for exclusion: {len(english_geonames)}\")\n",
    "print(f\"Sample geonames: {english_geonames[:10]}\")\n",
    "\n",
    "# Check if en_corrections2 exists\n",
    "if 'en_corrections2' in locals():\n",
    "    print(\"Processing English corrections...\")\n",
    "    print(f\"en_corrections2 has {en_corrections2.shape[0]} rows\")\n",
    "    \n",
    "    # Add additional terms to geonames list (including new Latin America terms)\n",
    "    additional_geonames = [\n",
    "        \"Asian\", \"European\", \"African\", \"American\", \"Australian\", \"Antarctic\",\n",
    "        \"Latin America\", \"Latin American\", \"CFA\", \"United States\", \"Polynesian\", \"Tibetan\",\n",
    "        \"Salvadorian\", \"Salvadoran\"\n",
    "    ]\n",
    "    english_geonames.extend(additional_geonames)\n",
    "    \n",
    "    # Remove duplicates and empty strings\n",
    "    english_geonames = list(set([name for name in english_geonames if name and str(name).strip()]))\n",
    "    \n",
    "    # OPTIMIZATION 1: Create comprehensive lookup sets\n",
    "    geoname_case_mapping = {name.lower(): name for name in english_geonames if name}\n",
    "    geoname_lower_set = set(geoname_case_mapping.keys())\n",
    "    \n",
    "    # OPTIMIZATION 2: Pre-compile regex patterns for common cases\n",
    "    # Updated pattern to include hyphenated words before \"language(s)\"\n",
    "    language_pattern = re.compile(r'\\b([\\w\\-]+(?:\\s+[\\w\\-]+)*)\\s+(languages?)\\b', re.IGNORECASE)\n",
    "    \n",
    "    # OPTIMIZATION 3: Create optimized lookup for plurals (geoname + 's')\n",
    "    geoname_plurals = {(name.lower() + 's'): (name + 's') for name in english_geonames if name}\n",
    "    \n",
    "    # Use Polars for vectorized processing\n",
    "    corrected_terms_series = en_corrections2.select(\"corrected\").to_series()\n",
    "    \n",
    "    print(f\"Starting geoname case correction on {len(corrected_terms_series)} terms...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use Polars map_elements with optimized function\n",
    "    try:\n",
    "        # Try vectorized approach first\n",
    "        print(\"Attempting vectorized processing...\")\n",
    "        en_corrections_low2 = en_corrections2.with_columns(\n",
    "            pl.col(\"corrected\").map_elements(\n",
    "                correct_geoname_casing,\n",
    "                return_dtype=pl.Utf8\n",
    "            ).alias(\"corrected\")\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Apply the fix to the corrected column\n",
    "        en_corrections_low2 = en_corrections_low2.with_columns(\n",
    "            pl.col(\"corrected\").map_elements(\n",
    "                fix_middle_cased_text,\n",
    "                return_dtype=pl.Utf8\n",
    "            ).alias(\"corrected\")\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Vectorized processing completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Vectorized processing failed: {e}\")\n",
    "        print(\"Falling back to iterative processing with optimizations...\")\n",
    "        \n",
    "        # Fallback to optimized iterative processing\n",
    "        corrected_terms = corrected_terms_series.to_list()\n",
    "        corrected_results = []\n",
    "        \n",
    "        # Use tqdm with less frequent updates for better performance\n",
    "        with tqdm(total=len(corrected_terms), \n",
    "                  desc=\"Processing corrections\", \n",
    "                  unit=\"terms\",\n",
    "                  mininterval=1.0,  # Update every 1 second minimum\n",
    "                  ncols=100) as pbar:\n",
    "            \n",
    "            batch_size = 1000  # Process in batches\n",
    "            for i in range(0, len(corrected_terms), batch_size):\n",
    "                batch = corrected_terms[i:i + batch_size]\n",
    "                batch_results = [correct_geoname_casing(term) for term in batch]\n",
    "                corrected_results.extend(batch_results)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "                # Calculate and display timing info every batch\n",
    "                if i % (batch_size * 5) == 0 or i + batch_size >= len(corrected_terms):\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    terms_per_second = (i + len(batch)) / elapsed_time\n",
    "                    remaining_terms = len(corrected_terms) - (i + len(batch))\n",
    "                    estimated_remaining_time = remaining_terms / terms_per_second if terms_per_second > 0 else 0\n",
    "                    \n",
    "                    # Update progress bar with timing info\n",
    "                    pbar.set_postfix({\n",
    "                        'rate': f'{terms_per_second:.1f} terms/sec',\n",
    "                        'ETA': f'{estimated_remaining_time:.1f}s',\n",
    "                        'changes': sum(1 for j in range(i + len(batch)) if corrected_results[j] != corrected_terms[j])\n",
    "                    })\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Iterative processing completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Average rate: {len(corrected_terms) / elapsed_time:.1f} terms per second\")\n",
    "        \n",
    "        # Create the corrected DataFrame\n",
    "        en_corrections_low2 = en_corrections2.clone().with_columns(\n",
    "            pl.Series(\"corrected\", corrected_results)\n",
    "        )\n",
    "    \n",
    "    # Save the corrected English data\n",
    "    en_corrections_low2.write_excel(\"data/en_corrections_processed2.xlsx\")\n",
    "    print(f\"English corrections saved to data/en_corrections_processed2.xlsx with {en_corrections_low2.shape[0]} rows\")\n",
    "    \n",
    "    # Display summary of changes\n",
    "    original_terms = en_corrections2.select(\"corrected\").to_series()\n",
    "    new_terms = en_corrections_low2.select(\"corrected\").to_series()\n",
    "    changes_made = (original_terms != new_terms).sum()\n",
    "    print(f\"Total corrections with changes: {changes_made}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nFirst few rows of processed corrections:\")\n",
    "    print(en_corrections_low2.head())\n",
    "    \n",
    "    # Test specific cases (updated test cases)\n",
    "    test_cases = [\n",
    "        \"French\", \"Argentinians\", \"Baltic Sea\", \"Indian Sea\", \n",
    "        \"french\", \"argentinians\", \"baltic sea\", \"indian sea\",\n",
    "        \"ASSYRIANS\", \"Assyrians\", \"asSyrians\", \"syrian\", \"syrians\", \n",
    "        \"french-syrian\", \"transfrench\", \"puerto ricans\", \"United States\",\n",
    "        \"slavic languages\", \"budukh language\", \"english language\", \"ural-Altaic languages\",\n",
    "        \"asian countries\", \"european union\", \"balanites aegyptiaca\",\n",
    "        \"Latin America\", \"Latin American\"\n",
    "    ]\n",
    "    print(\"\\nTesting specific cases:\")\n",
    "    for test_case in test_cases:\n",
    "        corrected = correct_geoname_casing(test_case)\n",
    "        print(f\"'{test_case}' -> '{corrected}'\")\n",
    "    \n",
    "else:\n",
    "    print(\"en_corrections2 not found. Please run the English batch processing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2c808",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89845640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy lemmatization samples:\n",
      "folders -> folder\n",
      "the Albanian refugees -> the Albanian refugee\n",
      "Americans -> Americans\n",
      "Syrian children -> Syrian child\n",
      "formalities -> formality\n",
      "French folders -> French folder\n",
      "French language -> French language\n",
      "Syrian locations -> Syrian location\n",
      "burkina-be -> burkina-be\n",
      "world war (1950-1953) -> world war (1950-1953)\n",
      "Mi'kmaq language -> Mi'kmaq language\n",
      "Total terms changed by spaCy lemmatization: 3133\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def spacy_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    entity_indices = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"NORP\", \"LANGUAGE\", \"LOC\"}:\n",
    "            entity_indices.update(range(ent.start, ent.end))\n",
    "    # Lemmatize only nouns and adjectives, except for named entities\n",
    "    return ''.join([\n",
    "        (token.text if i in entity_indices else (token.lemma_ if token.pos_ in {\"NOUN\", \"ADJ\"} else token.text)) + token.whitespace_\n",
    "        for i, token in enumerate(doc)\n",
    "    ])\n",
    "\n",
    "# Sample test cases\n",
    "sample_texts = [\n",
    "    \"folders\",\n",
    "    \"the Albanian refugees\",\n",
    "    \"Americans\",\n",
    "    \"Syrian children\",\n",
    "    \"formalities\",\n",
    "    \"French folders\",\n",
    "    \"French language\",\n",
    "    \"Syrian locations\",\n",
    "    \"burkina-be\",\n",
    "    \"world war (1950-1953)\",\n",
    "    \"Mi'kmaq language\",\n",
    "]\n",
    "\n",
    "print(\"spaCy lemmatization samples:\")\n",
    "for text in sample_texts:\n",
    "    print(f\"{text} -> {spacy_lemmatize(text)}\")\n",
    "\n",
    "# Apply spaCy lemmatization (with NORP, LANGUAGE, LOC exception) to the \"corrected\" column\n",
    "en_corrections_lowLemmatized = en_corrections_low2.with_columns(\n",
    "    pl.col(\"corrected\").map_elements(\n",
    "        spacy_lemmatize,\n",
    "        return_dtype=pl.Utf8\n",
    "    ).alias(\"lemmatized\")\n",
    ")\n",
    "\n",
    "# Count how many corrected terms were changed\n",
    "changes_count = (en_corrections_low2[\"corrected\"] != en_corrections_lowLemmatized[\"lemmatized\"]).sum()\n",
    "print(f\"Total terms changed by spaCy lemmatization: {changes_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb86b5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xlsxwriter.workbook.Workbook at 0x29195c69400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the lemmatized corrections\n",
    "en_corrections_lowLemmatized.write_excel(\"data/en_corrections_processed_lemmatized.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad33a5",
   "metadata": {},
   "source": [
    "# Agrovoc check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2af70af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terms changed by Agrovoc case fix: 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xlsxwriter.workbook.Workbook at 0x29184a627b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agrovoc_path = \"data/agrovoc_enhanced.xlsx\"\n",
    "agrovoc_df = pl.read_excel(agrovoc_path)\n",
    "\n",
    "# Create a lowercase lookup for Agrovoc terms\n",
    "agrovoc_lookup = {str(term).lower(): term for term in agrovoc_df[\"English\"].to_list() if term and str(term).strip()}\n",
    "\n",
    "def agrovoc_case_fix(term):\n",
    "    key = str(term).strip().lower()\n",
    "    return agrovoc_lookup.get(key, term)\n",
    "\n",
    "# Apply Agrovoc case fix to the \"corrected\" column\n",
    "en_corrections_Agrocheck = en_corrections_Agrocheck.with_columns(\n",
    "    pl.col(\"corrected\").map_elements(\n",
    "        agrovoc_case_fix,\n",
    "        return_dtype=pl.Utf8\n",
    "    ).alias(\"corrected\")\n",
    ")\n",
    "\n",
    "# Count how many terms were changed by Agrovoc case fix\n",
    "agrovoc_changes_count = (\n",
    "    en_corrections_low2[\"corrected\"] != en_corrections_Agrocheck[\"corrected\"]\n",
    ").sum()\n",
    "print(f\"Total terms changed by Agrovoc case fix: {agrovoc_changes_count}\")\n",
    "\n",
    "# Save the Agrovoc-checked corrections\n",
    "en_corrections_Agrocheck.write_excel(\"data/en_corrections_processed_agrovoc.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55c394",
   "metadata": {},
   "source": [
    "# Pluralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7373d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "agrovoc_path = \"data/agrovoc_enhanced.xlsx\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
