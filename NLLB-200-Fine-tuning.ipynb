{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/Adaptive-MT-LLM-Fine-tuning/blob/main/NLLB-200-Fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol4zOqWnGQn3"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade transformers accelerate datasets ctranslate2 sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcxSnN1lXKfW"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /workspace/models/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvLu8UPoGQ53"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "cache_dir = \"/workspace/models/cache\"\n",
        "\n",
        "model_name = \"facebook/nllb-200-3.3B\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                                            model_name,\n",
        "                                            device_map='auto',\n",
        "                                            use_cache=False,\n",
        "                                            cache_dir=cache_dir\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQLz0iJ4KD5S"
      },
      "outputs": [],
      "source": [
        "# Langauge codes supported by NLLB-200\n",
        "\n",
        "SRC_LANG = \"eng_Latn\"\n",
        "TGT_LANG = \"fra_Latn\"\n",
        "\n",
        "# SRC_LANG = \"eng_Latn\"\n",
        "# TGT_LANG = \"por_Latn\"\n",
        "\n",
        "# SRC_LANG = \"eng_Latn\"\n",
        "# TGT_LANG = \"swh_Latn\"\n",
        "\n",
        "# SRC_LANG = \"swh_Latn\"\n",
        "# TGT_LANG = \"eng_Latn\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          cache_dir=cache_dir,\n",
        "                                          src_lang=SRC_LANG,\n",
        "                                          tgt_lang=TGT_LANG\n",
        "                                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnh5nkfxQsyb"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EznpQIHQhGo"
      },
      "outputs": [],
      "source": [
        "# Load the training dataset\n",
        "\n",
        "import os\n",
        "\n",
        "trainsmall = True\n",
        "trainmedium = False if trainsmall else True\n",
        "\n",
        "src = \"en\"\n",
        "tgt = \"fr\"\n",
        "\n",
        "# src = \"en\"\n",
        "# tgt = \"pt\"\n",
        "\n",
        "# src = \"en\"\n",
        "# tgt = \"sw\"\n",
        "\n",
        "# src = \"sw\"\n",
        "# tgt = \"en\"\n",
        "\n",
        "\n",
        "# Change the path to your datasets\n",
        "directory = f\"/workspace/data/{src}-{tgt}/train\"\n",
        "\n",
        "if trainsmall and (tgt == \"fr\" or tgt == \"pt\"):\n",
        "    source_train_file = os.path.join(directory, f\"all-filtered.en.real.trainsmall\")\n",
        "    target_train_file = os.path.join(directory, f\"all-filtered.{tgt}.real.trainsmall\")\n",
        "elif trainmedium and (tgt == \"fr\" or tgt == \"pt\"):\n",
        "    source_train_file = os.path.join(directory, f\"all-filtered.en.real.trainmedium\")\n",
        "    target_train_file = os.path.join(directory, f\"all-filtered.{tgt}.real.trainmedium\")\n",
        "else:\n",
        "    source_train_file = os.path.join(directory, f\"mixed.filtered.{src}.real.train\")\n",
        "    target_train_file = os.path.join(directory, f\"mixed.filtered.{tgt}.real.train\")\n",
        "\n",
        "\n",
        "with open(source_train_file, encoding=\"utf-8\") as source, open(target_train_file, encoding=\"utf-8\") as target:\n",
        "    source_train_sentences = [sent.strip() for sent in source]\n",
        "    target_train_sentences = [sent.strip() for sent in target]\n",
        "\n",
        "print(source_train_file, target_train_file, sep=\"\\n\")\n",
        "print(len(source_train_sentences))\n",
        "print(source_train_sentences[10])\n",
        "print(target_train_sentences[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppKlceviXKfb"
      },
      "outputs": [],
      "source": [
        "# Load the test dataset\n",
        "\n",
        "directory = f\"/workspace/data/{src}-{tgt}/test\"\n",
        "\n",
        "if tgt == \"fr\" or tgt == \"pt\":\n",
        "    source_test_file = os.path.join(directory, f\"all-filtered.en.real.test\")\n",
        "    target_test_file = os.path.join(directory, f\"all-filtered.{tgt}.real.test\")\n",
        "else:\n",
        "    source_test_file = os.path.join(directory, f\"medical.filtered.{src}.real.test\")\n",
        "    target_test_file = os.path.join(directory, f\"medical.filtered.{tgt}.real.test\")\n",
        "\n",
        "with open(source_test_file, encoding=\"utf-8\") as source, open(target_test_file, encoding=\"utf-8\") as target:\n",
        "    source_test_sentences = [sent.strip() for sent in source][:1000]\n",
        "    target_test_sentences = [sent.strip() for sent in target][:1000]\n",
        "\n",
        "print(source_test_file, target_test_file, sep=\"\\n\")\n",
        "print(len(source_test_sentences))\n",
        "print(source_test_sentences[0])\n",
        "print(target_test_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnhRUrh1LnoU"
      },
      "outputs": [],
      "source": [
        "# # Test inference\n",
        "\n",
        "# from transformers import pipeline\n",
        "\n",
        "# translator = pipeline(\"translation\",\n",
        "#                       model=model,\n",
        "#                       tokenizer=tokenizer,\n",
        "#                       src_lang=SRC_LANG,\n",
        "#                       tgt_lang=TGT_LANG)\n",
        "\n",
        "# translator(source_sentences[0])[0][\"translation_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkUC0AC6MSoS"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzK-hBjhMR2Z"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "\n",
        "src_key = \"sentence_\" + SRC_LANG\n",
        "tgt_key = \"sentence_\" + TGT_LANG\n",
        "\n",
        "data_train = []\n",
        "\n",
        "for src_sent, tgt_sent in zip(source_train_sentences, target_train_sentences):\n",
        "    data_train.append({src_key: src_sent, tgt_key: tgt_sent})\n",
        "\n",
        "data_test = []\n",
        "\n",
        "for src_sent, tgt_sent in zip(source_test_sentences, target_test_sentences):\n",
        "    data_test.append({src_key: src_sent, tgt_key: tgt_sent})\n",
        "\n",
        "data_finetune = Dataset.from_list(data_train)\n",
        "data_validate = Dataset.from_list(data_test)\n",
        "\n",
        "print(data_finetune)\n",
        "print(data_validate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY9keNuEODmW"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[src_key], text_target=examples[tgt_key], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_finetune = data_finetune.map(tokenize_fn, batched=True)\n",
        "tokenized_validate = data_validate.map(tokenize_fn, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWrggllBrm3s"
      },
      "outputs": [],
      "source": [
        "print(tokenized_finetune)\n",
        "print(tokenized_validate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTmfHYCMlckB"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, logging\n",
        "from huggingface_hub import HfFolder\n",
        "import os\n",
        "\n",
        "epochs = 1\n",
        "learning_rate = 5e-5\n",
        "batch_size = 8\n",
        "\n",
        "hf_id = \"ymoslem\"  # change to your huggingface username\n",
        "output_dir = os.path.join(hf_id, f\"nllb-200-3.3B-{src}-{tgt}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    eval_accumulation_steps=4,\n",
        "    #gradient_checkpointing=True,\n",
        "\n",
        "    fp16=True,\n",
        "    fp16_full_eval=True,\n",
        "\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type='constant',  # \"constant\", \"linear\", \"cosine\"\n",
        "\n",
        "    eval_strategy=\"steps\",  # or \"epoch\"\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"tensorboard\", # \"tensorboard\", \"wandb\", or \"none\"\n",
        "\n",
        "    # push to hub parameters\n",
        "    push_to_hub=True,\n",
        "    hub_private_repo=True,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_token=HfFolder.get_token(),\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_finetune,\n",
        "    eval_dataset=tokenized_validate,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fjGn4cgXKfh"
      },
      "outputs": [],
      "source": [
        "# print(training_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tYT8-shwXKfh"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": [\"medical-en-fr\"],\n",
        "    \"language\": [\"en\", \"fr\"],\n",
        "    \"finetuned_from\": model_name,\n",
        "    \"tasks\": \"translation\",\n",
        "}"
      ],
      "metadata": {
        "id": "bL0cBOhDZ-gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "U-82nh68aCqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(output_dir, token=HfFolder.get_token())"
      ],
      "metadata": {
        "id": "hFqsjoh3aHuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbdNpOd2XKfi"
      },
      "source": [
        "# Convert to CTranslate2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT3ezBOEXKfi"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQEEjfn0XKfi"
      },
      "outputs": [],
      "source": [
        "quantization = \"float16\"  # or \"int8\"\n",
        "\n",
        "ct2_output_dir = f\"/workspace/models/{src}-{tgt}/ct2_model_{learning_rate}_{quantization}\"\n",
        "\n",
        "!ct2-transformers-converter --model {output_dir} \\\n",
        "--output_dir {ct2_output_dir} \\\n",
        "--quantization {quantization} --force \\\n",
        " && echo \"CTranslate2 model saved at: {ct2_output_dir}\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}