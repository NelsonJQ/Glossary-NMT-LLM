{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/Adaptive-MT-LLM-Fine-tuning/blob/main/vLLM-translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79qhTIN80L84"
      },
      "source": [
        "# Translation with vLLM\n",
        "\n",
        "This notebook is part of the code of my paper,  \n",
        "*Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis* ([link](https://arxiv.org/abs/2412.05862)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAzaY31JxDb"
      },
      "source": [
        "# Install vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jx_B2Jwz-lN"
      },
      "outputs": [],
      "source": [
        "# This might need a restart, so better run it in the Terminal first.\n",
        "!export VLLM_USE_MODELSCOPE=True\n",
        "!pip install -q vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhlAvEXozEj9"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /workspace/models/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bmmr-mbS8HdN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "model_directory = \"/workspace/models/cache\"\n",
        "\n",
        "os.chdir(model_directory)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHK9sTJPzEj_"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token $HF_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBQZMIj8zEkA"
      },
      "source": [
        "# Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfwtEms7iQ2",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# ✳️ Load the model -- modify the model name/path\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "# model_path = \"google/gemma-7b\"\n",
        "model_path = \"mistralai/Mistral-7B-v0.1\"\n",
        "# model_path = \"mistralai/Mixtral-8x7B-v0.1\"\n",
        "# model_path = \"meta-llama/Meta-Llama-3-8B\"\n",
        "# model_path = \"meta-llama/Meta-Llama-3-70B\"\n",
        "# model_path = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "# model_path = \"meta-llama/Llama-3.1-405B\"\n",
        "\n",
        "# AWQ models\n",
        "# model_path = \"RiversHaveWings/Meta-Llama-3.1-405B-AWQ\"\n",
        "# model_path = \"cognitivecomputations/DeepSeek-V3-AWQ\"\n",
        "\n",
        "\n",
        "# Get the number of available GPUs\n",
        "# Hint: Llama 70B -> 2 H100\n",
        "# Llama 405B AWQ -> 4 H100\n",
        "# DeepSeek V3 AWQ -> 4 H200 or 8 H100\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "max_len = 4096  # increase for longer context (withit memory limits)\n",
        "awq = True if \"-awq\" in model_path.lower() else False  # verify based on your model\n",
        "\n",
        "print(f\"Number of GPUs: {num_gpus}\")\n",
        "print(f\"Max length: {max_len}\")\n",
        "print(f\"AWQ: {awq}\")\n",
        "\n",
        "\n",
        "if awq:\n",
        "    llm = LLM(model=model_path,\n",
        "             download_dir=model_directory,\n",
        "              trust_remote_code=True,\n",
        "              tensor_parallel_size=num_gpus,\n",
        "              quantization=\"awq_marlin\",\n",
        "              max_model_len=max_len,\n",
        "             )\n",
        "else:\n",
        "    llm = LLM(model=model_path,\n",
        "              download_dir=model_directory,\n",
        "              trust_remote_code=True,\n",
        "              dtype=torch.bfloat16,\n",
        "              tensor_parallel_size=num_gpus,\n",
        "              max_model_len=max_len,\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UITdkdu2zEkB"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJxF1V-FzEkB"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cC_GuBv7V2a",
        "outputId": "8ad15166-b380-45cc-d7ed-a032b295a1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spanish: Período de validez después de abierto el envase: 10 horas.\n",
            "English:\n",
            "\n",
            "Spanish: Período de validez después de abierto el envase: 4 semanas\n",
            "English: Shelf life after opening the immediate packaging: 4 weeks.\n",
            "Spanish: Período de validez después de abierto el envase: 10 horas.\n",
            "English:\n"
          ]
        }
      ],
      "source": [
        "# Test prompts - Spanish to English\n",
        "\n",
        "src_lang = \"Spanish\"\n",
        "tgt_lang = \"English\"\n",
        "\n",
        "# Zero-shot prompt\n",
        "prompt_source = (\n",
        "    f\"{src_lang}: Período de validez después de abierto el envase: 10 horas.\\n\"\n",
        "    f\"{tgt_lang}:\"\n",
        ")\n",
        "\n",
        "# Fuzzy one-shot prompt\n",
        "prompt_fuzzy = (\n",
        "    f\"{src_lang}: Período de validez después de abierto el envase: 4 semanas\\n\"\n",
        "    f\"{tgt_lang}: Shelf life after opening the immediate packaging: 4 weeks.\\n\"\n",
        "    f\"{src_lang}: Período de validez después de abierto el envase: 10 horas.\\n\"\n",
        "    f\"{tgt_lang}:\"\n",
        ")\n",
        "\n",
        "test_prompts = [prompt_source, prompt_fuzzy]\n",
        "\n",
        "print(*test_prompts, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWYpMHA24aBF"
      },
      "outputs": [],
      "source": [
        "# Test Greedy search\n",
        "test_sampling_params = SamplingParams(\n",
        "                                 temperature=0.0,\n",
        "                                 top_p=1,\n",
        "                                 top_k=1,\n",
        "                                 max_tokens=30,\n",
        "                                 skip_special_tokens=False,\n",
        "                                 stop=[\"\\n\"],\n",
        "                                 )\n",
        "\n",
        "outputs = llm.generate(test_prompts,\n",
        "                       test_sampling_params)\n",
        "\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text.strip()\n",
        "    print()\n",
        "    print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnh5nkfxQsyb"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7jIwEIkCQe1b",
        "outputId": "f1d9bd50-5e9c-4a14-94f7-d64d66f19152"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/data/en-fr'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ✳️ Load the data -- modify the data directory\n",
        "\n",
        "import os\n",
        "\n",
        "# En-FR\n",
        "data_path = \"/workspace/data/\"\n",
        "\n",
        "directory = os.path.join(data_path, \"en-fr\")\n",
        "\n",
        "# EN-PT\n",
        "# directory = os.path.join(data_path, \"en-pt\")\n",
        "\n",
        "# EN-SW\n",
        "# directory = os.path.join(data_path, \"en-sw\")\n",
        "\n",
        "# SW-EN\n",
        "# directory = os.path.join(data_path, \"sw-en\")\n",
        "\n",
        "os.chdir(directory)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Enb8Wc8BkJe"
      },
      "outputs": [],
      "source": [
        "!ls $directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EznpQIHQhGo",
        "outputId": "50e8eb8a-67cb-44be-86b2-bcd1b031deff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reduce the dosage by initiating the patient at 90% of the previous total daily dosage, with 40% as basal rate and 50% as boluses divided between the three main meals.\n",
            "Lors du transfert de patients d’ un traitement par injection à la perfusion, il est généralement conseillé de diminuer la posologie en commençant par administrer 90% de la dose journalière totale précédente, dont 40% en débit de base et 50% en bolus répartis entre les trois repas principaux.\n"
          ]
        }
      ],
      "source": [
        "# ✳️ Load test datasets\n",
        "\n",
        "# EN-FR\n",
        "source_test_file = \"all-filtered.en.real.test\"\n",
        "target_test_file = \"all-filtered.fr.real.test\"\n",
        "\n",
        "# # EN-PT\n",
        "# source_test_file = \"all-filtered.en.real.test\"\n",
        "# target_test_file = \"all-filtered.pt.real.test\"\n",
        "\n",
        "# EN-SW - Generic\n",
        "# source_test_file = \"generic.filtered.en.real.test\"\n",
        "# target_test_file = \"generic.filtered.sw.real.test\"\n",
        "\n",
        "# EN-SW - Medical\n",
        "# source_test_file = \"medical.filtered.en.real.test\"\n",
        "# target_test_file = \"medical.filtered.sw.real.test\"\n",
        "\n",
        "# SW-EN - Generic\n",
        "# source_test_file = \"generic.filtered.sw.real.test\"\n",
        "# target_test_file = \"generic.filtered.en.real.test\"\n",
        "\n",
        "# SW-EN - Medical\n",
        "# source_test_file = \"medical.filtered.sw.real.test\"\n",
        "# target_test_file = \"medical.filtered.en.real.test\"\n",
        "\n",
        "with open(source_test_file, encoding=\"utf-8\") as source, open(target_test_file, encoding=\"utf-8\") as target:\n",
        "    source_sentences = [sent.strip() for sent in source.readlines()]\n",
        "    target_sentences = [sent.strip() for sent in target.readlines()]\n",
        "\n",
        "print(source_sentences[0])\n",
        "print(target_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzsxxv1tQj3b",
        "outputId": "914f0845-fcb5-4356-825c-2b1af58fbddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Titration and maintenance dose The daily dose is subsequently increased by doubling the dose at intervals of one to three days up to the target maintenance dose of 5 mg twice daily.\n",
            "reduce the dosage by initiating the patient at 90% of the previous total daily dosage, with 40% as basal rate and 50% as boluses divided between the three main meals.\n",
            "22 Titration et dose d’ entretien La dose quotidienne sera augmentée par la suite en doublant la dose à un à trois jours d’ intervalle jusqu’ à atteindre la dose d’ entretien cible de 5 mg deux fois par jour.\n"
          ]
        }
      ],
      "source": [
        "# ✳️ Load the fuzzy matches from the Context Dataset\n",
        "\n",
        "# EN-FR\n",
        "online_test_file = \"all-filtered.en-fr.ms-multi-12.context.test\"\n",
        "\n",
        "# EN-PT\n",
        "# online_test_file = \"all-filtered.en-pt.ms-multi-12.online.test\"\n",
        "\n",
        "# EN-SW\n",
        "# online_test_file = \"generic.filtered.ensw.ms-multi-12.online.test\"\n",
        "# online_test_file = \"medical.filtered.ensw.ms-multi-12.online.test\"\n",
        "\n",
        "# SW-EN\n",
        "# online_test_file = \"generic.filtered.swen.ms-multi-12.online.test\"\n",
        "# online_test_file = \"medical.filtered.swen.ms-multi-12.online.test\"\n",
        "\n",
        "with open(online_test_file, encoding=\"utf-8\") as online:\n",
        "    lines = [line.strip().split(\" ||| \") for line in online.readlines()]\n",
        "    scores = [float(line[0].strip()) for line in lines]\n",
        "    fuzzy_source_sentences = [line[1].strip() for line in lines]\n",
        "    online_source_sentences = [line[2].strip() for line in lines]\n",
        "    fuzzy_target_prefixes = [line[3].strip() for line in lines]\n",
        "\n",
        "print(fuzzy_source_sentences[0])\n",
        "print(online_source_sentences[0])\n",
        "print(fuzzy_target_prefixes[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYb9dxsAQtpt"
      },
      "source": [
        "# Create the prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhtPPU5zDFp0"
      },
      "outputs": [],
      "source": [
        "# Function to create zero-shot and one-shot prompts\n",
        "\n",
        "def create_prompt(source_lang,\n",
        "                  target_lang,\n",
        "                  fuzzy_sources,\n",
        "                  fuzzy_targets,\n",
        "                  new_sources,\n",
        "                  one_shot=True\n",
        "                  ):\n",
        "\n",
        "    prompts = []\n",
        "\n",
        "    if one_shot:\n",
        "        for fuzzy_src, fuzzy_tgt, new_src in zip(fuzzy_sources, fuzzy_targets, new_sources):\n",
        "            fuzzy_src = source_lang + \": \" + fuzzy_src\n",
        "            fuzzy_tgt = target_lang + \": \" + fuzzy_tgt\n",
        "            new_src = source_lang + \": \" + new_src\n",
        "            segment = fuzzy_src + \"\\n\" + fuzzy_tgt + \"\\n\" + new_src + \"\\n\" + target_lang + \":\"\n",
        "            prompts.append(segment)\n",
        "    else:\n",
        "        for new_src in new_sources:\n",
        "            new_src = source_lang + \": \" + new_src\n",
        "            segment = new_src + \"\\n\" + target_lang + \":\"\n",
        "            prompts.append(segment)\n",
        "\n",
        "    return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_XXEc-cQvno"
      },
      "outputs": [],
      "source": [
        "# ✳️ Define the source and target languages\n",
        "\n",
        "source_lang = \"English\"\n",
        "target_lang = \"French\"\n",
        "\n",
        "# source_lang = \"English\"\n",
        "# target_lang = \"Portuguese\"\n",
        "\n",
        "# source_lang = \"English\"\n",
        "# target_lang = \"Swahili\"\n",
        "\n",
        "# source_lang = \"Swahili\"\n",
        "# target_lang = \"English\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVTTpMt8QxKX"
      },
      "outputs": [],
      "source": [
        "# Create prompts\n",
        "\n",
        "prompts_zero_shot = create_prompt(source_lang,\n",
        "                                  target_lang,\n",
        "                                  fuzzy_source_sentences,\n",
        "                                  fuzzy_target_prefixes,\n",
        "                                  online_source_sentences,\n",
        "                                  one_shot=False\n",
        "                                  )\n",
        "\n",
        "prompts_one_shot = create_prompt(source_lang,\n",
        "                                  target_lang,\n",
        "                                  fuzzy_source_sentences,\n",
        "                                  fuzzy_target_prefixes,\n",
        "                                  online_source_sentences,\n",
        "                                  one_shot=True\n",
        "                                  )\n",
        "\n",
        "print(len(prompts_zero_shot))\n",
        "print(len(prompts_one_shot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAlrExMiEDhk"
      },
      "outputs": [],
      "source": [
        "print(prompts_zero_shot[0], \"\\n\")\n",
        "print(prompts_one_shot[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWNE742eekkC",
        "outputId": "040eb445-7e5b-4754-fefe-173337f981c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: This checklist will remind prescribers how to use the medicine safely.\n",
            "French: \n",
            "\n",
            "English: Follow your doctor’s instruction carefully on which medicines can be combined.\n",
            "French: Suivez attentivement les instructions de votre médecin quant aux médicaments qui peuvent être associés.\n",
            "English: This checklist will remind prescribers how to use the medicine safely.\n",
            "French:\n"
          ]
        }
      ],
      "source": [
        "print(prompts_zero_shot[20], \"\\n\")\n",
        "print(prompts_one_shot[20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFdPdl-jH67Q"
      },
      "source": [
        "# Test translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krZGcu2fH8Q9"
      },
      "outputs": [],
      "source": [
        "# Test: Tokenize and generate (single prompt)\n",
        "\n",
        "n = 0\n",
        "l = 1\n",
        "test_prompts = prompts_zero_shot[n:n+l]\n",
        "# test_prompts = prompts_one_shot[n:n+l]\n",
        "print(*test_prompts, sep=\"\\n\", end=\"\\n\\n\")\n",
        "print(\"Translations:\\n\")\n",
        "\n",
        "test_max_tokens = len(test_prompts[0].split() * 4)\n",
        "\n",
        "# Greedy search\n",
        "test_sampling_params = SamplingParams(\n",
        "                                 temperature=0.0,\n",
        "                                 top_p=1,\n",
        "                                 top_k=1,\n",
        "                                 max_tokens=test_max_tokens,\n",
        "                                 skip_special_tokens=False,\n",
        "                                 stop=[\"\\n\"],\n",
        "                                 #  stop_token_ids=[\"\\n\"],\n",
        "                                 logprobs=2,\n",
        "                                 #  prompt_logprobs=1\n",
        "                                 )\n",
        "\n",
        "outputs = llm.generate(test_prompts,\n",
        "                       test_sampling_params)\n",
        "\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text.strip()\n",
        "    print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS1a79KyMrJd"
      },
      "source": [
        "# Translation - full test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKh9naDxMHwl"
      },
      "outputs": [],
      "source": [
        "# @title ✳️ Set prompts (zero-shot, one-shot)\n",
        "\n",
        "prompts = prompts_zero_shot\n",
        "# prompts = prompts_one_shot\n",
        "\n",
        "print(prompts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXy_nPCPT_So"
      },
      "outputs": [],
      "source": [
        "length_multiplier = 4\n",
        "\n",
        "# Calculate max length\n",
        "length = [len(prompt.split(\"\\n\")[-2].split(\" \")[1:]) for prompt in prompts]\n",
        "max_len = max(length) * length_multiplier\n",
        "print(f\"Max length: {max_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oASh5Ho3M9Y2"
      },
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(\n",
        "                                 temperature=0.0,\n",
        "                                 top_p=1,\n",
        "                                 top_k=1,\n",
        "                                 max_tokens=max_len,\n",
        "                                 skip_special_tokens=True,\n",
        "                                 stop=[\"\\n\"]\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6vjWWbNMcr3"
      },
      "outputs": [],
      "source": [
        "generated_outputs = llm.generate(prompts,\n",
        "                                 sampling_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCXp1XZLM__C"
      },
      "outputs": [],
      "source": [
        "translations = []\n",
        "\n",
        "for generation in generated_outputs:\n",
        "    translation = generation.outputs[0].text.strip()\n",
        "    translations.append(translation)\n",
        "\n",
        "len(translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_NYdZP6WZqA"
      },
      "outputs": [],
      "source": [
        "print(*translations[:5], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZS2np8Iw9Lo"
      },
      "source": [
        "# Save the translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yF4um1LzEkJ"
      },
      "outputs": [],
      "source": [
        "# !mkdir -p \"/workspace/data/en-fr/translations-vllm\"\n",
        "# !mkdir -p \"/workspace/data/en-pt/translations-vllm\"\n",
        "# !mkdir -p \"/workspace/data/en-sw/translations-vllm\"\n",
        "# !mkdir -p \"/workspace/data/sw-en/translations-vllm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0PryqEOw-mV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = \"/workspace/data/\"\n",
        "\n",
        "translations_directory = os.path.join(data_path, \"en-fr\", \"translations-vllm\")\n",
        "\n",
        "# translations_directory = os.path.join(data_path, \"en-pt\", \"translations-vllm\")\n",
        "\n",
        "# translations_directory = os.path.join(data_path, \"en-sw\", \"translations-vllm\")\n",
        "\n",
        "# translations_directory = os.path.join(data_path, \"sw-en\", \"translations-vllm\")\n",
        "\n",
        "os.chdir(translations_directory)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtyZQenR7czR"
      },
      "outputs": [],
      "source": [
        "# ✳️ Change translations file name\n",
        "\n",
        "# EN-FR - Medical\n",
        "translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-zero-shot.fr\"\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-one-shot.fr\"\n",
        "\n",
        "# EN-PT - Medical\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-zero-shot.pt\"\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-one-shot.pt\"\n",
        "\n",
        "# EN-SW - Generic\n",
        "# translations_file_name = \"test-generic-translated-Llama-3.3-70b-baseline-vLLM-zero-shot.sw\"\n",
        "# translations_file_name = \"test-generic-translated-Llama-3.3-70b-baseline-vLLM-one-shot.sw\"\n",
        "\n",
        "# EN-SW - Medidal\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-zero-shot.sw\"\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-one-shot.sw\"\n",
        "\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-zero-shot-temp2.sw\"\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-one-shot-temp2.sw\"\n",
        "\n",
        "# SW-EN - Generic\n",
        "# translations_file_name = \"test-generic-translated-Llama-3.3-70b-baseline-vLLM-zero-shot.en\"\n",
        "# translations_file_name = \"test-generic-translated-Llama-3.3-70b-baseline-vLLM-one-shot.en\"\n",
        "\n",
        "# SW-EN - Medical\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-zero-shot.en\"\n",
        "# translations_file_name = \"test-medical-translated-Llama-3.3-70b-baseline-vLLM-one-shot.en\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAcRk5kRWsuW"
      },
      "outputs": [],
      "source": [
        "with open(translations_file_name, \"w+\", encoding=\"utf-8\") as output:\n",
        "    for translation in translations:\n",
        "        output.write(translation.strip() + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXl3b5QuXVun"
      },
      "outputs": [],
      "source": [
        "!wc -l $translations_file_name\n",
        "!head -n 3 $translations_file_name"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}